{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dwZnHBr_UnjF",
        "B193dMxQUz5a",
        "yPOZ-seeU6Zq",
        "7FN3tfmmVB_G",
        "IeCvk_ExVLpz",
        "DPJbweANVSDP"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Hybrid Inference Strategy 2: QP + Beam & Sampled CP + CP-Verifier\n",
        "\n",
        "**Objective**  \n",
        "Extend the lightweight two-stage pipeline by diversifying CoT parses via both deterministic beam-search and stochastic sampling, then rerank with the CP verifier:\n",
        "\n",
        "1. **Question Parsing (QP) Stage**  \n",
        "   - Use the fine-tuned LLaMA-3 QP model (beam-search, `num_beams=5`) to produce one deterministic JSON list of logical constraints.  \n",
        "   - Clean out any lingering multiple-choice artifacts (e.g. “A.”, “B.”, “Option”, etc.).  \n",
        "   - No verifier applied at this stage.\n",
        "\n",
        "2. **Chain-of-Thought Parsing (CP) Stage**  \n",
        "   - Given the cleaned QP list, call the fine-tuned LLaMA-3 CP model to generate **5 parses** per example:  \n",
        "     - **2** via beam-search (`num_beams=5`, `num_return_sequences=2`)  \n",
        "     - **3** via sampling (`do_sample=True`, `temperature=0.8`, `num_return_sequences=3`)  \n",
        "   - Parse each into structured steps (`statement`, `evidence`, `Verification`), then clean, dedupe, and normalize.\n",
        "\n",
        "3. **Verifier Reranking**  \n",
        "   - For each CP candidate, score **each step** using the CP verifier (DeBERTa-v3).  \n",
        "   - Convert the “true” probabilities to log-probs and **sum** across all steps:  \n",
        "     ```python\n",
        "     total_logprob = sum(log(p_true + 1e-12) for p_true in step_true_probs)\n",
        "     ```  \n",
        "   - Select the candidate with the **highest total_logprob**.  \n",
        "   - No threshold fallback here (sampling diversity is trusted at THR_CP = 0.60).\n",
        "\n",
        "4. **Output**  \n",
        "   - Emit a JSON record per example:  \n",
        "     ```json\n",
        "     {\n",
        "       \"question\": …,\n",
        "       \"question_parsing\": …,\n",
        "       \"cot\": …,\n",
        "       \"cot_parsing\": …,        // best-of-5 candidate\n",
        "       \"answer\": …,\n",
        "       \"id\": …,\n",
        "       \"sel_idx\": …\n",
        "     }\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "| Metric                         | v2 Score |\n",
        "|--------------------------------|----------|\n",
        "| **Question_Macro_F1**          | 0.7658   |\n",
        "| **Statement_Macro_F1**         | 0.4102   |\n",
        "| **Statement_Evidence_Macro_F1**| 0.1711   |\n",
        "| **Reasoning_F1**               | 0.1231   |\n",
        "\n",
        "> _Trading a small Question_F1 drop for substantial gains in evidence‐linking and reasoning robustness thanks to candidate diversity._\n"
      ],
      "metadata": {
        "id": "t4lJmpJbF9o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Thresholds"
      ],
      "metadata": {
        "id": "us4TmYCoUemg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install core evaluation utilities\n",
        "!pip install -q evaluate\n",
        "!pip install json5\n",
        "\n",
        "!pip uninstall -y nltk\n",
        "!pip install -q --upgrade nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAnsRdeeUrJO",
        "outputId": "d2a0ec0c-4692-45d7-93a7-95a3c71169b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting json5\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Downloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: json5\n",
            "Successfully installed json5-0.12.0\n",
            "Found existing installation: nltk 3.9.1\n",
            "Uninstalling nltk-3.9.1:\n",
            "  Successfully uninstalled nltk-3.9.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpP8AfiWUrsh",
        "outputId": "b105cded-448c-4985-ccba-eb66002e6c4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6PbBsF3F4sF"
      },
      "outputs": [],
      "source": [
        "import unsloth  # Must come first for 4-bit LoRA\n",
        "import torch, gc, json, re, ast, html, numpy as np\n",
        "from torch.nn.functional import log_softmax\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "\n",
        "# Paths & thresholds\n",
        "INPUT       = \"/content/drive/MyDrive/llm-sr-project/testingData-blank.json\"\n",
        "#OUTPUT      = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach.json\"\n",
        "OUTPUT      = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach3.json\"\n",
        "QP_LM_PATH  = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_question_parsing\"\n",
        "CP_LM_PATH  = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_cot_parsing\"\n",
        "QP_VER_PATH = \"/content/drive/MyDrive/deberta-qparse-verifier\"\n",
        "CP_VER_PATH = \"/content/drive/MyDrive/deberta-cotparse-verifier\"\n",
        "\n",
        "\n",
        "THR_QP = 0.75\n",
        "THR_CP = 0.60\n",
        "#THR_CP = 0.80\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "dwZnHBr_UnjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In-Context Learning (ICL) Prompts\n",
        "\n",
        "QP_DEMON = '''The question is:\n",
        "\n",
        "There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project. This assignment must satisfy:\n",
        "(1) If A works on Alpha, then B works on Beta.\n",
        "(2) If C works on Alpha, then D and E work on Beta.\n",
        "(3) F works on a different project than E.\n",
        "(4) D must work on a different project than A.\n",
        "(5) If F works on Alpha, then B works on Alpha.\n",
        "\n",
        "If A works on Beta, which of the following must be true?\n",
        "A. B works on Alpha\n",
        "B. C works on Beta\n",
        "C. D works on Alpha\n",
        "D. F works on Beta\n",
        "\n",
        "The parsing result is:\n",
        "\n",
        "[\n",
        "  \"There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project.\",\n",
        "  \"If A works on Alpha, then B works on Beta\",\n",
        "  \"If C works on Alpha, then D and E work on Beta\",\n",
        "  \"F works on a different project than E\",\n",
        "  \"D must work on a different project than A\",\n",
        "  \"If F works on Alpha, then B works on Alpha\",\n",
        "  \"A works on Beta\"\n",
        "]\n",
        "'''\n",
        "\n",
        "\n",
        "QP_TEMPLATE = '''Given a question, extract all relevant information from the question that would help to solve it.\n",
        "\n",
        "This includes:\n",
        "- General setup information (e.g., number of people, projects involved)\n",
        "- Explicit facts given in the question\n",
        "- All logical constraints or conditions\n",
        "\n",
        "Output only a JSON list and nothing else. Follow the format shown in the example.\n",
        "\n",
        "Example:\n",
        "\n",
        "{demon}\n",
        "\n",
        "Now, the question is:\n",
        "\n",
        "{question}\n",
        "\n",
        "Your output:\n",
        "'''\n",
        "\n",
        "CP_DEMON = '''The question is:\n",
        "\n",
        "There are 6 volunteers: A, B, C, D, E and F. Each person works on exactly one project.\n",
        "\n",
        "Conditions:\n",
        "(1) If A works on Alpha, then B works on Beta.\n",
        "(2) If C works on Alpha, then D and E work on Beta.\n",
        "(3) F works on a different project than E.\n",
        "(4) D must work on a different project than A.\n",
        "(5) If F works on Alpha, then B works on Alpha.\n",
        "\n",
        "Question:\n",
        "If A works on Beta, which of the following must be true?\n",
        "\n",
        "CoT:\n",
        "Since A works on Beta, Condition (1) is not triggered. Condition (2) is not triggered since C's assignment is unknown. Condition (3) doesn't give anything because E's assignment is unspecified. Condition (4) says D must work on a different project than A, so D must work on Alpha. Condition (5) depends on F, which is unknown.\n",
        "\n",
        "Parsing result:\n",
        "\n",
        "[\n",
        "  {\n",
        "    \"statement\": \"Condition (1) is not applicable\",\n",
        "    \"evidence\": \"Condition (1): If A works on Alpha, then B works on Beta. | A is working on Beta\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (2) is not applicable\",\n",
        "    \"evidence\": \"Condition (2): If C works on Alpha, then D and E work on Beta. | C's assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (3) does not provide any info\",\n",
        "    \"evidence\": \"Condition (3): F works on a different project than E. | E's assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"D must work on Alpha\",\n",
        "    \"evidence\": \"Condition (4): D must work on a different project than A, and A is working on Beta\",\n",
        "    \"Verification\": \"true\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (5) is not applicable\",\n",
        "    \"evidence\": \"Condition (5): If F works on Alpha, then B works on Alpha. | F's assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  }\n",
        "]\n",
        "'''\n",
        "\n",
        "\n",
        "CP_TEMPLATE = '''You are a reasoning assistant. Based on the question, conditions, and chain-of-thought (CoT), extract every inference or non-inference step as a JSON object.\n",
        "\n",
        "For each CoT sentence that either:\n",
        "  1. Refers to a condition (e.g. \"Condition (2) …\")\n",
        "  2. Starts with an inference cue (\"Since\", \"Therefore\", \"This means\", \"We can deduce\", etc.)\n",
        "\n",
        "Produce one object with:\n",
        "  • \"statement\": the new claim you read in that CoT sentence (don't quote the entire sentence—just the core inference).\n",
        "  • \"evidence\":\n",
        "      – if the claim restates a constraint, use the exact line from the **Conditions** block,\n",
        "      – otherwise, use the CoT fragment that you extracted it from.\n",
        "  • \"Verification\":\n",
        "      – `\"false\"` if the sentence rejects or blocks a condition (contains \"not applicable\", \"does not provide\", etc.),\n",
        "      – otherwise `\"true\"`.\n",
        "\n",
        "Keep the objects in the same order as they appear in the CoT.\n",
        "\n",
        "Example:\n",
        "\n",
        "{demon}\n",
        "\n",
        "Now, given:\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Conditions:\n",
        "{conditions}\n",
        "\n",
        "Chain-of-Thought:\n",
        "{cot}\n",
        "\n",
        "Your output:\n",
        "'''"
      ],
      "metadata": {
        "id": "YXKT9h3eUxeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "B193dMxQUz5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions\n",
        "def clean_quotes(t):\n",
        "    return (t.replace('\"','\"').replace('\"','\"').replace(\"'\",\"'\").replace(\"'\",\"'\"))\n",
        "\n",
        "def normalize_text(t):\n",
        "    t = clean_quotes(t)\n",
        "    t = re.sub(r'\\?\\s(?=[A-Z])', ', ', t)\n",
        "    t = re.sub(r'(?<=[a-zA-Z])\\.(?=[A-Z])', '. ', t)\n",
        "    t = re.sub(r'(?<![A-Da-d])\\\\n(?!\\s?[A-Da-d]\\\\.)', ' ', t)\n",
        "    return html.unescape(t).strip()\n",
        "\n",
        "def extract_json(raw):\n",
        "    raw = raw.strip()\n",
        "    i = raw.find('[')\n",
        "    if i < 0: return []\n",
        "    depth = 0\n",
        "    for j,ch in enumerate(raw[i:], i):\n",
        "        if ch=='[': depth+=1\n",
        "        elif ch==']': depth-=1\n",
        "        if depth==0:\n",
        "            blk = raw[i:j+1]\n",
        "            for p in (json.loads, ast.literal_eval):\n",
        "                try: return p(blk)\n",
        "                except: pass\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "def score_verifier_batch(prem_list, hyp_list, tok, mod):\n",
        "    enc = tok(prem_list, hyp_list, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = mod(**enc).logits\n",
        "    return torch.softmax(logits, dim=1)[:, 1].tolist()\n",
        "\n",
        "def clean_qp(qp_list):\n",
        "    return [s for s in qp_list if not re.match(r'^[A-Da-d][\\.:\\)]', s.strip()) and \"Option\" not in s and \"following\" not in s]"
      ],
      "metadata": {
        "id": "yOgR6RLSU3OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Models and Verifiers"
      ],
      "metadata": {
        "id": "yPOZ-seeU6Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QP LM - Using beam search\n",
        "qp_tok = AutoTokenizer.from_pretrained(QP_LM_PATH)\n",
        "qp_tok.model_max_length = 1024\n",
        "qp_mod = AutoModelForCausalLM.from_pretrained(QP_LM_PATH).to(device)\n",
        "qp_pipe = pipeline(\"text-generation\", model=qp_mod, tokenizer=qp_tok,\n",
        "                   return_full_text=False, do_sample=False,\n",
        "                   num_beams=5, early_stopping=True,\n",
        "                   max_new_tokens=512, batch_size=4)\n",
        "\n",
        "# CP LM - Using beam search\n",
        "cp_tok = AutoTokenizer.from_pretrained(CP_LM_PATH)\n",
        "cp_tok.model_max_length = 2048\n",
        "cp_mod = AutoModelForCausalLM.from_pretrained(CP_LM_PATH).to(device)\n",
        "\n",
        "# Generate 2 beam candidates + 3 sampled candidates = 5 total\n",
        "cp_pipe = pipeline(\n",
        "     \"text-generation\",\n",
        "     model=cp_mod, tokenizer=cp_tok,\n",
        "     return_full_text=False,\n",
        "     do_sample=False, num_beams=5, num_return_sequences=2,\n",
        "     max_new_tokens=1024,\n",
        "     batch_size=4\n",
        ")\n",
        "\n",
        "cp_sampler = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=cp_mod, tokenizer=cp_tok,\n",
        "    return_full_text=False,\n",
        "     do_sample=True, temperature=0.8, num_return_sequences=3,\n",
        "     max_new_tokens=1024,\n",
        "     batch_size=4\n",
        ")\n",
        "\n",
        "\n",
        "# Load QP and COT Verifiers\n",
        "def load_verifiers():\n",
        "    global qv_tok, qv_mod, cv_tok, cv_mod\n",
        "    qv_tok = AutoTokenizer.from_pretrained(QP_VER_PATH)\n",
        "    qv_mod = AutoModelForSequenceClassification.from_pretrained(QP_VER_PATH).to(device)\n",
        "    cv_tok = AutoTokenizer.from_pretrained(CP_VER_PATH)\n",
        "    cv_mod = AutoModelForSequenceClassification.from_pretrained(CP_VER_PATH).to(device)\n",
        "    return qv_tok, qv_mod, cv_tok, cv_mod\n",
        "\n",
        "\n",
        "qv_tok, qv_mod, cv_tok, cv_mod = load_verifiers()"
      ],
      "metadata": {
        "id": "hznz7NZGU6Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Inference Function"
      ],
      "metadata": {
        "id": "7FN3tfmmVB_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hybrid inference approach v2\n",
        "def process_one(example):\n",
        "    q_raw, cot_raw = example[\"question\"], example[\"cot\"]\n",
        "    sel_idx, ans = example.get(\"sel_idx\"), example.get(\"answer\")\n",
        "    q, cot = normalize_text(q_raw), normalize_text(cot_raw)\n",
        "\n",
        "    # 1) Single deterministic QP output\n",
        "    prompt = QP_TEMPLATE.format(demon=QP_DEMON, question=q)\n",
        "    raw_qp = qp_pipe(prompt, max_new_tokens=512)\n",
        "    if not isinstance(raw_qp, list):\n",
        "        raw_qp = [raw_qp]\n",
        "    best_qp = clean_qp(extract_json(raw_qp[0][\"generated_text\"]))\n",
        "\n",
        "\n",
        "\n",
        "    # 2) CP: generate 3 beam-search parses\n",
        "    conds_str = json.dumps(best_qp, ensure_ascii=False)\n",
        "    prompt_cp = CP_TEMPLATE.format(\n",
        "        demon      = CP_DEMON,\n",
        "        question   = q,\n",
        "        conditions = conds_str,\n",
        "        cot        = cot\n",
        "    )\n",
        "\n",
        "    # 2.a) get 5 raw outputs: 2 from beam, 3 from sampler\n",
        "    raw_beams = cp_pipe(prompt_cp, max_new_tokens=1024)\n",
        "    raw_samples = cp_sampler(prompt_cp, max_new_tokens=1024)\n",
        "    raw_cp_outs = (raw_beams if isinstance(raw_beams, list) else [raw_beams]) \\\n",
        "            + (raw_samples if isinstance(raw_samples, list) else [raw_samples])\n",
        "\n",
        "    # flatten HF’s list-of-lists (if any)\n",
        "    raw_cp_flat = [\n",
        "        item\n",
        "        for sub in raw_cp_outs\n",
        "        for item in (sub if isinstance(sub, list) else [sub])\n",
        "    ]\n",
        "\n",
        "    # 2.b) parse & clean each candidate\n",
        "    cps_candidates = []\n",
        "    for out in raw_cp_flat:\n",
        "        parsed = extract_json(out[\"generated_text\"])\n",
        "        if not parsed:\n",
        "            continue\n",
        "        seen = set()\n",
        "        cleaned = []\n",
        "        for st in parsed:\n",
        "            stmt = st.get(\"statement\",\"\").strip()\n",
        "            ev   = st.get(\"evidence\",\"\").strip() or \"logical deduction\"\n",
        "            ver  = str(st.get(\"Verification\",\"true\")).lower()\n",
        "            if len(stmt) < 3 or (stmt,ev) in seen:\n",
        "                continue\n",
        "            seen.add((stmt,ev))\n",
        "            cleaned.append({\"statement\":stmt,\"evidence\":ev,\"Verification\":ver})\n",
        "        if cleaned:\n",
        "            cps_candidates.append(cleaned)\n",
        "\n",
        "    # fallback if nothing survived\n",
        "    if not cps_candidates:\n",
        "        cps_candidates = [[]]\n",
        "\n",
        "    # 3) Score each candidate with your CP verifier\n",
        "    premise = f\"Question:\\n{q}\\n\\nConditions:\\n\" + \"\\n\".join(f\"- {s}\" for s in best_qp) + f\"\\n\\nCoT:\\n{cot}\"\n",
        "    avg_scores = []\n",
        "    for cp_list in cps_candidates:\n",
        "        if not cp_list:\n",
        "            avg_scores.append(0.0)\n",
        "            continue\n",
        "        prems = [premise]*len(cp_list)\n",
        "        hyps  = [f\"Statement: {st['statement']}\\nBased on: {st['evidence']}\" for st in cp_list]\n",
        "        scores = score_verifier_batch(prems, hyps, cv_tok, cv_mod)  # list of prob (0–1)\n",
        "        # convert to log‐probs and sum\n",
        "        sum_logprob = sum(math.log(s + 1e-12) for s in scores)\n",
        "        avg_scores.append(sum_logprob)\n",
        "\n",
        "    # 4) Pick best candidate (with threshold)\n",
        "    best_idx = int(np.argmax(avg_scores))\n",
        "    best_cp = cps_candidates[best_idx]\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"question\": q_raw,\n",
        "        \"question_parsing\": best_qp,\n",
        "        \"answer\": ans,\n",
        "        \"id\": example[\"id\"],\n",
        "        \"cot\": cot_raw,\n",
        "        \"cot_parsing\": best_cp,\n",
        "        \"sel_idx\": sel_idx\n",
        "    }"
      ],
      "metadata": {
        "id": "T8YYGJboVHu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch and Run"
      ],
      "metadata": {
        "id": "IeCvk_ExVLpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(batch):\n",
        "    outs = [process_one({\n",
        "        \"question\": batch[\"question\"][i],\n",
        "        \"cot\":       batch[\"cot\"][i],\n",
        "        \"id\":        batch[\"id\"][i],\n",
        "        \"sel_idx\":   batch.get(\"sel_idx\", [None]*len(batch[\"id\"]))[i],\n",
        "        \"answer\":    batch.get(\"answer\", [None]*len(batch[\"id\"]))[i],\n",
        "    }) for i in range(len(batch[\"question\"]))]\n",
        "\n",
        "    return {\n",
        "        \"question\":        [o[\"question\"]        for o in outs],\n",
        "        \"question_parsing\":[o[\"question_parsing\"]for o in outs],\n",
        "        \"answer\":          [o[\"answer\"]          for o in outs],\n",
        "        \"id\":              [o[\"id\"]              for o in outs],\n",
        "        \"cot\":             [o[\"cot\"]             for o in outs],\n",
        "        \"cot_parsing\":     [o[\"cot_parsing\"]     for o in outs],\n",
        "        \"sel_idx\":         [o[\"sel_idx\"]         for o in outs],\n",
        "    }\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    gc.collect()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    ds = load_dataset(\"json\", data_files={\"test\": INPUT})[\"test\"]\n",
        "\n",
        "    out_ds = ds.map(\n",
        "        process_batch,\n",
        "        batched=True,\n",
        "        batch_size=2,\n",
        "        remove_columns=ds.column_names\n",
        "    )\n",
        "\n",
        "    out_ds.to_json(OUTPUT, orient=\"records\", lines=False)\n",
        "    print(\"✅ Done — saved to\", OUTPUT)"
      ],
      "metadata": {
        "id": "f8Bb_SioVOtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Predictions"
      ],
      "metadata": {
        "id": "DPJbweANVSDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "INPUT_PATH  = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach3.json\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/llm-sr-project/final_results_hybrid_approach3.json\"\n",
        "\n",
        "def transform_example(ex):\n",
        "    # reorder each cot_parsing entry: statement → evidence → Verification\n",
        "    reordered = []\n",
        "    for step in ex.get(\"cot_parsing\", []):\n",
        "        reordered.append({\n",
        "            \"statement\":    step.get(\"statement\"),\n",
        "            \"evidence\":     step.get(\"evidence\"),\n",
        "            \"Verification\": step.get(\"Verification\"),\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"question\":         ex.get(\"question\"),\n",
        "        \"question_parsing\": ex.get(\"question_parsing\"),\n",
        "        \"answer\":           ex.get(\"answer\"),\n",
        "        \"id\":               ex.get(\"id\"),\n",
        "        \"cot\":              ex.get(\"cot\"),\n",
        "        \"cot_parsing\":      reordered,\n",
        "        \"sel_idx\":          ex.get(\"sel_idx\"),\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        examples = json.load(f)\n",
        "\n",
        "    structured = [transform_example(ex) for ex in examples]\n",
        "\n",
        "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(structured, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Wrote {len(structured)} examples to {OUTPUT_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "gza4FHS4VUKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "iuinkhJHVV8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EVAL_SCRIPT = \"/content/drive/MyDrive/llm-sr-project/eval.py\"\n",
        "PREDICTION_PATH = \"/content/drive/MyDrive/llm-sr-project/final_results_hybrid_approach3.json\"\n",
        "REFERENCE_PATH = \"/content/drive/MyDrive/llm-sr-project/test-reference.json\"\n",
        "\n",
        "!python {EVAL_SCRIPT} \\\n",
        "  --prediction {PREDICTION_PATH} \\\n",
        "  --reference {REFERENCE_PATH} \\\n",
        "  --question_threshold 0.95 \\\n",
        "  --statement_threshold 0.9 \\\n",
        "  --relation_threshold 0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS7PHawSVYTU",
        "outputId": "ab592350-ad1c-4dd3-d68d-76f0eb05d84e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-17 17:43:56.451126: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747503836.508014    1004 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747503836.525028    1004 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-17 17:43:56.580788: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 1.05k/1.05k [00:00<00:00, 6.19MB/s]\n",
            "model.safetensors: 100% 738M/738M [00:21<00:00, 34.8MB/s]\n",
            "tokenizer_config.json: 100% 1.28k/1.28k [00:00<00:00, 7.53MB/s]\n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 74.8MB/s]\n",
            "tokenizer.json: 100% 8.66M/8.66M [00:00<00:00, 21.1MB/s]\n",
            "added_tokens.json: 100% 23.0/23.0 [00:00<00:00, 158kB/s]\n",
            "special_tokens_map.json: 100% 286/286 [00:00<00:00, 1.57MB/s]\n",
            "\u001b[?25lTotal number of predictions: \u001b[1;36m24\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0mAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  4%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  8%\u001b[0m \u001b[36m0:05:34\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 12%\u001b[0m \u001b[36m0:09:29\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 17%\u001b[0m \u001b[36m0:05:06\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 21%\u001b[0m \u001b[36m0:05:09\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 25%\u001b[0m \u001b[36m0:03:50\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 29%\u001b[0m \u001b[36m0:02:26\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 33%\u001b[0m \u001b[36m0:03:57\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 38%\u001b[0m \u001b[36m0:03:00\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 42%\u001b[0m \u001b[36m0:04:54\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 46%\u001b[0m \u001b[36m0:02:23\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 50%\u001b[0m \u001b[36m0:02:32\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 54%\u001b[0m \u001b[36m0:02:01\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 58%\u001b[0m \u001b[36m0:01:40\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[35m 62%\u001b[0m \u001b[36m0:01:25\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[35m 67%\u001b[0m \u001b[36m0:01:44\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[35m 71%\u001b[0m \u001b[36m0:02:19\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[35m 75%\u001b[0m \u001b[36m0:01:13\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[35m 79%\u001b[0m \u001b[36m0:00:44\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[35m 83%\u001b[0m \u001b[36m0:00:53\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[35m 88%\u001b[0m \u001b[36m0:00:35\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m 92%\u001b[0m \u001b[36m0:00:29\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[35m 96%\u001b[0m \u001b[36m0:00:15\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[3m           Evaluation Results           \u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
            "┃\u001b[35m \u001b[0m\u001b[35mMetric                     \u001b[0m\u001b[35m \u001b[0m┃\u001b[35m \u001b[0m\u001b[35mValue \u001b[0m\u001b[35m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
            "│ Question_Macro_F1           │ 0.7658 │\n",
            "│ Statement_Macro_F1          │ 0.4102 │\n",
            "│ Statement_Evidence_Macro_F1 │ 0.1711 │\n",
            "│ Reasoning_F1                │ 0.1231 │\n",
            "└─────────────────────────────┴────────┘\n",
            "Question_Macro_F1: 0.7658\n",
            "Statement_Macro_F1: 0.4102\n",
            "Statement_Evidence_Macro_F1: 0.1711\n",
            "Reasoning_F1: 0.1231\n"
          ]
        }
      ]
    }
  ]
}