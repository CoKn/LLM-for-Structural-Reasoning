{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Hybrid Inference Strategy 3: Enhanced CoT Cleaning & Evidence Boosting\n",
        "\n",
        "**Objective**  \n",
        "Improve Statement–Evidence alignment and consistency in CoT parsing by adding post-processing intelligence on top of the beam+sampled reranking pipeline:\n",
        "\n",
        "1. **Question Parsing (QP) Stage**  \n",
        "   - Same as v2: single deterministic JSON list from the LLaMA-3 QP model (beam-search).  \n",
        "   - No verifier at this stage.\n",
        "\n",
        "2. **Chain-of-Thought Parsing (CP) Stage**  \n",
        "   - Generate **5** parses per example (2 beam + 3 sampled) from the LLaMA-3 CP model.  \n",
        "   - Parse each into `statement`, `evidence`, `Verification`.\n",
        "\n",
        "3. **Post-Processing & Cleaning**  \n",
        "   - **Evidence Boosting**: if a step’s `evidence` is missing/too short, extract a better CoT sentence via keyword overlap.  \n",
        "   - **Verification Normalization**: force every `Verification` to exactly `\"true\"` or `\"false\"`.  \n",
        "   - **Support Check**: if `statement` affirms but `evidence` negates (or vice versa), mark `\"false\"`.  \n",
        "   - **Reasoning Trace**: append a `reasoning_step` index and optional `related_to` links between steps.\n",
        "\n",
        "4. **Verifier Reranking**  \n",
        "   - Score each cleaned candidate by summing **log-probs** of the CP verifier’s “true” outputs across its steps.  \n",
        "   - Select the candidate with the highest total log-prob (no explicit threshold fallback).\n",
        "\n",
        "5. **Output**  \n",
        "   - JSON record per example with keys:  \n",
        "     ```json\n",
        "     {\n",
        "       \"question\": …,\n",
        "       \"question_parsing\": …,\n",
        "       \"cot\": …,\n",
        "       \"cot_parsing\": …,      // enhanced & verified steps\n",
        "       \"answer\": …,\n",
        "       \"id\": …,\n",
        "       \"sel_idx\": …\n",
        "     }\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "| Metric                         | v3 Score |\n",
        "|--------------------------------|----------|\n",
        "| **Question_Macro_F1**          | 0.7658   |\n",
        "| **Statement_Macro_F1**         | 0.3990   |\n",
        "| **Statement_Evidence_Macro_F1**| 0.1831   |\n",
        "| **Reasoning_F1**               | 0.1129   |\n",
        "\n",
        "> _Strong evidence-alignment gains (+0.010 vs. v2) at a small cost to overall reasoning F1, thanks to stricter step filtering and normalization._\n"
      ],
      "metadata": {
        "id": "3M4KccYUW3Du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Thresholds"
      ],
      "metadata": {
        "id": "Y9d68k-UXB9d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xIt2JNCWdw-",
        "outputId": "a3fc5595-3c86-410b-fad4-ec7d182adb6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting json5\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Downloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: json5\n",
            "Successfully installed json5-0.12.0\n",
            "Found existing installation: nltk 3.9.1\n",
            "Uninstalling nltk-3.9.1:\n",
            "  Successfully uninstalled nltk-3.9.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install core evaluation utilities\n",
        "!pip install -q evaluate\n",
        "!pip install json5\n",
        "\n",
        "!pip uninstall -y nltk\n",
        "!pip install -q --upgrade nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DinVXVBXH-1",
        "outputId": "04bd747e-068a-4ea6-b563-f3c4ecb14258"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth  # Must come first for 4-bit LoRA\n",
        "import torch, gc, json, re, ast, html, numpy as np\n",
        "from torch.nn.functional import log_softmax\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# Paths & thresholds\n",
        "INPUT       = \"/content/drive/MyDrive/llm-sr-project/testingData-blank.json\"\n",
        "OUTPUT      = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach4.json\"\n",
        "QP_LM_PATH  = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_question_parsing\"\n",
        "CP_LM_PATH  = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_cot_parsing\"\n",
        "QP_VER_PATH = \"/content/drive/MyDrive/deberta-qparse-verifier\"\n",
        "CP_VER_PATH = \"/content/drive/MyDrive/deberta-cotparse-verifier\"\n",
        "\n",
        "\n",
        "THR_QP = 0.75\n",
        "THR_CP = 0.60\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "T1ISLIxuXJqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "R5TrGoHuXQWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In-Context Learning (ICL) Prompts\n",
        "\n",
        "QP_DEMON = '''The question is:\n",
        "\n",
        "There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project. This assignment must satisfy:\n",
        "(1) If A works on Alpha, then B works on Beta.\n",
        "(2) If C works on Alpha, then D and E work on Beta.\n",
        "(3) F works on a different project than E.\n",
        "(4) D must work on a different project than A.\n",
        "(5) If F works on Alpha, then B works on Alpha.\n",
        "\n",
        "If A works on Beta, which of the following must be true?\n",
        "A. B works on Alpha\n",
        "B. C works on Beta\n",
        "C. D works on Alpha\n",
        "D. F works on Beta\n",
        "\n",
        "The parsing result is:\n",
        "\n",
        "[\n",
        "  \"There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project.\",\n",
        "  \"If A works on Alpha, then B works on Beta\",\n",
        "  \"If C works on Alpha, then D and E work on Beta\",\n",
        "  \"F works on a different project than E\",\n",
        "  \"D must work on a different project than A\",\n",
        "  \"If F works on Alpha, then B works on Alpha\",\n",
        "  \"A works on Beta\"\n",
        "]\n",
        "'''\n",
        "\n",
        "\n",
        "QP_TEMPLATE = '''Given a question, extract all relevant information from the question that would help to solve it.\n",
        "\n",
        "This includes:\n",
        "- General setup information (e.g., number of people, projects involved)\n",
        "- Explicit facts given in the question\n",
        "- All logical constraints or conditions\n",
        "\n",
        "Output only a JSON list and nothing else. Follow the format shown in the example.\n",
        "\n",
        "Example:\n",
        "\n",
        "{demon}\n",
        "\n",
        "Now, the question is:\n",
        "\n",
        "{question}\n",
        "\n",
        "Your output:\n",
        "'''\n",
        "\n",
        "CP_DEMON = '''The question is:\n",
        "\n",
        "There are 6 volunteers: A, B, C, D, E and F. Each person works on exactly one project.\n",
        "\n",
        "Conditions:\n",
        "(1) If A works on Alpha, then B works on Beta.\n",
        "(2) If C works on Alpha, then D and E work on Beta.\n",
        "(3) F works on a different project than E.\n",
        "(4) D must work on a different project than A.\n",
        "(5) If F works on Alpha, then B works on Alpha.\n",
        "\n",
        "Question:\n",
        "If A works on Beta, which of the following must be true?\n",
        "\n",
        "CoT:\n",
        "Since A works on Beta, Condition (1) is not triggered. Condition (2) is not triggered since C's assignment is unknown. Condition (3) doesn't give anything because E's assignment is unspecified. Condition (4) says D must work on a different project than A, so D must work on Alpha. Condition (5) depends on F, which is unknown.\n",
        "\n",
        "Parsing result:\n",
        "\n",
        "[\n",
        "  {\n",
        "    \"statement\": \"Condition (1) is not applicable\",\n",
        "    \"evidence\": \"Condition (1): If A works on Alpha, then B works on Beta. | A is working on Beta\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (2) is not applicable\",\n",
        "    \"evidence\": \"Condition (2): If C works on Alpha, then D and E work on Beta. | C's assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (3) does not provide any info\",\n",
        "    \"evidence\": \"Condition (3): F works on a different project than E. | E's assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"D must work on Alpha\",\n",
        "    \"evidence\": \"Condition (4): D must work on a different project than A, and A is working on Beta\",\n",
        "    \"Verification\": \"true\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (5) is not applicable\",\n",
        "    \"evidence\": \"Condition (5): If F works on Alpha, then B works on Alpha. | F's assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  }\n",
        "]\n",
        "'''\n",
        "\n",
        "CP_TEMPLATE = '''You are a reasoning assistant. Based on the question, conditions, and chain-of-thought (CoT), extract every inference or non-inference step as a JSON object.\n",
        "\n",
        "For each CoT sentence that either:\n",
        "  1. Refers to a condition (e.g. \"Condition (2) …\")\n",
        "  2. Starts with an inference cue (\"Since\", \"Therefore\", \"This means\", \"We can deduce\", etc.)\n",
        "\n",
        "Produce one object with:\n",
        "  • \"statement\": the new claim you read in that CoT sentence (don't quote the entire sentence—just the core inference).\n",
        "  • \"evidence\":\n",
        "      – if the claim restates a constraint, use the exact line from the **Conditions** block,\n",
        "      – otherwise, use the CoT fragment that you extracted it from.\n",
        "  • \"Verification\":\n",
        "      – MUST BE EXACTLY `\"false\"` if the sentence rejects or blocks a condition (contains \"not applicable\", \"does not provide\", etc.),\n",
        "      – MUST BE EXACTLY `\"true\"` in all other cases.\n",
        "\n",
        "Keep the objects in the same order as they appear in the CoT.\n",
        "\n",
        "IMPORTANT: \"Verification\" field MUST ONLY contain the string \"true\" or \"false\" (lowercase) and nothing else.\n",
        "\n",
        "Example:\n",
        "\n",
        "{demon}\n",
        "\n",
        "Now, given:\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Conditions:\n",
        "{conditions}\n",
        "\n",
        "Chain-of-Thought:\n",
        "{cot}\n",
        "\n",
        "Your output:\n",
        "'''"
      ],
      "metadata": {
        "id": "R-5YUSzHXSaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "Tifx_hTIXXWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_quotes(t):\n",
        "    return (t.replace('\"','\"').replace('\"','\"').replace(\"'\",\"'\").replace(\"'\",\"'\"))\n",
        "\n",
        "def normalize_text(t):\n",
        "    t = clean_quotes(t)\n",
        "    t = re.sub(r'\\?\\s(?=[A-Z])', ', ', t)\n",
        "    t = re.sub(r'(?<=[a-zA-Z])\\.(?=[A-Z])', '. ', t)\n",
        "    t = re.sub(r'(?<![A-Da-d])\\\\n(?!\\s?[A-Da-d]\\\\.)', ' ', t)\n",
        "    return html.unescape(t).strip()\n",
        "\n",
        "def extract_json(raw):\n",
        "    raw = raw.strip()\n",
        "    i = raw.find('[')\n",
        "    if i < 0: return []\n",
        "    depth = 0\n",
        "    for j,ch in enumerate(raw[i:], i):\n",
        "        if ch=='[': depth+=1\n",
        "        elif ch==']': depth-=1\n",
        "        if depth==0:\n",
        "            blk = raw[i:j+1]\n",
        "            for p in (json.loads, ast.literal_eval):\n",
        "                try: return p(blk)\n",
        "                except: pass\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "def score_verifier_batch(prem_list, hyp_list, tok, mod):\n",
        "    enc = tok(prem_list, hyp_list, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = mod(**enc).logits\n",
        "    return torch.softmax(logits, dim=1)[:, 1].tolist()\n",
        "\n",
        "def clean_qp(qp_list):\n",
        "    return [s for s in qp_list if not re.match(r'^[A-Da-d][\\.:\\)]', s.strip()) and \"Option\" not in s and \"following\" not in s]\n",
        "\n",
        "# New function to validate and enhance evidence quality\n",
        "def enhance_evidence(statement, evidence, cot, question):\n",
        "    if not evidence or len(evidence.strip()) < 15:\n",
        "        # Extract evidence from CoT if missing or too short\n",
        "        statement_kw = set(re.findall(r'\\b[a-zA-Z]{3,}\\b', statement.lower()))\n",
        "        sentences = re.split(r'[.!?]', cot)\n",
        "\n",
        "        # Find a better evidence sentence based on keyword overlap\n",
        "        best_score = 0\n",
        "        best_sent = \"\"\n",
        "        for sent in sentences:\n",
        "            sent = sent.strip()\n",
        "            if len(sent) < 10:\n",
        "                continue\n",
        "            sent_kw = set(re.findall(r'\\b[a-zA-Z]{3,}\\b', sent.lower()))\n",
        "            overlap = len(statement_kw.intersection(sent_kw))\n",
        "            if overlap > best_score:\n",
        "                best_score = overlap\n",
        "                best_sent = sent\n",
        "\n",
        "        if best_score >= 2:\n",
        "            return best_sent + \".\"\n",
        "\n",
        "    return evidence\n",
        "\n",
        "# New function to ensure verification is strictly \"true\" or \"false\"\n",
        "def normalize_verification(verification):\n",
        "    if not verification or verification is None:\n",
        "        return \"true\"  # Default to true\n",
        "\n",
        "    verification = verification.lower().strip()\n",
        "\n",
        "    # Detect negation patterns that indicate false\n",
        "    if any(neg in verification for neg in [\"false\", \"not\", \"cannot\", \"doesn't\", \"unlikely\", \"invalid\", \"incorrect\"]):\n",
        "        return \"false\"\n",
        "\n",
        "    # Default to true for all other cases\n",
        "    return \"true\"\n",
        "\n",
        "# New function to verify statement is properly supported by evidence\n",
        "def verify_statement_evidence(statement, evidence):\n",
        "    if not statement or not evidence:\n",
        "        return \"false\"\n",
        "\n",
        "    statement_kw = set(re.findall(r'\\b[a-zA-Z]{3,}\\b', statement.lower()))\n",
        "    evidence_kw = set(re.findall(r'\\b[a-zA-Z]{3,}\\b', evidence.lower()))\n",
        "\n",
        "    # Check for strong evidence support - keyword overlap\n",
        "    overlap = len(statement_kw.intersection(evidence_kw))\n",
        "\n",
        "    # Check for contradictions\n",
        "    negations = [\"not\", \"cannot\", \"doesn't\", \"don't\", \"isn't\", \"aren't\", \"won't\", \"wouldn't\"]\n",
        "\n",
        "    # If statement affirms but evidence negates, mark as false\n",
        "    statement_affirms = all(neg not in statement.lower().split() for neg in negations)\n",
        "    evidence_negates = any(neg in evidence.lower().split() for neg in negations)\n",
        "\n",
        "    if statement_affirms and evidence_negates:\n",
        "        return \"false\"\n",
        "\n",
        "    # If good overlap and no contradictions, mark as true\n",
        "    if overlap >= 2:\n",
        "        return \"true\"\n",
        "\n",
        "    return \"false\""
      ],
      "metadata": {
        "id": "5aaYMeeVXain"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Models and Verifiers"
      ],
      "metadata": {
        "id": "4h83aPYwXgPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QP LM - Using beam search\n",
        "qp_tok = AutoTokenizer.from_pretrained(QP_LM_PATH)\n",
        "qp_tok.model_max_length = 1024\n",
        "qp_mod = AutoModelForCausalLM.from_pretrained(QP_LM_PATH).to(device)\n",
        "qp_pipe = pipeline(\"text-generation\", model=qp_mod, tokenizer=qp_tok,\n",
        "                   return_full_text=False, do_sample=False,\n",
        "                   num_beams=5, early_stopping=True,\n",
        "                   max_new_tokens=512, batch_size=4)\n",
        "\n",
        "# CP LM - Using beam search\n",
        "cp_tok = AutoTokenizer.from_pretrained(CP_LM_PATH)\n",
        "cp_tok.model_max_length = 2048\n",
        "cp_mod = AutoModelForCausalLM.from_pretrained(CP_LM_PATH).to(device)\n",
        "\n",
        "# Generate 2 beam candidates + 3 sampled candidates = 5 total\n",
        "cp_pipe = pipeline(\n",
        "     \"text-generation\",\n",
        "     model=cp_mod, tokenizer=cp_tok,\n",
        "     return_full_text=False,\n",
        "     do_sample=False, num_beams=5, num_return_sequences=2,\n",
        "     max_new_tokens=1024,\n",
        "     batch_size=4\n",
        ")\n",
        "\n",
        "cp_sampler = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=cp_mod, tokenizer=cp_tok,\n",
        "    return_full_text=False,\n",
        "     do_sample=True, temperature=0.8, num_return_sequences=3,\n",
        "     max_new_tokens=1024,\n",
        "     batch_size=4\n",
        ")\n",
        "\n",
        "# Load Verifiers\n",
        "def load_verifiers():\n",
        "    global qv_tok, qv_mod, cv_tok, cv_mod\n",
        "    qv_tok = AutoTokenizer.from_pretrained(QP_VER_PATH)\n",
        "    qv_mod = AutoModelForSequenceClassification.from_pretrained(QP_VER_PATH).to(device)\n",
        "    cv_tok = AutoTokenizer.from_pretrained(CP_VER_PATH)\n",
        "    cv_mod = AutoModelForSequenceClassification.from_pretrained(CP_VER_PATH).to(device)\n",
        "    return qv_tok, qv_mod, cv_tok, cv_mod\n",
        "\n",
        "qv_tok, qv_mod, cv_tok, cv_mod = load_verifiers()"
      ],
      "metadata": {
        "id": "vFT1rixGXkym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Inference Function"
      ],
      "metadata": {
        "id": "Y6qqnGhiXnuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_one(example):\n",
        "    q_raw, cot_raw = example[\"question\"], example[\"cot\"]\n",
        "    sel_idx, ans = example.get(\"sel_idx\"), example.get(\"answer\")\n",
        "    q, cot = normalize_text(q_raw), normalize_text(cot_raw)\n",
        "\n",
        "    # Single deterministic QP output\n",
        "    prompt = QP_TEMPLATE.format(demon=QP_DEMON, question=q)\n",
        "    raw_qp = qp_pipe(prompt, max_new_tokens=512)\n",
        "    if not isinstance(raw_qp, list):\n",
        "        raw_qp = [raw_qp]\n",
        "    best_qp = clean_qp(extract_json(raw_qp[0][\"generated_text\"]))\n",
        "\n",
        "    # 2) CP: generate 3 beam-search parses\n",
        "    conds_str = json.dumps(best_qp, ensure_ascii=False)\n",
        "    prompt_cp = CP_TEMPLATE.format(\n",
        "        demon      = CP_DEMON,\n",
        "        question   = q,\n",
        "        conditions = conds_str,\n",
        "        cot        = cot\n",
        "    )\n",
        "\n",
        "    # 2.a) get 5 raw outputs: 2 from beam, 3 from sampler\n",
        "    raw_beams = cp_pipe(prompt_cp, max_new_tokens=1024)\n",
        "    raw_samples = cp_sampler(prompt_cp, max_new_tokens=1024)\n",
        "    raw_cp_outs = (raw_beams if isinstance(raw_beams, list) else [raw_beams]) \\\n",
        "            + (raw_samples if isinstance(raw_samples, list) else [raw_samples])\n",
        "\n",
        "    # flatten HF's list-of-lists (if any)\n",
        "    raw_cp_flat = [\n",
        "        item\n",
        "        for sub in raw_cp_outs\n",
        "        for item in (sub if isinstance(sub, list) else [sub])\n",
        "    ]\n",
        "\n",
        "    # 2.b) parse & clean each candidate\n",
        "    cps_candidates = []\n",
        "    for out in raw_cp_flat:\n",
        "        parsed = extract_json(out[\"generated_text\"])\n",
        "        if not parsed:\n",
        "            continue\n",
        "        seen = set()\n",
        "        cleaned = []\n",
        "        for st in parsed:\n",
        "            # Fix: Add type checking to handle unexpected objects\n",
        "            if not isinstance(st, dict):\n",
        "                continue\n",
        "\n",
        "            stmt = st.get(\"statement\",\"\").strip()\n",
        "            ev   = st.get(\"evidence\",\"\").strip() or \"logical deduction\"\n",
        "\n",
        "            # Enhance evidence quality if needed\n",
        "            ev = enhance_evidence(stmt, ev, cot, q)\n",
        "\n",
        "            # Normalize verification to strictly \"true\" or \"false\"\n",
        "            ver_orig = st.get(\"Verification\",\"true\")\n",
        "            ver = normalize_verification(ver_orig)\n",
        "\n",
        "            # Check if statement is properly supported by evidence\n",
        "            if ver == \"true\":\n",
        "                ver = verify_statement_evidence(stmt, ev)\n",
        "\n",
        "            if len(stmt) < 3 or (stmt,ev) in seen:\n",
        "                continue\n",
        "\n",
        "            seen.add((stmt,ev))\n",
        "            cleaned.append({\"statement\":stmt,\"evidence\":ev,\"Verification\":ver})\n",
        "\n",
        "        if cleaned:\n",
        "            cps_candidates.append(cleaned)\n",
        "\n",
        "    # fallback if nothing survived\n",
        "    if not cps_candidates:\n",
        "        cps_candidates = [[]]\n",
        "\n",
        "    # 3) Score each candidate with your CP verifier\n",
        "    premise = f\"Question:\\n{q}\\n\\nConditions:\\n\" + \"\\n\".join(f\"- {s}\" for s in best_qp) + f\"\\n\\nCoT:\\n{cot}\"\n",
        "    avg_scores = []\n",
        "    for cp_list in cps_candidates:\n",
        "        if not cp_list:\n",
        "            avg_scores.append(0.0)\n",
        "            continue\n",
        "        prems = [premise]*len(cp_list)\n",
        "        hyps  = [f\"Statement: {st['statement']}\\nBased on: {st['evidence']}\" for st in cp_list]\n",
        "        scores = score_verifier_batch(prems, hyps, cv_tok, cv_mod)  # list of prob (0–1)\n",
        "        # convert to log‐probs and sum\n",
        "        sum_logprob = sum(math.log(s + 1e-12) for s in scores)\n",
        "        avg_scores.append(sum_logprob)\n",
        "\n",
        "    # 4) Pick best candidate (with threshold)\n",
        "    best_idx = int(np.argmax(avg_scores)) if avg_scores else 0\n",
        "    best_cp = cps_candidates[best_idx] if cps_candidates and best_idx < len(cps_candidates) else []\n",
        "\n",
        "    # Final post-processing to ensure all verifications are strictly \"true\" or \"false\"\n",
        "    for item in best_cp:\n",
        "        if item[\"Verification\"] not in [\"true\", \"false\"]:\n",
        "            item[\"Verification\"] = normalize_verification(item[\"Verification\"])\n",
        "\n",
        "    # Add reasoning steps explicitly\n",
        "    for i, step in enumerate(best_cp):\n",
        "        step[\"reasoning_step\"] = i + 1\n",
        "\n",
        "        # Add references to previous steps when possible\n",
        "        if i > 0:\n",
        "            curr_keywords = set(re.findall(r'\\b[a-zA-Z]{3,}\\b', step[\"statement\"].lower()))\n",
        "            references = []\n",
        "\n",
        "            for j in range(i):\n",
        "                prev_keywords = set(re.findall(r'\\b[a-zA-Z]{3,}\\b', best_cp[j][\"statement\"].lower()))\n",
        "                if len(curr_keywords.intersection(prev_keywords)) >= 2:\n",
        "                    references.append(j + 1)\n",
        "\n",
        "            if references:\n",
        "                step[\"related_to\"] = references\n",
        "\n",
        "    return {\n",
        "        \"question\": q_raw,\n",
        "        \"question_parsing\": best_qp,\n",
        "        \"answer\": ans,\n",
        "        \"id\": example[\"id\"],\n",
        "        \"cot\": cot_raw,\n",
        "        \"cot_parsing\": best_cp,\n",
        "        \"sel_idx\": sel_idx\n",
        "    }"
      ],
      "metadata": {
        "id": "RMU_jj7vXqDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch and Run"
      ],
      "metadata": {
        "id": "I0woS_efXxyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(batch):\n",
        "    outs = [process_one({\n",
        "        \"question\": batch[\"question\"][i],\n",
        "        \"cot\":       batch[\"cot\"][i],\n",
        "        \"id\":        batch[\"id\"][i],\n",
        "        \"sel_idx\":   batch.get(\"sel_idx\", [None]*len(batch[\"id\"]))[i],\n",
        "        \"answer\":    batch.get(\"answer\", [None]*len(batch[\"id\"]))[i],\n",
        "    }) for i in range(len(batch[\"question\"]))]\n",
        "\n",
        "    return {\n",
        "        \"question\":        [o[\"question\"]        for o in outs],\n",
        "        \"question_parsing\":[o[\"question_parsing\"]for o in outs],\n",
        "        \"answer\":          [o[\"answer\"]          for o in outs],\n",
        "        \"id\":              [o[\"id\"]              for o in outs],\n",
        "        \"cot\":             [o[\"cot\"]             for o in outs],\n",
        "        \"cot_parsing\":     [o[\"cot_parsing\"]     for o in outs],\n",
        "        \"sel_idx\":         [o[\"sel_idx\"]         for o in outs],\n",
        "    }\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    gc.collect()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        ds = load_dataset(\"json\", data_files={\"test\": INPUT})[\"test\"]\n",
        "        print(f\"Loaded dataset with {len(ds)} examples\")\n",
        "\n",
        "        # Add more descriptive logging\n",
        "        print(\"Starting batch processing...\")\n",
        "        out_ds = ds.map(\n",
        "            process_batch,\n",
        "            batched=True,\n",
        "            batch_size=2,\n",
        "            remove_columns=ds.column_names\n",
        "        )\n",
        "\n",
        "        print(f\"Processing complete. Writing results to {OUTPUT}\")\n",
        "        out_ds.to_json(OUTPUT, orient=\"records\", lines=False)\n",
        "        print(\"✅ Done — saved to\", OUTPUT)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {type(e).__name__}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "bwn2I-wVX1hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Predictions"
      ],
      "metadata": {
        "id": "h5aOALiKX47O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "INPUT_PATH  = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach4.json\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/llm-sr-project/final_results_hybrid_approach4.json\"\n",
        "\n",
        "def transform_example(ex):\n",
        "    # reorder each cot_parsing entry: statement → evidence → Verification\n",
        "    reordered = []\n",
        "    for step in ex.get(\"cot_parsing\", []):\n",
        "        reordered.append({\n",
        "            \"statement\":    step.get(\"statement\"),\n",
        "            \"evidence\":     step.get(\"evidence\"),\n",
        "            \"Verification\": step.get(\"Verification\"),\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"question\":         ex.get(\"question\"),\n",
        "        \"question_parsing\": ex.get(\"question_parsing\"),\n",
        "        \"answer\":           ex.get(\"answer\"),\n",
        "        \"id\":               ex.get(\"id\"),\n",
        "        \"cot\":              ex.get(\"cot\"),\n",
        "        \"cot_parsing\":      reordered,\n",
        "        \"sel_idx\":          ex.get(\"sel_idx\"),\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        examples = json.load(f)\n",
        "\n",
        "    structured = [transform_example(ex) for ex in examples]\n",
        "\n",
        "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(structured, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Wrote {len(structured)} examples to {OUTPUT_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "40LaboqZX4T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "vNsoBPF4X-Gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EVAL_SCRIPT = \"/content/drive/MyDrive/llm-sr-project/eval.py\"\n",
        "PREDICTION_PATH = \"/content/drive/MyDrive/llm-sr-project/final_results_hybrid_approach4.json\"\n",
        "REFERENCE_PATH = \"/content/drive/MyDrive/llm-sr-project/test-reference.json\"\n",
        "\n",
        "!python {EVAL_SCRIPT} \\\n",
        "  --prediction {PREDICTION_PATH} \\\n",
        "  --reference {REFERENCE_PATH} \\\n",
        "  --question_threshold 0.95 \\\n",
        "  --statement_threshold 0.9 \\\n",
        "  --relation_threshold 0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YdlVfkUX9f8",
        "outputId": "dcfa059e-01c6-414a-a107-00bb3fdc6c8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-17 17:55:11.330251: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-17 17:55:11.347705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747504511.369627    1366 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747504511.376187    1366 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-17 17:55:11.397483: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 1.05k/1.05k [00:00<00:00, 9.33MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 738M/738M [00:03<00:00, 237MB/s]\n",
            "tokenizer_config.json: 100% 1.28k/1.28k [00:00<00:00, 12.6MB/s]\n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 32.2MB/s]\n",
            "tokenizer.json: 100% 8.66M/8.66M [00:00<00:00, 34.2MB/s]\n",
            "added_tokens.json: 100% 23.0/23.0 [00:00<00:00, 262kB/s]\n",
            "special_tokens_map.json: 100% 286/286 [00:00<00:00, 2.63MB/s]\n",
            "\u001b[?25lTotal number of predictions: \u001b[1;36m24\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0mAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  4%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  8%\u001b[0m \u001b[36m0:01:06\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 12%\u001b[0m \u001b[36m0:01:10\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 17%\u001b[0m \u001b[36m0:00:57\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 21%\u001b[0m \u001b[36m0:00:52\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 25%\u001b[0m \u001b[36m0:00:44\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 29%\u001b[0m \u001b[36m0:00:38\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 33%\u001b[0m \u001b[36m0:00:40\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 38%\u001b[0m \u001b[36m0:00:37\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 42%\u001b[0m \u001b[36m0:00:36\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 46%\u001b[0m \u001b[36m0:00:32\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 50%\u001b[0m \u001b[36m0:00:30\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 54%\u001b[0m \u001b[36m0:00:26\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 58%\u001b[0m \u001b[36m0:00:23\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[35m 62%\u001b[0m \u001b[36m0:00:21\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[35m 67%\u001b[0m \u001b[36m0:00:17\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[35m 71%\u001b[0m \u001b[36m0:00:16\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[35m 75%\u001b[0m \u001b[36m0:00:13\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[35m 79%\u001b[0m \u001b[36m0:00:11\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[35m 83%\u001b[0m \u001b[36m0:00:09\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[35m 88%\u001b[0m \u001b[36m0:00:07\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m 92%\u001b[0m \u001b[36m0:00:05\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[35m 96%\u001b[0m \u001b[36m0:00:02\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[3m           Evaluation Results           \u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
            "┃\u001b[35m \u001b[0m\u001b[35mMetric                     \u001b[0m\u001b[35m \u001b[0m┃\u001b[35m \u001b[0m\u001b[35mValue \u001b[0m\u001b[35m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
            "│ Question_Macro_F1           │ 0.7658 │\n",
            "│ Statement_Macro_F1          │ 0.399  │\n",
            "│ Statement_Evidence_Macro_F1 │ 0.1831 │\n",
            "│ Reasoning_F1                │ 0.1129 │\n",
            "└─────────────────────────────┴────────┘\n",
            "Question_Macro_F1: 0.7658\n",
            "Statement_Macro_F1: 0.399\n",
            "Statement_Evidence_Macro_F1: 0.1831\n",
            "Reasoning_F1: 0.1129\n"
          ]
        }
      ]
    }
  ]
}