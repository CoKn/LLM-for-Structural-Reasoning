{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11328925,"sourceType":"datasetVersion","datasetId":7086714},{"sourceId":11329386,"sourceType":"datasetVersion","datasetId":7086995},{"sourceId":11345969,"sourceType":"datasetVersion","datasetId":7099191},{"sourceId":11346262,"sourceType":"datasetVersion","datasetId":7099405},{"sourceId":11346288,"sourceType":"datasetVersion","datasetId":7099426}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Converting Data for Fine-Tuning","metadata":{}},{"cell_type":"code","source":"import json\n\n\nwith open(\"/kaggle/input/trainingdata/Final_Selection_Train_v2.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n\nquestion_parsing_entries = []\ncot_parsing_entries = []\n\n\nfor example in data:\n    question = example[\"question\"]\n    cot = example[\"cot\"]\n    qparse = example[\"question_parsing\"]\n    cotparse = example[\"cot_parsing\"]\n\n  \n    q_output = \"Question Parsing:\\n\" + \"\\n\".join(f\"{i+1}. {line}\" for i, line in enumerate(qparse))\n    question_parsing_entries.append({\n        \"input\": f\"Question:\\n{question}\",\n        \"output\": q_output\n    })\n\n   \n    cot_output_lines = []\n    for entry in cotparse:\n        statement = entry[\"statement\"]\n        evidence = entry[\"evidence\"]\n        verification = entry[\"Verification\"]\n        cot_output_lines.append(\n            f\"Statement: {statement}\\nEvidence: {evidence}\\nVerification: {verification}\"\n        )\n    cot_output = \"CoT Parsing:\\n\" + \"\\n\\n\".join(cot_output_lines)\n    cot_parsing_entries.append({\n        \"input\": f\"Question:\\n{question}\\n\\nCoT:\\n{cot}\",\n        \"output\": cot_output\n    })\n\n\nwith open(\"/kaggle/working/train_question_parsing.jsonl\", \"w\", encoding=\"utf-8\") as f:\n    for item in question_parsing_entries:\n        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n\nwith open(\"/kaggle/working/train_cot_parsing.jsonl\", \"w\", encoding=\"utf-8\") as f:\n    for item in cot_parsing_entries:\n        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n\nprint(\"✅ Files generated: train_question_parsing.jsonl and train_cot_parsing.jsonl\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y nltk","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade nltk","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PART 1: Fine-tune LLaMA-3-8B-Instruct (Question Parsing) using Unsloth\n","metadata":{}},{"cell_type":"code","source":"\n!pip uninstall -y transformers unsloth unsloth-zoo\n\n\n!pip install transformers==4.51.1  # Required by Unsloth 2025.3.19\n!pip install unsloth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --force-reinstall numpy==1.26.4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import unsloth \nfrom unsloth import FastLanguageModel\n\nimport torch\nfrom transformers import pipeline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    max_seq_length = 2048,\n    dtype = torch.float16,\n    load_in_4bit = True,\n)\n\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=64,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    use_gradient_checkpointing=False,\n    random_state=42,\n    max_seq_length=2048,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files=\"/kaggle/working/train_question_parsing.jsonl\", split=\"train\")\n\n# Check a sample\nprint(dataset[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format(example_batch):\n    inputs = [\n        f\"{inp}\\n\\n{out}\" for inp, out in zip(example_batch[\"input\"], example_batch[\"output\"])\n    ]\n    tokenized = tokenizer(\n        inputs,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=2048,\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\n\ntokenized_dataset = dataset.map(format, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/llama3-question-parser\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    num_train_epochs=5,\n    logging_steps=5,\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    report_to=\"none\",\n    bf16=False,\n    fp16=True,  \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    train_dataset=tokenized_dataset,\n    args=training_args,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/finetuned_llama3_question_parsing\")\ntokenizer.save_pretrained(\"/kaggle/working/finetuned_llama3_question_parsing\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n\noutput_dir = \"/kaggle/working/finetuned_llama3_question_parsing\"\n\n\nshutil.make_archive(output_dir, 'zip', output_dir)\n\nprint(f\"✅ Question Parsing Model saved and zipped at {output_dir}.zip\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PART 2: Fine-tune LLaMA-3-8B-Instruct for cot_parsing","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ncot_dataset = load_dataset(\"json\", data_files=\"/kaggle/working/train_cot_parsing.jsonl\", split=\"train\")\n\nprint(cot_dataset[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    max_seq_length = 2048, \n    dtype = torch.float16,\n    load_in_4bit = True,\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=64,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    use_gradient_checkpointing=False,\n    random_state=42,\n    max_seq_length=2048,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_cot(example_batch):\n    inputs = [\n        f\"{question}\\n\\n{cot}\" \n        for question, cot in zip(example_batch[\"input\"], example_batch[\"output\"])\n    ]\n\n    model_inputs = tokenizer(\n        inputs,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=2048,\n        return_tensors=\"pt\",  \n    )\n\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n\n   \n    for key in model_inputs:\n        model_inputs[key] = model_inputs[key].to(\"cuda\")\n\n    return model_inputs\n\n\ntokenized_cot_dataset = cot_dataset.map(format_cot, batched=True)\ntokenized_cot_dataset.set_format(type=\"torch\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ncot_training_args = TrainingArguments(\n    output_dir=\"/kaggle/working/llama3-cot-parser\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=2,\n    gradient_checkpointing=False,\n    learning_rate=2e-5,\n    num_train_epochs=5,\n    logging_steps=5,\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    report_to=\"none\",\n    bf16=False,\n    fp16=True,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer\n\ncot_trainer = Trainer(\n    model=model,\n    args=cot_training_args,\n    train_dataset=tokenized_cot_dataset,\n    tokenizer=tokenizer,  \n\ncot_trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/finetuned_llama3_cot_parsing\")\ntokenizer.save_pretrained(\"/kaggle/working/finetuned_llama3_cot_parsing\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n\ncot_output_dir = \"/kaggle/working/finetuned_llama3_cot_parsing\"\n\n\nshutil.make_archive(cot_output_dir, 'zip', cot_output_dir)\n\nprint(f\"✅ CoT parsing model zipped at {cot_output_dir}.zip\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference Pipeline for results.json","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport json\n\n\nquestion_model_path = \"/kaggle/working/finetuned_llama3_question_parsing\"\nquestion_tokenizer = AutoTokenizer.from_pretrained(question_model_path)\nquestion_model = AutoModelForCausalLM.from_pretrained(question_model_path)\nquestion_pipe = pipeline(\"text-generation\", model=question_model, tokenizer=question_tokenizer)\n\n\ncot_model_path = \"/kaggle/working/finetuned_llama3_cot_parsing\"\ncot_tokenizer = AutoTokenizer.from_pretrained(cot_model_path)\ncot_model = AutoModelForCausalLM.from_pretrained(cot_model_path)\ncot_pipe = pipeline(\"text-generation\", model=cot_model, tokenizer=cot_tokenizer)\n\n\ndef generate_question_parsing(question_text):\n    prompt = f\"Question:\\n{question_text}\\n\\nQuestion Parsing:\\n\"\n    response = question_pipe(prompt, max_new_tokens=512, temperature=0.7)[0]['generated_text']\n    parsed = response.split(\"Question Parsing:\\n\")[-1].strip()\n    return [line.strip(\"1234567890. -\") for line in parsed.split(\"\\n\") if line.strip()]\n\ndef generate_cot_parsing(question_text, cot_text):\n    prompt = f\"\"\"You are a reasoning parser. Given a multiple-choice question and a chain-of-thought explanation (CoT), extract the reasoning steps into a structured JSON list. \n\nEach step must include:\n- \"statement\": the reasoning claim\n- \"evidence\": what justifies the claim\n- \"Verification\": \"true\" or \"false\"\n\nExample:\n\nQuestion:\nIf the lamp is on, the room is bright. The room is not bright.\nIs the lamp on?\n\nCoT:\nIf the lamp is on, the room is bright. The room is not bright. Therefore, the lamp is not on.\n\nCoT Parsing:\n[\n  {{\n    \"statement\": \"If the lamp is on, the room is bright.\",\n    \"evidence\": \"Provided in the question.\",\n    \"Verification\": \"true\"\n  }},\n  {{\n    \"statement\": \"The room is not bright.\",\n    \"evidence\": \"Given directly in the question.\",\n    \"Verification\": \"true\"\n  }},\n  {{\n    \"statement\": \"Therefore, the lamp is not on.\",\n    \"evidence\": \"Contrapositive of the conditional statement.\",\n    \"Verification\": \"true\"\n  }}\n]\n\nNow do the same for:\n\nQuestion:\n{question_text}\n\nCoT:\n{cot_text}\n\nCoT Parsing:\"\"\"\n\n    response = cot_pipe(prompt, max_new_tokens=1024, temperature=0.7)[0][\"generated_text\"]\n    parsed = response.split(\"CoT Parsing:\")[-1].strip()\n\n    try:\n    \n        cot_entries = json.loads(parsed)\n\n        cot_entries = [e for e in cot_entries if all(k in e for k in [\"statement\", \"evidence\", \"Verification\"]) and e[\"Verification\"] in (\"true\", \"false\")]\n    except:\n        cot_entries = []\n\n    return cot_entries\n\n\nwith open(\"/kaggle/input/publictesta5examples/Public_Test_A copy.json\", \"r\", encoding=\"utf-8\") as f:\n    val_data = json.load(f)\n\nresults = []\n\nfor item in val_data:\n    q_id = item[\"id\"]\n    question_text = item[\"question\"]\n    cot_text = item[\"cot\"]\n    answer = item.get(\"answer\", None)\n    \n    question_parsing = generate_question_parsing(question_text)\n    cot_parsing = generate_cot_parsing(question_text, cot_text)\n    \n    results.append({\n        \"id\": q_id,\n        \"question\": question_text,\n        \"answer\": answer,\n        \"cot\": cot_text,\n        \"question_parsing\": question_parsing,\n        \"cot_parsing\": cot_parsing\n    })\n\n\nwith open(\"/kaggle/working/validation_results.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(results, f, indent=2, ensure_ascii=False)\n\nprint(\"✅ validation_results.json saved at /kaggle/working/validation_results.json\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"!python /kaggle/input/evalscript/eval.py \\\n  --prediction /kaggle/working/validation_results.json \\\n  --reference /kaggle/input/publictesta5examples/Public_Test_A\\ copy.json \\\n  --question_threshold 0.95 \\\n  --statement_threshold 0.9 \\\n  --relation_threshold 0.9","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}