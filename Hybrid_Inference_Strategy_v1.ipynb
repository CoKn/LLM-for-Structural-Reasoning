{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jTqLGHwcEDnb",
        "eu_JRUEPELHW",
        "Fr5k4oMPES2g",
        "PUzWumL5Ea7Q",
        "hgQ4YqUsEgcK"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Hybrid Inference Strategy 1: QP + 3-Beam CP + CP-Verifier\n",
        "\n",
        "**Objective**  \n",
        "Combine two independently trained DeBERTa verifiers (QP and CP) with fine-tuned LLaMA‐3 models to improve structured reasoning outputs via a lightweight two-stage pipeline:\n",
        "\n",
        "1. **Question Parsing (QP) Stage**  \n",
        "   - Use a fine-tuned LLaMA-3 QP model (beam-search) to produce one deterministic JSON list of logical constraints.  \n",
        "   - Clean out any residual multiple-choice artifacts (e.g. “A.”, “B.”, “Option”, etc.).\n",
        "   - No reranking or verifier filtering at this stage.\n",
        "\n",
        "2. **Chain-of-Thought Parsing (CP) Stage**  \n",
        "   - Given the cleaned QP list, call a fine-tuned LLaMA-3 CP model to generate **3 beam-search** parses of the CoT into structured steps (`statement`, `evidence`, `Verification`).  \n",
        "   - Clean, dedupe, and normalize each candidate.\n",
        "\n",
        "3. **Verifier Reranking**  \n",
        "   - Score each CP candidate with the **CP verifier** (DeBERTa-v3), averaging the True-class probabilities over its steps.  \n",
        "   - If the top-scoring candidate’s average verifier score ≥ **THR_CP (0.70)**, select it; otherwise, fall back to the first beam result.\n",
        "\n",
        "4. **Output**  \n",
        "   - Emit a JSON record for each example containing:  \n",
        "     ```json\n",
        "     {\n",
        "       \"question\": …,\n",
        "       \"question_parsing\": …,\n",
        "       \"cot\": …,\n",
        "       \"cot_parsing\": …,        // either the verified or fallback candidate\n",
        "       \"answer\": …,\n",
        "       \"id\": …,\n",
        "       \"sel_idx\": …\n",
        "     }\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "| Metric                         | Score  |\n",
        "|--------------------------------|--------|\n",
        "| **Question_Macro_F1**          | 0.7781 |\n",
        "| **Statement_Macro_F1**         | 0.4007 |\n",
        "| **Statement_Evidence_Macro_F1**| 0.1276 |\n",
        "| **Reasoning_F1**               | 0.088  |\n",
        "\n",
        "> _This approach achieved the highest Question_F1 of all pipelines tested, though deeper evidence-linking and reasoning metrics remain areas for future improvement._\n"
      ],
      "metadata": {
        "id": "HqNzBjjMC1R3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Thresholds"
      ],
      "metadata": {
        "id": "fCkXzVrADyaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install core evaluation utilities\n",
        "!pip install -q evaluate\n",
        "!pip install json5\n",
        "\n",
        "!pip uninstall -y nltk\n",
        "!pip install -q --upgrade nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9zbv5ZEEvjX",
        "outputId": "efcaa4fc-9e53-481a-befb-fdd9bdcbb3c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting json5\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Downloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: json5\n",
            "Successfully installed json5-0.12.0\n",
            "Found existing installation: nltk 3.9.1\n",
            "Uninstalling nltk-3.9.1:\n",
            "  Successfully uninstalled nltk-3.9.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmU6XQoTEzPE",
        "outputId": "cb0f69f9-e6f5-4228-daba-830ce540e4b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EawMZtb0BL-k"
      },
      "outputs": [],
      "source": [
        "import unsloth  # Must come first for 4-bit LoRA\n",
        "import torch, gc, json, re, ast, html, numpy as np\n",
        "from torch.nn.functional import log_softmax\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "#  Paths & thresholds\n",
        "INPUT       = \"/content/drive/MyDrive/llm-sr-project/testingData-blank.json\"\n",
        "#OUTPUT      = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach.json\"\n",
        "OUTPUT      = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach2.json\"\n",
        "QP_LM_PATH  = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_question_parsing\"\n",
        "CP_LM_PATH  = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_cot_parsing\"\n",
        "QP_VER_PATH = \"/content/drive/MyDrive/deberta-qparse-verifier\"\n",
        "CP_VER_PATH = \"/content/drive/MyDrive/deberta-cotparse-verifier\"\n",
        "\n",
        "\n",
        "THR_QP = 0.75\n",
        "THR_CP = 0.70\n",
        "#THR_CP = 0.80\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "qd9vgLi2DWIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In-Context Learning (ICL) Prompts\n",
        "\n",
        "QP_DEMON = '''The question is:\n",
        "\n",
        "There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project. This assignment must satisfy:\n",
        "(1) If A works on Alpha, then B works on Beta.\n",
        "(2) If C works on Alpha, then D and E work on Beta.\n",
        "(3) F works on a different project than E.\n",
        "(4) D must work on a different project than A.\n",
        "(5) If F works on Alpha, then B works on Alpha.\n",
        "\n",
        "If A works on Beta, which of the following must be true?\n",
        "A. B works on Alpha\n",
        "B. C works on Beta\n",
        "C. D works on Alpha\n",
        "D. F works on Beta\n",
        "\n",
        "The parsing result is:\n",
        "\n",
        "[\n",
        "  \"There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project.\",\n",
        "  \"If A works on Alpha, then B works on Beta\",\n",
        "  \"If C works on Alpha, then D and E work on Beta\",\n",
        "  \"F works on a different project than E\",\n",
        "  \"D must work on a different project than A\",\n",
        "  \"If F works on Alpha, then B works on Alpha\",\n",
        "  \"A works on Beta\"\n",
        "]\n",
        "'''\n",
        "\n",
        "QP_TEMPLATE = '''Given a question, extract all relevant information from the question that would help to solve it.\n",
        "\n",
        "This includes:\n",
        "- General setup information (e.g., number of people, projects involved)\n",
        "- Explicit facts given in the question\n",
        "- All logical constraints or conditions\n",
        "\n",
        "Output only a JSON list and nothing else. Follow the format shown in the example.\n",
        "\n",
        "Example:\n",
        "\n",
        "{demon}\n",
        "\n",
        "Now, the question is:\n",
        "\n",
        "{question}\n",
        "\n",
        "Your output:\n",
        "'''\n",
        "\n",
        "CP_DEMON = '''The question is:\n",
        "\n",
        "There are 6 volunteers: A, B, C, D, E and F. Each person works on exactly one project.\n",
        "\n",
        "Conditions:\n",
        "(1) If A works on Alpha, then B works on Beta.\n",
        "(2) If C works on Alpha, then D and E work on Beta.\n",
        "(3) F works on a different project than E.\n",
        "(4) D must work on a different project than A.\n",
        "(5) If F works on Alpha, then B works on Alpha.\n",
        "\n",
        "Question:\n",
        "If A works on Beta, which of the following must be true?\n",
        "\n",
        "CoT:\n",
        "Since A works on Beta, Condition (1) is not triggered. Condition (2) is not triggered since C's assignment is unknown. Condition (3) doesn't give anything because E's assignment is unspecified. Condition (4) says D must work on a different project than A, so D must work on Alpha. Condition (5) depends on F, which is unknown.\n",
        "\n",
        "Parsing result:\n",
        "\n",
        "[\n",
        "  {\n",
        "    \"statement\": \"Condition (1) is not applicable\",\n",
        "    \"evidence\": \"Condition (1): If A works on Alpha, then B works on Beta. | A is working on Beta\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (2) is not applicable\",\n",
        "    \"evidence\": \"Condition (2): If C works on Alpha, then D and E work on Beta. | C's assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (3) does not provide any info\",\n",
        "    \"evidence\": \"Condition (3): F works on a different project than E. | E's assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"D must work on Alpha\",\n",
        "    \"evidence\": \"Condition (4): D must work on a different project than A, and A is working on Beta\",\n",
        "    \"Verification\": \"true\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (5) is not applicable\",\n",
        "    \"evidence\": \"Condition (5): If F works on Alpha, then B works on Alpha. | F's assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  }\n",
        "]\n",
        "'''\n",
        "\n",
        "\n",
        "CP_TEMPLATE = '''You are a reasoning assistant. Based on the question, conditions, and chain-of-thought (CoT), extract every inference or non-inference step as a JSON object.\n",
        "\n",
        "For each CoT sentence that either:\n",
        "  1. Refers to a condition (e.g. \"Condition (2) …\")\n",
        "  2. Starts with an inference cue (\"Since\", \"Therefore\", \"This means\", \"We can deduce\", etc.)\n",
        "\n",
        "Produce one object with:\n",
        "  • \"statement\": the new claim you read in that CoT sentence (don't quote the entire sentence—just the core inference).\n",
        "  • \"evidence\":\n",
        "      – if the claim restates a constraint, use the exact line from the **Conditions** block,\n",
        "      – otherwise, use the CoT fragment that you extracted it from.\n",
        "  • \"Verification\":\n",
        "      – `\"false\"` if the sentence rejects or blocks a condition (contains \"not applicable\", \"does not provide\", etc.),\n",
        "      – otherwise `\"true\"`.\n",
        "\n",
        "Keep the objects in the same order as they appear in the CoT.\n",
        "\n",
        "Example:\n",
        "\n",
        "{demon}\n",
        "\n",
        "Now, given:\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Conditions:\n",
        "{conditions}\n",
        "\n",
        "Chain-of-Thought:\n",
        "{cot}\n",
        "\n",
        "Your output:\n",
        "'''"
      ],
      "metadata": {
        "id": "ApW5pT_nD-gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "jTqLGHwcEDnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_quotes(t):\n",
        "    return (t.replace('\"','\"').replace('\"','\"').replace(\"'\",\"'\").replace(\"'\",\"'\"))\n",
        "\n",
        "def normalize_text(t):\n",
        "    t = clean_quotes(t)\n",
        "    t = re.sub(r'\\?\\s(?=[A-Z])', ', ', t)\n",
        "    t = re.sub(r'(?<=[a-zA-Z])\\.(?=[A-Z])', '. ', t)\n",
        "    t = re.sub(r'(?<![A-Da-d])\\\\n(?!\\s?[A-Da-d]\\\\.)', ' ', t)\n",
        "    return html.unescape(t).strip()\n",
        "\n",
        "def extract_json(raw):\n",
        "    raw = raw.strip()\n",
        "    i = raw.find('[')\n",
        "    if i < 0: return []\n",
        "    depth = 0\n",
        "    for j,ch in enumerate(raw[i:], i):\n",
        "        if ch=='[': depth+=1\n",
        "        elif ch==']': depth-=1\n",
        "        if depth==0:\n",
        "            blk = raw[i:j+1]\n",
        "            for p in (json.loads, ast.literal_eval):\n",
        "                try: return p(blk)\n",
        "                except: pass\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "def score_verifier_batch(prem_list, hyp_list, tok, mod):\n",
        "    enc = tok(prem_list, hyp_list, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = mod(**enc).logits\n",
        "    return torch.softmax(logits, dim=1)[:, 1].tolist()\n",
        "\n",
        "def clean_qp(qp_list):\n",
        "    return [s for s in qp_list if not re.match(r'^[A-Da-d][\\.:\\)]', s.strip()) and \"Option\" not in s and \"following\" not in s]"
      ],
      "metadata": {
        "id": "SQIc13TsEHbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Models and Verifiers"
      ],
      "metadata": {
        "id": "eu_JRUEPELHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QP LM - deterministic\n",
        "qp_tok = AutoTokenizer.from_pretrained(QP_LM_PATH)\n",
        "qp_tok.model_max_length = 1024\n",
        "qp_mod = AutoModelForCausalLM.from_pretrained(QP_LM_PATH).to(device)\n",
        "qp_pipe = pipeline(\"text-generation\", model=qp_mod, tokenizer=qp_tok,\n",
        "                   return_full_text=False, do_sample=False,\n",
        "                   num_beams=5, early_stopping=True,\n",
        "                   max_new_tokens=512, batch_size=4)\n",
        "\n",
        "# CP LM - Using beam search\n",
        "cp_tok = AutoTokenizer.from_pretrained(CP_LM_PATH)\n",
        "cp_tok.model_max_length = 2048\n",
        "cp_mod = AutoModelForCausalLM.from_pretrained(CP_LM_PATH).to(device)\n",
        "cp_pipe = pipeline(\"text-generation\", model=cp_mod, tokenizer=cp_tok,\n",
        "                   return_full_text=False, do_sample=True,temperature=0.7,\n",
        "                   num_beams=5, early_stopping=True, num_return_sequences=3,\n",
        "                   max_new_tokens=1024, batch_size=4)\n",
        "\n",
        "\n",
        "# Load QP and COT Verifiers\n",
        "def load_verifiers():\n",
        "    global qv_tok, qv_mod, cv_tok, cv_mod\n",
        "    qv_tok = AutoTokenizer.from_pretrained(QP_VER_PATH)\n",
        "    qv_mod = AutoModelForSequenceClassification.from_pretrained(QP_VER_PATH).to(device)\n",
        "    cv_tok = AutoTokenizer.from_pretrained(CP_VER_PATH)\n",
        "    cv_mod = AutoModelForSequenceClassification.from_pretrained(CP_VER_PATH).to(device)\n",
        "    return qv_tok, qv_mod, cv_tok, cv_mod\n",
        "\n",
        "\n",
        "qv_tok, qv_mod, cv_tok, cv_mod = load_verifiers()"
      ],
      "metadata": {
        "id": "t8GvgyvbEP4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Inference"
      ],
      "metadata": {
        "id": "Fr5k4oMPES2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid inference approach\n",
        "def process_one(example):\n",
        "    q_raw, cot_raw = example[\"question\"], example[\"cot\"]\n",
        "    sel_idx, ans = example.get(\"sel_idx\"), example.get(\"answer\")\n",
        "    q, cot = normalize_text(q_raw), normalize_text(cot_raw)\n",
        "\n",
        "    # 1) Single deterministic QP output\n",
        "    prompt = QP_TEMPLATE.format(demon=QP_DEMON, question=q)\n",
        "    raw_qp = qp_pipe(prompt, max_new_tokens=512)\n",
        "    if not isinstance(raw_qp, list):\n",
        "        raw_qp = [raw_qp]\n",
        "    best_qp = clean_qp(extract_json(raw_qp[0][\"generated_text\"]))\n",
        "\n",
        "\n",
        "    # 2) CP: generate 3 beam-search parses\n",
        "    conds_str = json.dumps(best_qp, ensure_ascii=False)\n",
        "    prompt_cp = CP_TEMPLATE.format(\n",
        "        demon      = CP_DEMON,\n",
        "        question   = q,\n",
        "        conditions = conds_str,\n",
        "        cot        = cot\n",
        "    )\n",
        "\n",
        "    # 2.a) get 3 raw outputs\n",
        "    raw_cp_outs = cp_pipe(prompt_cp, max_new_tokens=1024)\n",
        "    # flatten HF’s list-of-lists (if any)\n",
        "    raw_cp_flat = [\n",
        "        item\n",
        "        for sub in raw_cp_outs\n",
        "        for item in (sub if isinstance(sub, list) else [sub])\n",
        "    ]\n",
        "\n",
        "    # 2.b) parse & clean each candidate\n",
        "    cps_candidates = []\n",
        "    for out in raw_cp_flat:\n",
        "        parsed = extract_json(out[\"generated_text\"])\n",
        "        if not parsed:\n",
        "            continue\n",
        "        seen = set()\n",
        "        cleaned = []\n",
        "        for st in parsed:\n",
        "            stmt = st.get(\"statement\",\"\").strip()\n",
        "            ev   = st.get(\"evidence\",\"\").strip() or \"logical deduction\"\n",
        "            ver  = str(st.get(\"Verification\",\"true\")).lower()\n",
        "            if len(stmt) < 3 or (stmt,ev) in seen:\n",
        "                continue\n",
        "            seen.add((stmt,ev))\n",
        "            cleaned.append({\"statement\":stmt,\"evidence\":ev,\"Verification\":ver})\n",
        "        if cleaned:\n",
        "            cps_candidates.append(cleaned)\n",
        "\n",
        "    # fallback if nothing survived\n",
        "    if not cps_candidates:\n",
        "        cps_candidates = [[]]\n",
        "\n",
        "    # 3) Score each candidate with your CP verifier\n",
        "    premise = f\"Question:\\n{q}\\n\\nConditions:\\n\" + \"\\n\".join(f\"- {s}\" for s in best_qp) + f\"\\n\\nCoT:\\n{cot}\"\n",
        "    avg_scores = []\n",
        "    for cp_list in cps_candidates:\n",
        "        if not cp_list:\n",
        "            avg_scores.append(0.0)\n",
        "            continue\n",
        "        prems = [premise]*len(cp_list)\n",
        "        hyps  = [f\"Statement: {st['statement']}\\nBased on: {st['evidence']}\" for st in cp_list]\n",
        "        scores = score_verifier_batch(prems, hyps, cv_tok, cv_mod)\n",
        "        avg_scores.append(sum(scores)/len(scores))\n",
        "\n",
        "    # 4) Pick best candidate (with threshold)\n",
        "    best_idx = int(np.argmax(avg_scores))\n",
        "    if avg_scores[best_idx] < THR_CP:\n",
        "        best_idx = 0\n",
        "    best_cp = cps_candidates[best_idx]\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"question\": q_raw,\n",
        "        \"question_parsing\": best_qp,\n",
        "        \"answer\": ans,\n",
        "        \"id\": example[\"id\"],\n",
        "        \"cot\": cot_raw,\n",
        "        \"cot_parsing\": best_cp,\n",
        "        \"sel_idx\": sel_idx\n",
        "    }"
      ],
      "metadata": {
        "id": "z9iXVBMzEaPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch and Run"
      ],
      "metadata": {
        "id": "PUzWumL5Ea7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(batch):\n",
        "    outs = [process_one({\n",
        "        \"question\": batch[\"question\"][i],\n",
        "        \"cot\":       batch[\"cot\"][i],\n",
        "        \"id\":        batch[\"id\"][i],\n",
        "        \"sel_idx\":   batch.get(\"sel_idx\", [None]*len(batch[\"id\"]))[i],\n",
        "        \"answer\":    batch.get(\"answer\", [None]*len(batch[\"id\"]))[i],\n",
        "    }) for i in range(len(batch[\"question\"]))]\n",
        "\n",
        "    return {\n",
        "        \"question\":        [o[\"question\"]        for o in outs],\n",
        "        \"question_parsing\":[o[\"question_parsing\"]for o in outs],\n",
        "        \"answer\":          [o[\"answer\"]          for o in outs],\n",
        "        \"id\":              [o[\"id\"]              for o in outs],\n",
        "        \"cot\":             [o[\"cot\"]             for o in outs],\n",
        "        \"cot_parsing\":     [o[\"cot_parsing\"]     for o in outs],\n",
        "        \"sel_idx\":         [o[\"sel_idx\"]         for o in outs],\n",
        "    }\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    gc.collect()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    ds = load_dataset(\"json\", data_files={\"test\": INPUT})[\"test\"]\n",
        "\n",
        "    out_ds = ds.map(\n",
        "        process_batch,\n",
        "        batched=True,\n",
        "        batch_size=2,\n",
        "        remove_columns=ds.column_names\n",
        "    )\n",
        "\n",
        "    out_ds.to_json(OUTPUT, orient=\"records\", lines=False)\n",
        "    print(\"✅ Done — saved to\", OUTPUT)"
      ],
      "metadata": {
        "id": "aWqHd4PrEc21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Predictions"
      ],
      "metadata": {
        "id": "hgQ4YqUsEgcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "INPUT_PATH  = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach.json\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/llm-sr-project/final_results_hybrid_approach.json\"\n",
        "\n",
        "def transform_example(ex):\n",
        "    # reorder each cot_parsing entry: statement → evidence → Verification\n",
        "    reordered = []\n",
        "    for step in ex.get(\"cot_parsing\", []):\n",
        "        reordered.append({\n",
        "            \"statement\":    step.get(\"statement\"),\n",
        "            \"evidence\":     step.get(\"evidence\"),\n",
        "            \"Verification\": step.get(\"Verification\"),\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"question\":         ex.get(\"question\"),\n",
        "        \"question_parsing\": ex.get(\"question_parsing\"),\n",
        "        \"answer\":           ex.get(\"answer\"),\n",
        "        \"id\":               ex.get(\"id\"),\n",
        "        \"cot\":              ex.get(\"cot\"),\n",
        "        \"cot_parsing\":      reordered,\n",
        "        \"sel_idx\":          ex.get(\"sel_idx\"),\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        examples = json.load(f)\n",
        "\n",
        "    structured = [transform_example(ex) for ex in examples]\n",
        "\n",
        "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(structured, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Wrote {len(structured)} examples to {OUTPUT_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "loPwQZsFEl9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "jRp3r43OEmn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EVAL_SCRIPT = \"/content/drive/MyDrive/llm-sr-project/eval.py\"\n",
        "PREDICTION_PATH = \"/content/drive/MyDrive/llm-sr-project/final_results_hybrid_approach.json\"\n",
        "REFERENCE_PATH = \"/content/drive/MyDrive/llm-sr-project/test-reference.json\"\n",
        "\n",
        "!python {EVAL_SCRIPT} \\\n",
        "  --prediction {PREDICTION_PATH} \\\n",
        "  --reference {REFERENCE_PATH} \\\n",
        "  --question_threshold 0.95 \\\n",
        "  --statement_threshold 0.9 \\\n",
        "  --relation_threshold 0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSzseeS9EoN0",
        "outputId": "bbdd7215-d037-4832-e90e-4a050d88c177"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-17 16:31:00.138542: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-17 16:31:00.156097: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747499460.177402    3160 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747499460.183750    3160 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-17 16:31:00.204552: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 1.05k/1.05k [00:00<00:00, 7.61MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 738M/738M [00:02<00:00, 250MB/s]\n",
            "tokenizer_config.json: 100% 1.28k/1.28k [00:00<00:00, 11.1MB/s]\n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 31.0MB/s]\n",
            "tokenizer.json: 100% 8.66M/8.66M [00:00<00:00, 22.7MB/s]\n",
            "added_tokens.json: 100% 23.0/23.0 [00:00<00:00, 230kB/s]\n",
            "special_tokens_map.json: 100% 286/286 [00:00<00:00, 2.61MB/s]\n",
            "\u001b[?25lTotal number of predictions: \u001b[1;36m24\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0mAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  4%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  8%\u001b[0m \u001b[36m0:01:17\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 12%\u001b[0m \u001b[36m0:01:26\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 17%\u001b[0m \u001b[36m0:01:08\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 21%\u001b[0m \u001b[36m0:01:02\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 25%\u001b[0m \u001b[36m0:00:54\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 29%\u001b[0m \u001b[36m0:00:47\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 33%\u001b[0m \u001b[36m0:00:47\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 38%\u001b[0m \u001b[36m0:00:44\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 42%\u001b[0m \u001b[36m0:00:42\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 46%\u001b[0m \u001b[36m0:00:37\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 50%\u001b[0m \u001b[36m0:00:33\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 54%\u001b[0m \u001b[36m0:00:30\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 58%\u001b[0m \u001b[36m0:00:24\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[35m 62%\u001b[0m \u001b[36m0:00:21\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[35m 67%\u001b[0m \u001b[36m0:00:19\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[35m 71%\u001b[0m \u001b[36m0:00:17\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[35m 75%\u001b[0m \u001b[36m0:00:14\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[35m 79%\u001b[0m \u001b[36m0:00:13\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[35m 83%\u001b[0m \u001b[36m0:00:10\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[35m 88%\u001b[0m \u001b[36m0:00:07\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m 92%\u001b[0m \u001b[36m0:00:05\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[35m 96%\u001b[0m \u001b[36m0:00:03\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[3m           Evaluation Results           \u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
            "┃\u001b[35m \u001b[0m\u001b[35mMetric                     \u001b[0m\u001b[35m \u001b[0m┃\u001b[35m \u001b[0m\u001b[35mValue \u001b[0m\u001b[35m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
            "│ Question_Macro_F1           │ 0.7781 │\n",
            "│ Statement_Macro_F1          │ 0.4007 │\n",
            "│ Statement_Evidence_Macro_F1 │ 0.1276 │\n",
            "│ Reasoning_F1                │ 0.088  │\n",
            "└─────────────────────────────┴────────┘\n",
            "Question_Macro_F1: 0.7781\n",
            "Statement_Macro_F1: 0.4007\n",
            "Statement_Evidence_Macro_F1: 0.1276\n",
            "Reasoning_F1: 0.088\n"
          ]
        }
      ]
    }
  ]
}
