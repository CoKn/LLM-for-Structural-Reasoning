{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "59X2spfIk-0-"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#4. Evaluation\n",
        "\n",
        "**Purpose:**  \n",
        "Run inference on the blank test set using our fine-tuned QP & CoT models, log all raw outputs, and compute final F1 metrics using the shared-task `eval.py` script.\n",
        "\n",
        "**Inputs:**  \n",
        "- `/content/drive/MyDrive/llm-sr-project/testingData-blank.json`  \n",
        "- Fine-tuned QP model at `/content/drive/MyDrive/llm-sr-project/finetuned_llama3_question_parsing`  \n",
        "- Fine-tuned CoT model at `/content/drive/MyDrive/llm-sr-project/finetuned_llama3_cot_parsing`  \n",
        "- Evaluation script at `/content/drive/MyDrive/llm-sr-project/eval.py`  \n",
        "- Reference file at `/content/drive/MyDrive/llm-sr-project/test-reference.json`\n",
        "\n",
        "**Outputs:**  \n",
        "- `/content/drive/MyDrive/llm-sr-project/testingDataresultsfor700-2.json` (inference results)  \n",
        "- Printed metrics:  \n",
        "  - Question_Macro_F1  \n",
        "  - Statement_Macro_F1  \n",
        "  - Statement_Evidence_Macro_F1  \n",
        "  - Reasoning_F1  \n",
        "\n",
        "**Workflow:**  \n",
        "1. Load the LoRA-adapter checkpoints for QP and CoT.  \n",
        "2. For each test example:  \n",
        "   - Generate `question_parsing` via structured ICL prompt.  \n",
        "   - Generate `cot_parsing` using extracted constraints and the Chain-of-Thought.  \n",
        "3. Deduplicate and clean each parse.  \n",
        "4. Save full inference JSON.  \n",
        "5. Run `eval.py` with high-precision thresholds to compute F1 scores.  \n"
      ],
      "metadata": {
        "id": "hYRRnnethZOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and File Paths"
      ],
      "metadata": {
        "id": "iHE2zK8mkM3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install core evaluation utilities\n",
        "!pip install -q evaluate\n",
        "!pip install json5\n",
        "\n",
        "!pip uninstall -y nltk\n",
        "!pip install -q --upgrade nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjR0M8cxllUC",
        "outputId": "8415267c-9390-4e6b-a821-f28b503e499e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting json5\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Downloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: json5\n",
            "Successfully installed json5-0.12.0\n",
            "Found existing installation: nltk 3.9.1\n",
            "Uninstalling nltk-3.9.1:\n",
            "  Successfully uninstalled nltk-3.9.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUOk9iZ7lpza",
        "outputId": "3eae7980-7471-4eea-93ef-f00f946d1ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth  # Must come first for 4-bit LoRA compatibility\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import json, re, ast, os, html\n",
        "\n",
        "# Optional: robust JSON parsing\n",
        "try:\n",
        "    import json5\n",
        "    USE_JSON5 = True\n",
        "except ImportError:\n",
        "    USE_JSON5 = False"
      ],
      "metadata": {
        "id": "gkBFiQz0kIDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path  = \"/content/drive/MyDrive/llm-sr-project/testingData-blank.json\"\n",
        "output_path = \"/content/drive/MyDrive/llm-sr-project/testingDataresultsfor700-2.json\"\n",
        "log_path    = \"/content/drive/MyDrive/llm-sr-project/raw_outputs_log.jsonl\""
      ],
      "metadata": {
        "id": "vsu754VDkgIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "rrF5UGAHkWIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ICL (In-Context Learning) Prompt Templates and Demonstrations (same as in training)\n",
        "\n",
        "QP_DEMON = '''The question is:\n",
        "\n",
        "There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project. This assignment must satisfy:\n",
        "(1) If A works on Alpha, then B works on Beta.\n",
        "(2) If C works on Alpha, then D and E work on Beta.\n",
        "(3) F works on a different project than E.\n",
        "(4) D must work on a different project than A.\n",
        "(5) If F works on Alpha, then B works on Alpha.\n",
        "\n",
        "If A works on Beta, which of the following must be true?\n",
        "A. B works on Alpha\n",
        "B. C works on Beta\n",
        "C. D works on Alpha\n",
        "D. F works on Beta\n",
        "\n",
        "The parsing result is:\n",
        "\n",
        "[\n",
        "  \"There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project.\",\n",
        "  \"If A works on Alpha, then B works on Beta\",\n",
        "  \"If C works on Alpha, then D and E work on Beta\",\n",
        "  \"F works on a different project than E\",\n",
        "  \"D must work on a different project than A\",\n",
        "  \"If F works on Alpha, then B works on Alpha\",\n",
        "  \"A works on Beta\"\n",
        "]\n",
        "'''\n",
        "\n",
        "\n",
        "QP_TEMPLATE = '''Given a question, extract all relevant information from the question that would help to solve it.\n",
        "\n",
        "This includes:\n",
        "- General setup information (e.g., number of people, projects involved)\n",
        "- Explicit facts given in the question\n",
        "- All logical constraints or conditions\n",
        "\n",
        "Output only a JSON list and nothing else. Follow the format shown in the example.\n",
        "\n",
        "Example:\n",
        "\n",
        "{demon}\n",
        "\n",
        "Now, the question is:\n",
        "\n",
        "{question}\n",
        "\n",
        "Your output:\n",
        "'''\n",
        "\n",
        "\n",
        "CP_DEMON = '''The question is:\n",
        "\n",
        "There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project.\n",
        "\n",
        "Conditions:\n",
        "(1) If A works on Alpha, then B works on Beta.\n",
        "(2) If C works on Alpha, then D and E work on Beta.\n",
        "(3) F works on a different project than E.\n",
        "(4) D must work on a different project than A.\n",
        "(5) If F works on Alpha, then B works on Alpha.\n",
        "\n",
        "Question:\n",
        "If A works on Beta, which of the following must be true?\n",
        "\n",
        "CoT:\n",
        "Since A works on Beta, Condition (1) is not triggered. Condition (2) is not triggered since C’s assignment is unknown. Condition (3) doesn’t give anything because E’s assignment is unspecified. Condition (4) says D must work on a different project than A, so D must work on Alpha. Condition (5) depends on F, which is unknown.\n",
        "\n",
        "Parsing result:\n",
        "\n",
        "[\n",
        "  {\n",
        "    \"statement\": \"Condition (1) is not applicable\",\n",
        "    \"evidence\": \"Condition (1): If A works on Alpha, then B works on Beta. | A is working on Beta\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (2) is not applicable\",\n",
        "    \"evidence\": \"Condition (2): If C works on Alpha, then D and E work on Beta. | C’s assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (3) does not provide any info\",\n",
        "    \"evidence\": \"Condition (3): F works on a different project than E. | E’s assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"D must work on Alpha\",\n",
        "    \"evidence\": \"Condition (4): D must work on a different project than A, and A is working on Beta\",\n",
        "    \"Verification\": \"true\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (5) is not applicable\",\n",
        "    \"evidence\": \"Condition (5): If F works on Alpha, then B works on Alpha. | F’s assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  }\n",
        "]\n",
        "'''\n",
        "\n",
        "CP_TEMPLATE = '''You are a reasoning assistant. Based on the question, conditions, and chain-of-thought (CoT), extract every inference or non-inference step as a JSON object.\n",
        "\n",
        "For each CoT sentence that either:\n",
        "  1. Refers to a condition (e.g. “Condition (2) …”)\n",
        "  2. Starts with an inference cue (“Since”, “Therefore”, “This means”, “We can deduce”, etc.)\n",
        "\n",
        "Produce one object with:\n",
        "  • \"statement\": the new claim you read in that CoT sentence (don’t quote the entire sentence—just the core inference).\n",
        "  • \"evidence\":\n",
        "      – if the claim restates a constraint, use the exact line from the **Conditions** block,\n",
        "      – otherwise, use the CoT fragment that you extracted it from.\n",
        "  • \"Verification\":\n",
        "      – `\"false\"` if the sentence rejects or blocks a condition (contains “not applicable”, “does not provide”, etc.),\n",
        "      – otherwise `\"true\"`.\n",
        "\n",
        "Keep the objects in the same order as they appear in the CoT.\n",
        "\n",
        "Example:\n",
        "\n",
        "{demon}\n",
        "\n",
        "Now, given:\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Conditions:\n",
        "{conditions}\n",
        "\n",
        "Chain-of-Thought:\n",
        "{cot}\n",
        "\n",
        "Your output:\n",
        "'''"
      ],
      "metadata": {
        "id": "11i2sW2dkL2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Models"
      ],
      "metadata": {
        "id": "7YpUIVlDksW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question Parser\n",
        "question_model_path = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_question_parsing\"\n",
        "question_tokenizer  = AutoTokenizer.from_pretrained(question_model_path)\n",
        "question_tokenizer.model_max_length = 1024\n",
        "question_model      = AutoModelForCausalLM.from_pretrained(question_model_path, load_in_4bit=True)\n",
        "question_pipe       = pipeline(\"text-generation\", model=question_model, tokenizer=question_tokenizer,\n",
        "                               return_full_text=False, num_beams=5, early_stopping=True, do_sample=False)\n",
        "\n",
        "# CoT Parser\n",
        "cot_model_path = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_cot_parsing\"\n",
        "cot_tokenizer  = AutoTokenizer.from_pretrained(cot_model_path)\n",
        "cot_tokenizer.model_max_length = 2048\n",
        "cot_model      = AutoModelForCausalLM.from_pretrained(cot_model_path, load_in_4bit=True)\n",
        "cot_pipe       = pipeline(\"text-generation\", model=cot_model, tokenizer=cot_tokenizer,\n",
        "                          return_full_text=False, num_beams=5, early_stopping=True, do_sample=False)\n",
        "\n",
        "print(\"✅ Models loaded.\")"
      ],
      "metadata": {
        "id": "Jyba-_8GkfZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "Wf3X3tb0k1cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_first_json_array(raw: str):\n",
        "    raw = raw.strip()\n",
        "    start = raw.find('[')\n",
        "    if start == -1: return None\n",
        "    depth = 0\n",
        "    for i, ch in enumerate(raw[start:], start):\n",
        "        if ch=='[': depth+=1\n",
        "        elif ch==']': depth-=1\n",
        "        if depth==0:\n",
        "            block = raw[start:i+1]\n",
        "            for parser in (json.loads, ast.literal_eval, (json5.loads if USE_JSON5 else None)):\n",
        "                if parser:\n",
        "                    try: return parser(block)\n",
        "                    except: pass\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def clean_quotes(text):\n",
        "    return text.replace('“','\"').replace('”','\"').replace(\"‘\",\"'\").replace(\"’\",\"'\")\n",
        "\n",
        "def normalize_question_text(text):\n",
        "    text = clean_quotes(text)\n",
        "    text = re.sub(r'\\?\\s(?=[A-Z])', ', ', text)\n",
        "    text = re.sub(r'(?<=[a-zA-Z])\\.(?=[A-Z])', '. ', text)\n",
        "    text = re.sub(r'(?<![A-Da-d])\\\\n(?!\\s?[A-Da-d]\\\\.)', ' ', text)\n",
        "    return html.unescape(text).strip()"
      ],
      "metadata": {
        "id": "D07ypCY5kjxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference Functions"
      ],
      "metadata": {
        "id": "EXlc2x0Sk6f-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_question_parsing(question: str):\n",
        "    q = normalize_question_text(question)\n",
        "    prompt = QP_TEMPLATE.format(demon=QP_DEMON, question=q)\n",
        "    resp = question_pipe(prompt, max_new_tokens=512)[0][\"generated_text\"]\n",
        "    with open(log_path, \"a\") as f:\n",
        "        f.write(json.dumps({\"type\":\"QP\",\"question\":question,\"raw\":resp})+\"\\n\")\n",
        "    return extract_first_json_array(resp) or []\n",
        "\n",
        "def generate_cot_parsing(question: str, cot: str, constraints):\n",
        "    q = normalize_question_text(question)\n",
        "    c = normalize_question_text(cot)\n",
        "    prompt = CP_TEMPLATE.format(demon=CP_DEMON, question=q,\n",
        "                                conditions=json.dumps(constraints, ensure_ascii=False),\n",
        "                                cot=c)\n",
        "    resp_list = cot_pipe(prompt, max_new_tokens=1024)\n",
        "    if not resp_list or \"generated_text\" not in resp_list[0]:\n",
        "        print(\"⚠️ Malformed response\")\n",
        "        return []\n",
        "    resp = resp_list[0][\"generated_text\"]\n",
        "    with open(log_path, \"a\") as f:\n",
        "        f.write(json.dumps({\"type\":\"CP\",\"question\":question,\"cot\":cot,\"raw\":resp})+\"\\n\")\n",
        "    steps = extract_first_json_array(resp)\n",
        "    if not steps: return []\n",
        "    clean, seen = [], set()\n",
        "    for st in steps:\n",
        "        s = st.get(\"statement\",\"\").strip()\n",
        "        e = st.get(\"evidence\",\"\").strip() or \"logical deduction\"\n",
        "        v = str(st.get(\"Verification\",\"true\")).lower()\n",
        "        if len(s)<5 or (s,e) in seen: continue\n",
        "        seen.add((s,e))\n",
        "        clean.append({\"statement\":s,\"evidence\":e,\"Verification\":v})\n",
        "    return clean"
      ],
      "metadata": {
        "id": "75ARGoK3kk3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Inference and Save"
      ],
      "metadata": {
        "id": "59X2spfIk-0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(input_path) as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "results = []\n",
        "for ex in test_data:\n",
        "    qp = generate_question_parsing(ex[\"question\"])\n",
        "    print(f\"→ QP extracted {len(qp)} constraints\")\n",
        "    cp = generate_cot_parsing(ex[\"question\"], ex[\"cot\"], qp)\n",
        "    print(f\"→ CoT parsed {len(cp)} steps\")\n",
        "    results.append({**ex, \"question_parsing\": qp, \"cot_parsing\": cp})\n",
        "\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"✅ Saved:\", output_path)"
      ],
      "metadata": {
        "id": "zukkFfZXkoS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "tMyY3SYzlFqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EVAL_SCRIPT = \"/content/drive/MyDrive/llm-sr-project/eval.py\"\n",
        "PREDICTION_PATH = \"/content/drive/MyDrive/llm-sr-project/testingDataresultsfor700-2.json\"\n",
        "REFERENCE_PATH = \"/content/drive/MyDrive/llm-sr-project/test-reference.json\"\n",
        "\n",
        "!python {EVAL_SCRIPT} \\\n",
        "  --prediction {PREDICTION_PATH} \\\n",
        "  --reference {REFERENCE_PATH} \\\n",
        "  --question_threshold 0.95 \\\n",
        "  --statement_threshold 0.9 \\\n",
        "  --relation_threshold 0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6XiZ0e6hxe6",
        "outputId": "12c94901-5cbb-4704-fe49-9bd382bbf23c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-17 14:13:00.489505: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-17 14:13:00.507025: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747491180.528260    2927 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747491180.534701    2927 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-17 14:13:00.555637: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 1.05k/1.05k [00:00<00:00, 7.95MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 738M/738M [00:33<00:00, 22.0MB/s]\n",
            "tokenizer_config.json: 100% 1.28k/1.28k [00:00<00:00, 7.28MB/s]\n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 108MB/s]\n",
            "tokenizer.json: 100% 8.66M/8.66M [00:01<00:00, 7.67MB/s]\n",
            "added_tokens.json: 100% 23.0/23.0 [00:00<00:00, 206kB/s]\n",
            "special_tokens_map.json: 100% 286/286 [00:00<00:00, 2.16MB/s]\n",
            "\u001b[?25lTotal number of predictions: \u001b[1;36m24\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0mAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  4%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  8%\u001b[0m \u001b[36m0:01:05\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 12%\u001b[0m \u001b[36m0:01:13\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 17%\u001b[0m \u001b[36m0:01:07\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 21%\u001b[0m \u001b[36m0:01:02\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 25%\u001b[0m \u001b[36m0:00:54\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 29%\u001b[0m \u001b[36m0:00:46\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 33%\u001b[0m \u001b[36m0:00:44\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 38%\u001b[0m \u001b[36m0:00:43\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 42%\u001b[0m \u001b[36m0:00:42\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 46%\u001b[0m \u001b[36m0:00:38\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 50%\u001b[0m \u001b[36m0:00:34\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 54%\u001b[0m \u001b[36m0:00:30\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 58%\u001b[0m \u001b[36m0:00:26\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[35m 62%\u001b[0m \u001b[36m0:00:23\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[35m 67%\u001b[0m \u001b[36m0:00:20\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[35m 71%\u001b[0m \u001b[36m0:00:17\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[35m 75%\u001b[0m \u001b[36m0:00:14\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[35m 79%\u001b[0m \u001b[36m0:00:12\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[35m 83%\u001b[0m \u001b[36m0:00:10\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[35m 88%\u001b[0m \u001b[36m0:00:07\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m 92%\u001b[0m \u001b[36m0:00:05\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[35m 96%\u001b[0m \u001b[36m0:00:03\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[3m           Evaluation Results           \u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
            "┃\u001b[35m \u001b[0m\u001b[35mMetric                     \u001b[0m\u001b[35m \u001b[0m┃\u001b[35m \u001b[0m\u001b[35mValue \u001b[0m\u001b[35m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
            "│ Question_Macro_F1           │ 0.7526 │\n",
            "│ Statement_Macro_F1          │ 0.4015 │\n",
            "│ Statement_Evidence_Macro_F1 │ 0.1849 │\n",
            "│ Reasoning_F1                │ 0.1405 │\n",
            "└─────────────────────────────┴────────┘\n",
            "Question_Macro_F1: 0.7526\n",
            "Statement_Macro_F1: 0.4015\n",
            "Statement_Evidence_Macro_F1: 0.1849\n",
            "Reasoning_F1: 0.1405\n"
          ]
        }
      ]
    }
  ]
}