{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqNzBjjMC1R3"
   },
   "source": [
    "# Hybrid Inference Strategy (Ablation): QP Verifier + CP Verifier\n",
    "\n",
    "**Objective**  \n",
    "Use two independently fine-tuned DeBERTa verifiers (one for QP, one for CP) alongside LoRA-adapted LLaMA-3 models in a lightweight, two-stage pipeline:\n",
    "\n",
    "1. **Question Parsing (QP) Stage**  \n",
    "   - Generate three QP candidates via a fine-tuned LLaMA-3 QP model (beam-search + sampling).  \n",
    "   - Clean each JSON list of logical constraints (remove â€œA.â€, â€œOptionâ€, etc.).  \n",
    "   - Score all three with the QP verifier (DeBERTa-v3); pick the highest-scoring candidate if its score â‰¥ **THR_QP** (0.75), otherwise fall back to the first beam output.\n",
    "\n",
    "2. **Chain-of-Thought Parsing (CP) Stage**  \n",
    "   - Given the selected QP list, generate three CP candidates via a fine-tuned LLaMA-3 CP model (beam-search + sampling).  \n",
    "   - Clean, dedupe, and normalize each candidateâ€™s `statement`, `evidence`, and `Verification` fields.\n",
    "\n",
    "3. **Verifier Reranking**  \n",
    "   - Score each CP candidate with the CP verifier (DeBERTa-v3), averaging the True-class probabilities over its steps.  \n",
    "   - If the top CP candidateâ€™s average score â‰¥ **THR_CP** (0.70), select it; otherwise, fall back to the first beam output.\n",
    "\n",
    "4. **Output**  \n",
    "   Emit a JSON record per example containing:  \n",
    "   ```json\n",
    "   {\n",
    "     \"question\": â€¦,\n",
    "     \"question_parsing\": â€¦,\n",
    "     \"cot\": â€¦,\n",
    "     \"cot_parsing\": â€¦,        // verified or fallback candidate\n",
    "     \"answer\": â€¦,\n",
    "     \"id\": â€¦,\n",
    "     \"sel_idx\": â€¦\n",
    "   }\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "| Metric                         | Score  |\n",
    "|--------------------------------|--------|\n",
    "| **Question_Macro_F1**          | 0.7321 |\n",
    "| **Statement_Macro_F1**         | 0.3654 |\n",
    "| **Statement_Evidence_Macro_F1**| 0.1383 |\n",
    "| **Reasoning_F1**               | 0.0946 |\n",
    "\n",
    "> This ablation underperforms the main initial strategy on Question_F1 (down from 0.7526 to 0.7321) and other metrics, indicating that adding a standalone QP verifier before CP can hurt downstream reasoning performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCkXzVrADyaE"
   },
   "source": [
    "## Setup and Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mSxKNto3ac41"
   },
   "outputs": [],
   "source": [
    "# Install Unsloth for efficient LLM fine-tuning\n",
    "%%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9zbv5ZEEvjX",
    "outputId": "01e4e040-acde-47ac-e9f8-7d233850c62a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m81.9/84.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting json5\n",
      "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
      "Downloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: json5\n",
      "Successfully installed json5-0.12.0\n",
      "Found existing installation: nltk 3.9.1\n",
      "Uninstalling nltk-3.9.1:\n",
      "  Successfully uninstalled nltk-3.9.1\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install core evaluation utilities\n",
    "!pip install -q evaluate\n",
    "!pip install json5\n",
    "\n",
    "!pip uninstall -y nltk\n",
    "!pip install -q --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmU6XQoTEzPE",
    "outputId": "ecdf39ed-6b64-4c84-b171-eb9580bcf961"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EawMZtb0BL-k",
    "outputId": "a911b0d5-9224-4a78-9d55-fbc7ffbc2d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Patching Xformers to fix some performance issues.\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.3.0+cu121 with CUDA 1201 (you have 2.6.0+cu124)\n",
      "    Python  3.11.9 (you have 3.11.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import unsloth  \n",
    "import torch, gc, json, re, ast, html, numpy as np\n",
    "from torch.nn.functional import log_softmax\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#  Paths & thresholds\n",
    "INPUT       = \"/content/drive/MyDrive/llm-sr-project/testingData-blank.json\"\n",
    "#OUTPUT      = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach.json\"\n",
    "OUTPUT      = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach_with2verifiers.json\"\n",
    "QP_LM_PATH  = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_question_parsing\"\n",
    "CP_LM_PATH  = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_cot_parsing\"\n",
    "QP_VER_PATH = \"/content/drive/MyDrive/deberta-qparse-verifier\"\n",
    "CP_VER_PATH = \"/content/drive/MyDrive/deberta-cotparse-verifier\"\n",
    "\n",
    "\n",
    "THR_QP = 0.75\n",
    "THR_CP = 0.70\n",
    "#THR_CP = 0.80\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd9vgLi2DWIO"
   },
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ApW5pT_nD-gB"
   },
   "outputs": [],
   "source": [
    "# In-Context Learning (ICL) Prompts\n",
    "\n",
    "QP_DEMON = '''The question is:\n",
    "\n",
    "There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project. This assignment must satisfy:\n",
    "(1) If A works on Alpha, then B works on Beta.\n",
    "(2) If C works on Alpha, then D and E work on Beta.\n",
    "(3) F works on a different project than E.\n",
    "(4) D must work on a different project than A.\n",
    "(5) If F works on Alpha, then B works on Alpha.\n",
    "\n",
    "If A works on Beta, which of the following must be true?\n",
    "A. B works on Alpha\n",
    "B. C works on Beta\n",
    "C. D works on Alpha\n",
    "D. F works on Beta\n",
    "\n",
    "The parsing result is:\n",
    "\n",
    "[\n",
    "  \"There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project.\",\n",
    "  \"If A works on Alpha, then B works on Beta\",\n",
    "  \"If C works on Alpha, then D and E work on Beta\",\n",
    "  \"F works on a different project than E\",\n",
    "  \"D must work on a different project than A\",\n",
    "  \"If F works on Alpha, then B works on Alpha\",\n",
    "  \"A works on Beta\"\n",
    "]\n",
    "'''\n",
    "\n",
    "QP_TEMPLATE = '''Given a question, extract all relevant information from the question that would help to solve it.\n",
    "\n",
    "This includes:\n",
    "- General setup information (e.g., number of people, projects involved)\n",
    "- Explicit facts given in the question\n",
    "- All logical constraints or conditions\n",
    "\n",
    "Output only a JSON list and nothing else. Follow the format shown in the example.\n",
    "\n",
    "Example:\n",
    "\n",
    "{demon}\n",
    "\n",
    "Now, the question is:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your output:\n",
    "'''\n",
    "\n",
    "CP_DEMON = '''The question is:\n",
    "\n",
    "There are 6 volunteers: A, B, C, D, E and F. Each person works on exactly one project.\n",
    "\n",
    "Conditions:\n",
    "(1) If A works on Alpha, then B works on Beta.\n",
    "(2) If C works on Alpha, then D and E work on Beta.\n",
    "(3) F works on a different project than E.\n",
    "(4) D must work on a different project than A.\n",
    "(5) If F works on Alpha, then B works on Alpha.\n",
    "\n",
    "Question:\n",
    "If A works on Beta, which of the following must be true?\n",
    "\n",
    "CoT:\n",
    "Since A works on Beta, Condition (1) is not triggered. Condition (2) is not triggered since C's assignment is unknown. Condition (3) doesn't give anything because E's assignment is unspecified. Condition (4) says D must work on a different project than A, so D must work on Alpha. Condition (5) depends on F, which is unknown.\n",
    "\n",
    "Parsing result:\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"statement\": \"Condition (1) is not applicable\",\n",
    "    \"evidence\": \"Condition (1): If A works on Alpha, then B works on Beta. | A is working on Beta\",\n",
    "    \"Verification\": \"false\"\n",
    "  },\n",
    "  {\n",
    "    \"statement\": \"Condition (2) is not applicable\",\n",
    "    \"evidence\": \"Condition (2): If C works on Alpha, then D and E work on Beta. | C's assignment is unknown\",\n",
    "    \"Verification\": \"false\"\n",
    "  },\n",
    "  {\n",
    "    \"statement\": \"Condition (3) does not provide any info\",\n",
    "    \"evidence\": \"Condition (3): F works on a different project than E. | E's assignment is unknown\",\n",
    "    \"Verification\": \"false\"\n",
    "  },\n",
    "  {\n",
    "    \"statement\": \"D must work on Alpha\",\n",
    "    \"evidence\": \"Condition (4): D must work on a different project than A, and A is working on Beta\",\n",
    "    \"Verification\": \"true\"\n",
    "  },\n",
    "  {\n",
    "    \"statement\": \"Condition (5) is not applicable\",\n",
    "    \"evidence\": \"Condition (5): If F works on Alpha, then B works on Alpha. | F's assignment is unknown\",\n",
    "    \"Verification\": \"false\"\n",
    "  }\n",
    "]\n",
    "'''\n",
    "\n",
    "\n",
    "CP_TEMPLATE = '''You are a reasoning assistant. Based on the question, conditions, and chain-of-thought (CoT), extract every inference or non-inference step as a JSON object.\n",
    "\n",
    "For each CoT sentence that either:\n",
    "  1. Refers to a condition (e.g. \"Condition (2) â€¦\")\n",
    "  2. Starts with an inference cue (\"Since\", \"Therefore\", \"This means\", \"We can deduce\", etc.)\n",
    "\n",
    "Produce one object with:\n",
    "  â€¢ \"statement\": the new claim you read in that CoT sentence (don't quote the entire sentenceâ€”just the core inference).\n",
    "  â€¢ \"evidence\":\n",
    "      â€“ if the claim restates a constraint, use the exact line from the **Conditions** block,\n",
    "      â€“ otherwise, use the CoT fragment that you extracted it from.\n",
    "  â€¢ \"Verification\":\n",
    "      â€“ `\"false\"` if the sentence rejects or blocks a condition (contains \"not applicable\", \"does not provide\", etc.),\n",
    "      â€“ otherwise `\"true\"`.\n",
    "\n",
    "Keep the objects in the same order as they appear in the CoT.\n",
    "\n",
    "Example:\n",
    "\n",
    "{demon}\n",
    "\n",
    "Now, given:\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Conditions:\n",
    "{conditions}\n",
    "\n",
    "Chain-of-Thought:\n",
    "{cot}\n",
    "\n",
    "Your output:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTqLGHwcEDnb"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SQIc13TsEHbC"
   },
   "outputs": [],
   "source": [
    "def clean_quotes(t):\n",
    "    return (t.replace('\"','\"').replace('\"','\"').replace(\"'\",\"'\").replace(\"'\",\"'\"))\n",
    "\n",
    "def normalize_text(t):\n",
    "    t = clean_quotes(t)\n",
    "    t = re.sub(r'\\?\\s(?=[A-Z])', ', ', t)\n",
    "    t = re.sub(r'(?<=[a-zA-Z])\\.(?=[A-Z])', '. ', t)\n",
    "    t = re.sub(r'(?<![A-Da-d])\\\\n(?!\\s?[A-Da-d]\\\\.)', ' ', t)\n",
    "    return html.unescape(t).strip()\n",
    "\n",
    "def extract_json(raw):\n",
    "    raw = raw.strip()\n",
    "    i = raw.find('[')\n",
    "    if i < 0: return []\n",
    "    depth = 0\n",
    "    for j,ch in enumerate(raw[i:], i):\n",
    "        if ch=='[': depth+=1\n",
    "        elif ch==']': depth-=1\n",
    "        if depth==0:\n",
    "            blk = raw[i:j+1]\n",
    "            for p in (json.loads, ast.literal_eval):\n",
    "                try: return p(blk)\n",
    "                except: pass\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def score_verifier_batch(prem_list, hyp_list, tok, mod):\n",
    "    enc = tok(prem_list, hyp_list, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = mod(**enc).logits\n",
    "    return torch.softmax(logits, dim=1)[:, 1].tolist()\n",
    "\n",
    "def clean_qp(qp_list):\n",
    "    return [s for s in qp_list if not re.match(r'^[A-Da-d][\\.:\\)]', s.strip()) and \"Option\" not in s and \"following\" not in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu_JRUEPELHW"
   },
   "source": [
    "## Load Models and Verifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162,
     "referenced_widgets": [
      "a88d45c9bc044983b8822b3bbfff039c",
      "6f28565adda54a67ae13078cece6e6d7",
      "df537d101cd94fc7aebefcd396dd89a9",
      "1e3d3273d4eb47938662da9cfdfc38ac",
      "cedefb9571f845c4bcf62bb96f5ef907",
      "33a2f54a189548fba5106f5a1cb93d0e",
      "c9054a5631664cb6bc4e8c252eac2756",
      "c66097b957024ece9063673661c5a1ed",
      "f47770b24df84ad896794e577d5cbee1",
      "3ac9f7517ed24ca6931e0eece686b989",
      "dd77815edefc4727b29b4c1346eac23b",
      "3f01308a20e7467bb09de41449a00322",
      "431f3bac14d44dd3ab0091e13872d119",
      "3b53b3e236c44cf794e7da6a6bbb1d1d",
      "cc104395b54245c691909f65f312a8ab",
      "464b26ac0f75462aa058d5ed3f0308f8",
      "316ec562159a4b3586633b7142697994",
      "043d4add73824643b2ecd8b46e3f4497",
      "514ed85346bb46e19b26255dcec02c7c",
      "2470931686c04429a23a252db80b67e2",
      "d52b2da3ea09492493b03dfb1a0ea381",
      "ce25640074bc4af88ca2c76e971ccf15",
      "55547ea8a05c4bde8080dd815486d893",
      "bcd989cb60834b758570e82dcb3b0843",
      "adae0d4123b24991bef27b7d13c508e8",
      "8277bf8aab454866b3e5b9c6dec88d22",
      "72b6dc28db2c4b6b89fda202b9b96456",
      "158e286295d34186a76da28b5f82b52f",
      "8c788b97493345d4973a7a683e0f69ff",
      "98bf8421c540485492b303cc7a9be08f",
      "05ea59a5cd264567b7039bc5f7586507",
      "28d5eb29e10b4b61a4b741ed4b3b3ae5",
      "cb71add8abfd4cfab0977c58fd58804e"
     ]
    },
    "id": "t8GvgyvbEP4_",
    "outputId": "566f772a-db94-45e0-a7fd-80c221d87bca"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88d45c9bc044983b8822b3bbfff039c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f01308a20e7467bb09de41449a00322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55547ea8a05c4bde8080dd815486d893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# QP LM - deterministic\n",
    "qp_tok = AutoTokenizer.from_pretrained(QP_LM_PATH)\n",
    "qp_tok.model_max_length = 1024\n",
    "qp_mod = AutoModelForCausalLM.from_pretrained(QP_LM_PATH).to(device)\n",
    "qp_pipe = pipeline(\"text-generation\", model=qp_mod, tokenizer=qp_tok,\n",
    "                   return_full_text=False, do_sample=False,\n",
    "                   num_beams=5, early_stopping=True,\n",
    "                   max_new_tokens=512, batch_size=4)\n",
    "\n",
    "# CP LM - Using beam search\n",
    "cp_tok = AutoTokenizer.from_pretrained(CP_LM_PATH)\n",
    "cp_tok.model_max_length = 2048\n",
    "cp_mod = AutoModelForCausalLM.from_pretrained(CP_LM_PATH).to(device)\n",
    "cp_pipe = pipeline(\"text-generation\", model=cp_mod, tokenizer=cp_tok,\n",
    "                   return_full_text=False, do_sample=True,temperature=0.7,\n",
    "                   num_beams=5, early_stopping=True, num_return_sequences=3,\n",
    "                   max_new_tokens=1024, batch_size=4)\n",
    "\n",
    "\n",
    "# Load QP and COT Verifiers\n",
    "def load_verifiers():\n",
    "    global qv_tok, qv_mod, cv_tok, cv_mod\n",
    "    qv_tok = AutoTokenizer.from_pretrained(QP_VER_PATH)\n",
    "    qv_mod = AutoModelForSequenceClassification.from_pretrained(QP_VER_PATH).to(device)\n",
    "    cv_tok = AutoTokenizer.from_pretrained(CP_VER_PATH)\n",
    "    cv_mod = AutoModelForSequenceClassification.from_pretrained(CP_VER_PATH).to(device)\n",
    "    return qv_tok, qv_mod, cv_tok, cv_mod\n",
    "\n",
    "\n",
    "qv_tok, qv_mod, cv_tok, cv_mod = load_verifiers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr5k4oMPES2g"
   },
   "source": [
    "## Hybrid Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9iXVBMzEaPA"
   },
   "outputs": [],
   "source": [
    "def process_one(example):\n",
    "    q_raw, cot_raw = example[\"question\"], example[\"cot\"]\n",
    "    sel_idx, ans   = example.get(\"sel_idx\"), example.get(\"answer\")\n",
    "    q, cot         = normalize_text(q_raw), normalize_text(cot_raw)\n",
    "\n",
    "    # 1) Generate 3 QP candidates and rerank with QP verifier\n",
    "    prompt = QP_TEMPLATE.format(demon=QP_DEMON, question=q)\n",
    "    raw_qp_list = qp_pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=3\n",
    "    )\n",
    "\n",
    "    qp_lists = []\n",
    "    for item in raw_qp_list:\n",
    "        candidate = extract_json(item[\"generated_text\"])\n",
    "        cleaned   = clean_qp(candidate)\n",
    "        qp_lists.append(cleaned)\n",
    "\n",
    "    qp_premises = [q] * len(qp_lists)\n",
    "    qp_hypotheses = [\n",
    "        f\"QuestionParsing: {json.dumps(qp_json, ensure_ascii=False)}\"\n",
    "        for qp_json in qp_lists\n",
    "    ]\n",
    "    qp_scores = score_verifier_batch(qp_premises, qp_hypotheses, qv_tok, qv_mod)\n",
    "\n",
    "    best_qp_idx = int(np.argmax(qp_scores))\n",
    "    if qp_scores[best_qp_idx] < THR_QP:\n",
    "        best_qp = qp_lists[0]\n",
    "    else:\n",
    "        best_qp = qp_lists[best_qp_idx]\n",
    "\n",
    "    # CP: generate 3 beamâ€search parses\n",
    "    conds_str = json.dumps(best_qp, ensure_ascii=False)\n",
    "    prompt_cp = CP_TEMPLATE.format(\n",
    "        demon      = CP_DEMON,\n",
    "        question   = q,\n",
    "        conditions = conds_str,\n",
    "        cot        = cot\n",
    "    )\n",
    "\n",
    "    raw_cp_outs = cp_pipe(prompt_cp, max_new_tokens=1024)\n",
    "    raw_cp_flat = [\n",
    "        item\n",
    "        for sub in raw_cp_outs\n",
    "        for item in (sub if isinstance(sub, list) else [sub])\n",
    "    ]\n",
    "\n",
    "    # 3) Parse & clean each CP candidate\n",
    "    cps_candidates = []\n",
    "    for out in raw_cp_flat:\n",
    "        parsed = extract_json(out[\"generated_text\"])\n",
    "        if not parsed:\n",
    "            continue\n",
    "        seen = set()\n",
    "        cleaned = []\n",
    "        for st in parsed:\n",
    "            stmt = st.get(\"statement\",\"\").strip()\n",
    "            ev   = st.get(\"evidence\",\"\").strip() or \"logical deduction\"\n",
    "            ver  = str(st.get(\"Verification\",\"true\")).lower()\n",
    "            if len(stmt) < 3 or (stmt, ev) in seen:\n",
    "                continue\n",
    "            seen.add((stmt, ev))\n",
    "            cleaned.append({\n",
    "                \"statement\": stmt,\n",
    "                \"evidence\":  ev,\n",
    "                \"Verification\": ver\n",
    "            })\n",
    "        if cleaned:\n",
    "            cps_candidates.append(cleaned)\n",
    "\n",
    "    if not cps_candidates:\n",
    "        cps_candidates = [[]]\n",
    "\n",
    "    # 4) Score each CP candidate with CP verifier \n",
    "    premise = (\n",
    "        f\"Question:\\n{q}\\n\\n\"\n",
    "        f\"Conditions:\\n\" + \"\\n\".join(f\"- {s}\" for s in best_qp) +\n",
    "        f\"\\n\\nCoT:\\n{cot}\"\n",
    "    )\n",
    "    avg_scores = []\n",
    "    for cp_list in cps_candidates:\n",
    "        if not cp_list:\n",
    "            avg_scores.append(0.0)\n",
    "            continue\n",
    "        prems = [premise] * len(cp_list)\n",
    "        hyps  = [\n",
    "            f\"Statement: {st['statement']}\\nBased on: {st['evidence']}\"\n",
    "            for st in cp_list\n",
    "        ]\n",
    "        scores = score_verifier_batch(prems, hyps, cv_tok, cv_mod)\n",
    "        avg_scores.append(sum(scores) / len(scores))\n",
    "\n",
    "    # 5) Pick best CP candidate (with threshold fallback)\n",
    "    best_idx = int(np.argmax(avg_scores))\n",
    "    if avg_scores[best_idx] < THR_CP:\n",
    "        best_idx = 0\n",
    "    best_cp = cps_candidates[best_idx]\n",
    "\n",
    "    return {\n",
    "        \"question\":         q_raw,\n",
    "        \"question_parsing\": best_qp,\n",
    "        \"answer\":           ans,\n",
    "        \"id\":               example[\"id\"],\n",
    "        \"cot\":              cot_raw,\n",
    "        \"cot_parsing\":      best_cp,\n",
    "        \"sel_idx\":          sel_idx\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUzWumL5Ea7Q"
   },
   "source": [
    "## Batch and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "b4ef469649af45a0bdafe352b7b1f74c",
      "5f64350b97ba4342a80e3ce7142f2796",
      "0d972fcc223f4796bf0fa9d83c92e43b",
      "799f6b442e5a4115a70423cc90068f06",
      "1d4f3d0a6cba46ed91725f602e041783",
      "6f6b8f32c3be482199c2d97ec0115dd4",
      "7baa572905e54e559297b99bde63621a",
      "2324a264d2f64c28a196a035f86b7bdc",
      "f140be351cb343feb0ebe6145f584da5",
      "5ce03732c0d74b7aa297ce70f26d25f1",
      "57bd197a7d874c70a791d7e601d842b1",
      "92463ba5e94f4eaba739135a7e592ec3",
      "691f3be6330f45f5b87123ca726f4ab2",
      "7fb1c15cb96043b7b60d8364915d9605",
      "457c5b132aac40d4a09d041844094b16",
      "7c1124b8d6f44c8e99dedf3d4230e1f7",
      "21f55d6134bb4f528fcae811adf526c0",
      "bfcbebc10eaa446790fc081bf1ac83cd",
      "4cc098ff76184322a2c8ccd892a35a5a",
      "adbdfa2fcd44427b97886b76114309da",
      "8d047aeff626427088da51571de7cf86",
      "40aa788d0483421ca16d2bda4bfe254a",
      "05c6204059a94d4184b505260ecb08b6",
      "ec55d047b211461d96321e49990cf8b5",
      "0d71f2ce70ea4370868317ddcd10c699",
      "7ed5475abfa6427fb4a5b6530356bb32",
      "a93a2cadc287426eab6bbeddeabf5a95",
      "43a6c0f0be4f42cea257b445300c51e5",
      "823b5d0d20be440b810c8de69fb35560",
      "16d562551809488a9c3dbc1fae2dfa02",
      "16893d8e947c4657954113b36e1fa7df",
      "e6ae03084c834da897bf3f87257a7a8f",
      "056b6cfc6bae46b7845fe2c61620f6bf"
     ]
    },
    "id": "aWqHd4PrEc21",
    "outputId": "3cf7bade-4c30-4a21-d9d5-5fa60c1383f6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ef469649af45a0bdafe352b7b1f74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92463ba5e94f4eaba739135a7e592ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c6204059a94d4184b505260ecb08b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done â€” saved to /content/drive/MyDrive/llm-sr-project/results_hybrid_approach_with2verifiers.json\n"
     ]
    }
   ],
   "source": [
    "def process_batch(batch):\n",
    "    outs = [process_one({\n",
    "        \"question\": batch[\"question\"][i],\n",
    "        \"cot\":       batch[\"cot\"][i],\n",
    "        \"id\":        batch[\"id\"][i],\n",
    "        \"sel_idx\":   batch.get(\"sel_idx\", [None]*len(batch[\"id\"]))[i],\n",
    "        \"answer\":    batch.get(\"answer\", [None]*len(batch[\"id\"]))[i],\n",
    "    }) for i in range(len(batch[\"question\"]))]\n",
    "\n",
    "    return {\n",
    "        \"question\":        [o[\"question\"]        for o in outs],\n",
    "        \"question_parsing\":[o[\"question_parsing\"]for o in outs],\n",
    "        \"answer\":          [o[\"answer\"]          for o in outs],\n",
    "        \"id\":              [o[\"id\"]              for o in outs],\n",
    "        \"cot\":             [o[\"cot\"]             for o in outs],\n",
    "        \"cot_parsing\":     [o[\"cot_parsing\"]     for o in outs],\n",
    "        \"sel_idx\":         [o[\"sel_idx\"]         for o in outs],\n",
    "    }\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    gc.collect()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    ds = load_dataset(\"json\", data_files={\"test\": INPUT})[\"test\"]\n",
    "\n",
    "    out_ds = ds.map(\n",
    "        process_batch,\n",
    "        batched=True,\n",
    "        batch_size=2,\n",
    "        remove_columns=ds.column_names\n",
    "    )\n",
    "\n",
    "    out_ds.to_json(OUTPUT, orient=\"records\", lines=False)\n",
    "    print(\"âœ… Done â€” saved to\", OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgQ4YqUsEgcK"
   },
   "source": [
    "## Transform Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loPwQZsFEl9w",
    "outputId": "fdcf2075-5d19-458e-dbab-5276f6c10354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 24 examples to /content/drive/MyDrive/llm-sr-project/final_results_hybrid_approach_with2verifiers.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "INPUT_PATH  = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach_with2verifiers.json\"\n",
    "OUTPUT_PATH = \"/content/drive/MyDrive/llm-sr-project/final_results_hybrid_approach_with2verifiers.json\"\n",
    "\n",
    "def transform_example(ex):\n",
    "    # reorder each cot_parsing entry: statement â†’ evidence â†’ Verification\n",
    "    reordered = []\n",
    "    for step in ex.get(\"cot_parsing\", []):\n",
    "        reordered.append({\n",
    "            \"statement\":    step.get(\"statement\"),\n",
    "            \"evidence\":     step.get(\"evidence\"),\n",
    "            \"Verification\": step.get(\"Verification\"),\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"question\":         ex.get(\"question\"),\n",
    "        \"question_parsing\": ex.get(\"question_parsing\"),\n",
    "        \"answer\":           ex.get(\"answer\"),\n",
    "        \"id\":               ex.get(\"id\"),\n",
    "        \"cot\":              ex.get(\"cot\"),\n",
    "        \"cot_parsing\":      reordered,\n",
    "        \"sel_idx\":          ex.get(\"sel_idx\"),\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        examples = json.load(f)\n",
    "\n",
    "    structured = [transform_example(ex) for ex in examples]\n",
    "\n",
    "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(structured, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Wrote {len(structured)} examples to {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRp3r43OEmn4"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSzseeS9EoN0",
    "outputId": "210a3f5e-0b29-49d6-8a5c-8229bd039269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 17:38:24.824655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748799504.846218   16056 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748799504.852727   16056 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "config.json: 100% 1.05k/1.05k [00:00<00:00, 8.52MB/s]\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "model.safetensors: 100% 738M/738M [00:02<00:00, 271MB/s]\n",
      "tokenizer_config.json: 100% 1.28k/1.28k [00:00<00:00, 9.40MB/s]\n",
      "spm.model: 100% 2.46M/2.46M [00:00<00:00, 32.1MB/s]\n",
      "tokenizer.json: 100% 8.66M/8.66M [00:00<00:00, 28.5MB/s]\n",
      "added_tokens.json: 100% 23.0/23.0 [00:00<00:00, 206kB/s]\n",
      "special_tokens_map.json: 100% 286/286 [00:00<00:00, 2.65MB/s]\n",
      "\u001b[?25lTotal number of predictions: \u001b[1;36m24\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0mAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  4%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  8%\u001b[0m \u001b[36m0:00:52\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 12%\u001b[0m \u001b[36m0:01:05\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 17%\u001b[0m \u001b[36m0:01:04\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 21%\u001b[0m \u001b[36m0:00:59\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 25%\u001b[0m \u001b[36m0:00:50\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 29%\u001b[0m \u001b[36m0:00:43\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 33%\u001b[0m \u001b[36m0:00:42\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 38%\u001b[0m \u001b[36m0:00:43\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 42%\u001b[0m \u001b[36m0:00:41\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 46%\u001b[0m \u001b[36m0:00:37\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 50%\u001b[0m \u001b[36m0:00:34\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 54%\u001b[0m \u001b[36m0:00:30\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 58%\u001b[0m \u001b[36m0:00:26\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 62%\u001b[0m \u001b[36m0:00:23\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 67%\u001b[0m \u001b[36m0:00:20\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 71%\u001b[0m \u001b[36m0:00:18\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 75%\u001b[0m \u001b[36m0:00:15\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”\u001b[0m \u001b[35m 79%\u001b[0m \u001b[36m0:00:13\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”\u001b[0m \u001b[35m 83%\u001b[0m \u001b[36m0:00:10\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”\u001b[0m \u001b[35m 88%\u001b[0m \u001b[36m0:00:07\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”\u001b[0m \u001b[35m 92%\u001b[0m \u001b[36m0:00:05\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”\u001b[0m \u001b[35m 96%\u001b[0m \u001b[36m0:00:03\u001b[0m\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n",
      "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[3m           Evaluation Results           \u001b[0m\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“\n",
      "â”ƒ\u001b[35m \u001b[0m\u001b[35mMetric                     \u001b[0m\u001b[35m \u001b[0mâ”ƒ\u001b[35m \u001b[0m\u001b[35mValue \u001b[0m\u001b[35m \u001b[0mâ”ƒ\n",
      "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©\n",
      "â”‚ Question_Macro_F1           â”‚ 0.7321 â”‚\n",
      "â”‚ Statement_Macro_F1          â”‚ 0.3654 â”‚\n",
      "â”‚ Statement_Evidence_Macro_F1 â”‚ 0.1383 â”‚\n",
      "â”‚ Reasoning_F1                â”‚ 0.0946 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "Question_Macro_F1: 0.7321\n",
      "Statement_Macro_F1: 0.3654\n",
      "Statement_Evidence_Macro_F1: 0.1383\n",
      "Reasoning_F1: 0.0946\n"
     ]
    }
   ],
   "source": [
    "EVAL_SCRIPT = \"/content/drive/MyDrive/llm-sr-project/eval.py\"\n",
    "PREDICTION_PATH = \"/content/drive/MyDrive/llm-sr-project/final_results_hybrid_approach_with2verifiers.json\"\n",
    "REFERENCE_PATH = \"/content/drive/MyDrive/llm-sr-project/test-reference.json\"\n",
    "\n",
    "!python {EVAL_SCRIPT} \\\n",
    "  --prediction {PREDICTION_PATH} \\\n",
    "  --reference {REFERENCE_PATH} \\\n",
    "  --question_threshold 0.95 \\\n",
    "  --statement_threshold 0.9 \\\n",
    "  --relation_threshold 0.9"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
