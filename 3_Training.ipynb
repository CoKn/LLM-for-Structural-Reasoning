{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["-5ZELLZYar27","SKqr0OaYavcG","P3lhjITqa2r9","QoAm6HtVb-Nz"],"authorship_tag":"ABX9TyMRX4yGCJo0TYVV643YxIix"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 3. Model Training\n","\n","**Purpose:**  \n","Fine-tune two LLaMA-3-8B-Instruct models (Question Parser and CoT Parser) using LoRA adapters and our ICL prompt templates.\n","\n","**Training Inputs:**  \n","- `train_question_parsing.jsonl`  \n","- `train_cot_parsing.jsonl`  \n","\n","**Key Config:**\n","- LoRA: rank=64, α=16, dropout=0.05\n","- Model: `unsloth/llama-3-8b-Instruct-bnb-4bit`\n","- Epochs: 12, Batch Size: 8 (×2 grad steps)\n","\n","**Outputs:**  \n","- `…/llm-sr-project/finetuned_llama3_qp_parsing/` (LoRA-adapter weights + tokenizer)  \n","- `…/llm-sr-project/finetuned_llama3_cot_parsing/`  \n"],"metadata":{"id":"XiUE-8doWTHb"}},{"cell_type":"markdown","source":["## Environments and Imports"],"metadata":{"id":"-5ZELLZYar27"}},{"cell_type":"code","source":["# Install core evaluation utilities\n","!pip install -q evaluate\n","!pip install json5\n","\n","!pip uninstall -y nltk\n","!pip install -q --upgrade nltk"],"metadata":{"id":"0ZnQj6tZWbMC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CEuFEKUBVwMD"},"outputs":[],"source":["# Install Unsloth for efficient LLM fine-tuning\n","%%capture\n","!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"]},{"cell_type":"code","source":["# ─────────────────────────────────────────────────────────────────────────────\n","# Define In-Context Learning Demonstrations and Prompt Templates\n","# ─────────────────────────────────────────────────────────────────────────────\n","\n","# QP_DEMON: One-shot example for Question Parsing\n","QP_DEMON = '''The question is:\n","\n","There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project. This assignment must satisfy:\n","(1) If A works on Alpha, then B works on Beta.\n","(2) If C works on Alpha, then D and E work on Beta.\n","(3) F works on a different project than E.\n","(4) D must work on a different project than A.\n","(5) If F works on Alpha, then B works on Alpha.\n","\n","If A works on Beta, which of the following must be true?\n","A. B works on Alpha\n","B. C works on Beta\n","C. D works on Alpha\n","D. F works on Beta\n","\n","The parsing result is:\n","\n","[\n","  \"There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project.\",\n","  \"If A works on Alpha, then B works on Beta\",\n","  \"If C works on Alpha, then D and E work on Beta\",\n","  \"F works on a different project than E\",\n","  \"D must work on a different project than A\",\n","  \"If F works on Alpha, then B works on Alpha\",\n","  \"A works on Beta\"\n","]\n","'''\n","\n","# QP_TEMPLATE: Formats a new question with the demonstration\n","QP_TEMPLATE = '''Given a question, extract all relevant information from the question that would help to solve it.\n","\n","This includes:\n","- General setup information (e.g., number of people, projects involved)\n","- Explicit facts given in the question\n","- All logical constraints or conditions\n","\n","Output only a JSON list and nothing else. Follow the format shown in the example.\n","\n","Example:\n","\n","{demon}\n","\n","Now, the question is:\n","\n","{question}\n","\n","Your output:\n","'''\n","\n","# CP_DEMON: One-shot example for Chain-of-Thought Parsing\n","CP_DEMON = '''The question is:\n","\n","There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project.\n","\n","Conditions:\n","(1) If A works on Alpha, then B works on Beta.\n","(2) If C works on Alpha, then D and E work on Beta.\n","(3) F works on a different project than E.\n","(4) D must work on a different project than A.\n","(5) If F works on Alpha, then B works on Alpha.\n","\n","Question:\n","If A works on Beta, which of the following must be true?\n","\n","CoT:\n","Since A works on Beta, Condition (1) is not triggered. Condition (2) is not triggered since C’s assignment is unknown. Condition (3) doesn’t give anything because E’s assignment is unspecified. Condition (4) says D must work on a different project than A, so D must work on Alpha. Condition (5) depends on F, which is unknown.\n","\n","Parsing result:\n","\n","[\n","  {\n","    \"statement\": \"Condition (1) is not applicable\",\n","    \"evidence\": \"Condition (1): If A works on Alpha, then B works on Beta. | A is working on Beta\",\n","    \"Verification\": \"false\"\n","  },\n","  {\n","    \"statement\": \"Condition (2) is not applicable\",\n","    \"evidence\": \"Condition (2): If C works on Alpha, then D and E work on Beta. | C’s assignment is unknown\",\n","    \"Verification\": \"false\"\n","  },\n","  {\n","    \"statement\": \"Condition (3) does not provide any info\",\n","    \"evidence\": \"Condition (3): F works on a different project than E. | E’s assignment is unknown\",\n","    \"Verification\": \"false\"\n","  },\n","  {\n","    \"statement\": \"D must work on Alpha\",\n","    \"evidence\": \"Condition (4): D must work on a different project than A, and A is working on Beta\",\n","    \"Verification\": \"true\"\n","  },\n","  {\n","    \"statement\": \"Condition (5) is not applicable\",\n","    \"evidence\": \"Condition (5): If F works on Alpha, then B works on Alpha. | F’s assignment is unknown\",\n","    \"Verification\": \"false\"\n","  }\n","]\n","'''\n","# CP_TEMPLATE: Formats a question + CoT + conditions for CP model\n","CP_TEMPLATE = '''You are a reasoning assistant. Based on the question, conditions, and chain-of-thought (CoT), extract every inference or non-inference step as a JSON object.\n","\n","For each CoT sentence that either:\n","  1. Refers to a condition (e.g. “Condition (2) …”)\n","  2. Starts with an inference cue (“Since”, “Therefore”, “This means”, “We can deduce”, etc.)\n","\n","Produce one object with:\n","  • \"statement\": the new claim you read in that CoT sentence (don’t quote the entire sentence—just the core inference).\n","  • \"evidence\":\n","      – if the claim restates a constraint, use the exact line from the **Conditions** block,\n","      – otherwise, use the CoT fragment that you extracted it from.\n","  • \"Verification\":\n","      – `\"false\"` if the sentence rejects or blocks a condition (contains “not applicable”, “does not provide”, etc.),\n","      – otherwise `\"true\"`.\n","\n","Keep the objects in the same order as they appear in the CoT.\n","\n","Example:\n","\n","{demon}\n","\n","Now, given:\n","\n","Question:\n","{question}\n","\n","Conditions:\n","{conditions}\n","\n","Chain-of-Thought:\n","{cot}\n","\n","Your output:\n","'''"],"metadata":{"id":"Fp2WsJRwaNIM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import unsloth\n","import os\n","import shutil\n","import torch\n","from datasets import load_dataset\n","from transformers import TrainingArguments, Trainer\n","from unsloth import FastLanguageModel"],"metadata":{"id":"dBjtBhwIWfj-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Factory"],"metadata":{"id":"SKqr0OaYavcG"}},{"cell_type":"code","source":["# Factory to load LLaMA-3 + LoRA adapter\n","def make_lora_model(model_name, max_length):\n","    model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name,\n","        max_seq_length=max_length,\n","        dtype=torch.float16,\n","        load_in_4bit=True\n","    )\n","    model = FastLanguageModel.get_peft_model(\n","        model,\n","        r=64, lora_alpha=16, lora_dropout=0.05,\n","        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n","        bias=\"none\", random_state=42,\n","        max_seq_length=max_length\n","    )\n","    return model, tokenizer\n","\n","# Mask out prompt tokens so only the generated part contributes to loss\n","def preprocess_fn(batch, tokenizer, max_length):\n","    prompt, output = batch[\"input\"], batch[\"output\"]\n","    full = tokenizer(prompt + output,\n","                     truncation=True, padding=\"max_length\", max_length=max_length)\n","    prompt_ids = tokenizer(prompt, truncation=True, max_length=max_length)[\"input_ids\"]\n","    labels = full[\"input_ids\"].copy()\n","    labels[:len(prompt_ids)] = [-100] * len(prompt_ids)\n","    full[\"labels\"] = labels\n","    return full"],"metadata":{"id":"H_U17VTMWiRF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-Tune Question Parsing (QP)"],"metadata":{"id":"P3lhjITqa2r9"}},{"cell_type":"code","source":["# Paths\n","PROJECT_DIR  = \"/content/drive/MyDrive/llm-sr-project\"\n","TRAIN_FILE   = os.path.join(PROJECT_DIR, \"train_question_parsing.jsonl\")\n","MODEL_OUTPUT = os.path.join(PROJECT_DIR, \"qp_model\")\n","MODEL_NAME   = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n","MAX_LEN      = 2048\n","\n","# Load and tokenize\n","ds_qp = load_dataset(\"json\", data_files={\"train\": TRAIN_FILE})[\"train\"]\n","qp_model, qp_tokenizer = make_lora_model(MODEL_NAME, MAX_LEN)\n","\n","qp_tokenizer_ds = ds_qp.map(\n","    lambda x: preprocess_fn(x, qp_tokenizer, max_length=1024),\n","    batched=False, remove_columns=ds_qp.column_names\n",")\n","\n","# Trainer\n","args_qp = TrainingArguments(\n","    output_dir=MODEL_OUTPUT,\n","    num_train_epochs=12,\n","    per_device_train_batch_size=8,\n","    gradient_accumulation_steps=2,\n","    learning_rate=5e-5,\n","    fp16=True,\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    report_to=\"none\",\n","    eval_strategy=\"no\"\n",")\n","trainer_qp = Trainer(\n","    model=qp_model,\n","    args=args_qp,\n","    train_dataset=qp_tokenizer_ds,\n","    tokenizer=qp_tokenizer\n",")\n","trainer_qp.train()\n","\n","# Save\n","lora_qp_dir = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_question_parsing\"\n","\n","qp_model.save_pretrained(lora_qp_dir)\n","qp_tokenizer.save_pretrained(lora_qp_dir)\n","\n","# Create a ZIP archive\n","shutil.make_archive(lora_qp_dir, 'zip', lora_qp_dir)\n","\n","print(f\"✅ QP model saved and zipped at: {lora_qp_dir}.zip\")"],"metadata":{"id":"lW_uh9RGa8iq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-Tune Chain-Of-Thought (CoT) Parsing"],"metadata":{"id":"QoAm6HtVb-Nz"}},{"cell_type":"code","source":["# Load CoT model (using model factory)\n","cot_model, cot_tokenizer = make_lora_model(MODEL_NAME, MAX_LEN)\n","MAX_LEN = 2048\n","\n","# Paths\n","PROJECT_DIR = \"/content/drive/MyDrive/llm-sr-project\"\n","TRAIN_COT_FILE = os.path.join(PROJECT_DIR, \"train_cot_parsing.jsonl\")\n","COT_MODEL_OUTPUT = os.path.join(PROJECT_DIR, \"cot_model\")\n","\n","\n","# Load CoT dataset\n","cot_data = load_dataset(\"json\", data_files={\n","    \"train\": TRAIN_COT_FILE,\n","})\n","\n","cot_tokenizer_ds = cot_data.map(\n","    lambda x: preprocess_fn(x, cot_tokenizer, MAX_LEN),\n","    batched=False, remove_columns=cot_data.column_names\n",")\n","\n","# Training\n","cot_args = TrainingArguments(\n","    output_dir = COT_MODEL_OUTPUT,\n","    per_device_train_batch_size = 8,\n","    gradient_accumulation_steps = 2,\n","    learning_rate = 5e-5,\n","    num_train_epochs = 12,\n","    fp16 = True,\n","    save_strategy = \"epoch\",\n","    eval_strategy = \"no\",\n","    logging_strategy = \"epoch\",\n","    logging_steps = 1,\n","    save_total_limit = 2,\n","    load_best_model_at_end = False,\n","    metric_for_best_model = \"loss\",\n","    greater_is_better = False,\n","    report_to = \"none\",\n",")\n","\n","cot_trainer = Trainer(\n","    model = cot_model,\n","    args = cot_args,\n","    train_dataset = cot_tokenizer_ds,\n","    eval_dataset= None,\n","    tokenizer = cot_tokenizer,\n","    callbacks = None\n",")\n","\n","cot_trainer.train()\n","\n","# Save\n","lora_cot_dir = os.path.join(PROJECT_DIR, \"finetuned_llama3_cot_parsing\")\n","\n","cot_model.save_pretrained(lora_cot_dir)\n","cot_tokenizer.save_pretrained(lora_cot_dir)\n","\n","# Zip the directory\n","shutil.make_archive(lora_cot_dir, 'zip', lora_cot_dir)\n","\n","print(f\"✅ CoT model saved and zipped at: {lora_cot_dir}.zip\")"],"metadata":{"id":"qUwzcDCIcJXo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Training complete!\n","\n","- QP model → `…/llm-sr-project/finetuned_llama3_question_parsing/`\n","- CoT model → `…/llm-sr-project/finetuned_llama3_cot_parsing/`\n","\n","Proceed to **4_Evaluation.ipynb** for inference and metrics.\n"],"metadata":{"id":"J_F9jPtXddAc"}}]}