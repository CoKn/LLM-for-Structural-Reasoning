{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 14. Hybrid Inference Strategy 4: Structured Reasoning + Verifier-Guided Scoring\n",
        "\n",
        "**Objective**  \n",
        "Balance rule-based logic and verifier trust by adding structured dependency mapping and weighted scoring heuristics atop the v3 pipeline.\n",
        "\n",
        "1. **Question Parsing (QP)**  \n",
        "   - Deterministic LLaMA-3 beam-search parse.  \n",
        "   - No verifier filtering.\n",
        "\n",
        "2. **CoT Parsing (CP)**  \n",
        "   - Generate 5 candidates (2 beam + 3 sampled).  \n",
        "   - Parse into `{statement, evidence, Verification}`.\n",
        "\n",
        "3. **Advanced Post-Processing**  \n",
        "   - **Enhanced Evidence Extraction** via condition references & entity overlap.  \n",
        "   - **Strict Verification Normalization** to `\"true\"`/`\"false\"` patterns.  \n",
        "   - **Reasoning Chains**: add `reasoning_step` and `related_to`.\n",
        "\n",
        "4. **Candidate Scoring**  \n",
        "   - Compute a **weighted score** combining:\n",
        "     - Step count & verification balance  \n",
        "     - Structure (intra-step connections)  \n",
        "     - Evidence length/clarity  \n",
        "     - CP verifier logits (scaled)\n",
        "\n",
        "5. **Output**  \n",
        "   Same JSON schema as before.\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "| Metric                         | v4 Score |\n",
        "|--------------------------------|----------|\n",
        "| **Question_Macro_F1**          | 0.7658   |\n",
        "| **Statement_Macro_F1**         | 0.4185   |\n",
        "| **Statement_Evidence_Macro_F1**| 0.1214   |\n",
        "| **Reasoning_F1**               | 0.0939   |\n",
        "\n",
        "> _v4 achieves the highest Statement_F1 yet but with lower evidence/reasoning metrics, reflecting its stricter structural filtering and heuristic scoring._\n",
        "\n"
      ],
      "metadata": {
        "id": "aQARHNVhYvMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Thresholds"
      ],
      "metadata": {
        "id": "jsDxx8bCY2WI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7PnHO_6YusM",
        "outputId": "5656f386-49e6-40e0-88df-fbeccfef25bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting json5\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Downloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: json5\n",
            "Successfully installed json5-0.12.0\n",
            "Found existing installation: nltk 3.9.1\n",
            "Uninstalling nltk-3.9.1:\n",
            "  Successfully uninstalled nltk-3.9.1\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install core evaluation utilities\n",
        "!pip install -q evaluate\n",
        "!pip install json5\n",
        "\n",
        "!pip uninstall -y nltk\n",
        "!pip install -q --upgrade nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XITLIZnfY603",
        "outputId": "5f7f7675-b026-4c24-f5fb-a9cbbe9383e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth  # Must come first for 4-bit LoRA\n",
        "import torch, gc, json, re, ast, html, numpy as np\n",
        "from torch.nn.functional import log_softmax\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "\n",
        "# Paths & thresholds\n",
        "INPUT       = \"/content/drive/MyDrive/llm-sr-project/testingData-blank.json\"\n",
        "OUTPUT      = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach5.json\"\n",
        "QP_LM_PATH  = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_question_parsing\"\n",
        "CP_LM_PATH  = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_cot_parsing\"\n",
        "QP_VER_PATH = \"/content/drive/MyDrive/deberta-qparse-verifier\"\n",
        "CP_VER_PATH = \"/content/drive/MyDrive/deberta-cotparse-verifier\"\n",
        "\n",
        "\n",
        "THR_QP = 0.70\n",
        "THR_CP = 0.58\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "nxEVmAsXZN-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "ZzQ07uVRZT0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In-Context Learning (ICL) Prompts\n",
        "\n",
        "QP_DEMON = '''The question is:\n",
        "\n",
        "There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project. This assignment must satisfy:\n",
        "(1) If A works on Alpha, then B works on Beta.\n",
        "(2) If C works on Alpha, then D and E work on Beta.\n",
        "(3) F works on a different project than E.\n",
        "(4) D must work on a different project than A.\n",
        "(5) If F works on Alpha, then B works on Alpha.\n",
        "\n",
        "If A works on Beta, which of the following must be true?\n",
        "A. B works on Alpha\n",
        "B. C works on Beta\n",
        "C. D works on Alpha\n",
        "D. F works on Beta\n",
        "\n",
        "The parsing result is:\n",
        "\n",
        "[\n",
        "  \"There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project.\",\n",
        "  \"If A works on Alpha, then B works on Beta\",\n",
        "  \"If C works on Alpha, then D and E work on Beta\",\n",
        "  \"F works on a different project than E\",\n",
        "  \"D must work on a different project than A\",\n",
        "  \"If F works on Alpha, then B works on Alpha\",\n",
        "  \"A works on Beta\"\n",
        "]\n",
        "'''\n",
        "\n",
        "QP_TEMPLATE = '''Given a question, extract all relevant information from the question that would help to solve it.\n",
        "\n",
        "This includes:\n",
        "- General setup information (e.g., number of people, projects involved)\n",
        "- Explicit facts given in the question\n",
        "- All logical constraints or conditions\n",
        "\n",
        "Output only a JSON list and nothing else. Follow the format shown in the example.\n",
        "\n",
        "Example:\n",
        "\n",
        "{demon}\n",
        "\n",
        "Now, the question is:\n",
        "\n",
        "{question}\n",
        "\n",
        "Your output:\n",
        "'''\n",
        "\n",
        "CP_DEMON = '''The question is:\n",
        "\n",
        "There are 6 volunteers: A, B, C, D, E and F. Each person works on exactly one project.\n",
        "\n",
        "Conditions:\n",
        "(1) If A works on Alpha, then B works on Beta.\n",
        "(2) If C works on Alpha, then D and E work on Beta.\n",
        "(3) F works on a different project than E.\n",
        "(4) D must work on a different project than A.\n",
        "(5) If F works on Alpha, then B works on Alpha.\n",
        "\n",
        "Question:\n",
        "If A works on Beta, which of the following must be true?\n",
        "\n",
        "CoT:\n",
        "Since A works on Beta, Condition (1) is not triggered. Condition (2) is not triggered since C's assignment is unknown. Condition (3) doesn't give anything because E's assignment is unspecified. Condition (4) says D must work on a different project than A, so D must work on Alpha. Condition (5) depends on F, which is unknown.\n",
        "\n",
        "Parsing result:\n",
        "\n",
        "[\n",
        "  {\n",
        "    \"statement\": \"Condition (1) is not applicable\",\n",
        "    \"evidence\": \"Condition (1): If A works on Alpha, then B works on Beta. | A is working on Beta\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (2) is not applicable\",\n",
        "    \"evidence\": \"Condition (2): If C works on Alpha, then D and E work on Beta. | C's assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (3) does not provide any info\",\n",
        "    \"evidence\": \"Condition (3): F works on a different project than E. | E's assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"D must work on Alpha\",\n",
        "    \"evidence\": \"Condition (4): D must work on a different project than A, and A is working on Beta\",\n",
        "    \"Verification\": \"true\"\n",
        "  },\n",
        "  {\n",
        "    \"statement\": \"Condition (5) is not applicable\",\n",
        "    \"evidence\": \"Condition (5): If F works on Alpha, then B works on Alpha. | F's assignment is unknown\",\n",
        "    \"Verification\": \"false\"\n",
        "  }\n",
        "]\n",
        "'''\n",
        "\n",
        "CP_TEMPLATE = '''You are a reasoning assistant. Based on the question, conditions, and chain-of-thought (CoT), extract every inference or non-inference step as a JSON object.\n",
        "\n",
        "For each CoT sentence that either:\n",
        "  1. Refers to a condition (e.g. \"Condition (2) …\")\n",
        "  2. Starts with an inference cue (\"Since\", \"Therefore\", \"This means\", \"We can deduce\", etc.)\n",
        "\n",
        "Produce one object with:\n",
        "  • \"statement\": the new claim you read in that CoT sentence (don't quote the entire sentence—just the core inference).\n",
        "  • \"evidence\":\n",
        "      – if the claim restates a constraint, use the exact line from the **Conditions** block,\n",
        "      – otherwise, use the CoT fragment that you extracted it from.\n",
        "  • \"Verification\":\n",
        "      – MUST BE EXACTLY `\"false\"` if the sentence rejects or blocks a condition (contains \"not applicable\", \"does not provide\", etc.),\n",
        "      – MUST BE EXACTLY `\"true\"` in all other cases.\n",
        "  • \"reasoning_step\": number indicating the logical step in the reasoning chain (starting from 1)\n",
        "  • \"related_to\": [list of numbers] indicating which previous steps this one builds upon (optional)\n",
        "\n",
        "Keep the objects in the same order as they appear in the CoT.\n",
        "\n",
        "IMPORTANT: \"Verification\" field MUST ONLY contain the string \"true\" or \"false\" (lowercase) and nothing else.\n",
        "\n",
        "Example:\n",
        "\n",
        "{demon}\n",
        "\n",
        "Now, given:\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Conditions:\n",
        "{conditions}\n",
        "\n",
        "Chain-of-Thought:\n",
        "{cot}\n",
        "\n",
        "Your output:\n",
        "'''"
      ],
      "metadata": {
        "id": "yfUQNXEBZTU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "Po6v5peNZZtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_quotes(t):\n",
        "    return (t.replace('\"','\"').replace('\"','\"').replace(\"'\",\"'\").replace(\"'\",\"'\"))\n",
        "\n",
        "def normalize_text(t):\n",
        "    t = clean_quotes(t)\n",
        "    t = re.sub(r'\\?\\s(?=[A-Z])', ', ', t)\n",
        "    t = re.sub(r'(?<=[a-zA-Z])\\.(?=[A-Z])', '. ', t)\n",
        "    t = re.sub(r'(?<![A-Da-d])\\\\n(?!\\s?[A-Da-d]\\\\.)', ' ', t)\n",
        "    return html.unescape(t).strip()\n",
        "\n",
        "def extract_json(raw):\n",
        "    raw = raw.strip()\n",
        "    i = raw.find('[')\n",
        "    if i < 0: return []\n",
        "    depth = 0\n",
        "    for j,ch in enumerate(raw[i:], i):\n",
        "        if ch=='[': depth+=1\n",
        "        elif ch==']': depth-=1\n",
        "        if depth==0:\n",
        "            blk = raw[i:j+1]\n",
        "            for p in (json.loads, ast.literal_eval):\n",
        "                try: return p(blk)\n",
        "                except: pass\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "def score_verifier_batch(prem_list, hyp_list, tok, mod):\n",
        "    enc = tok(prem_list, hyp_list, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = mod(**enc).logits\n",
        "    return torch.softmax(logits, dim=1)[:, 1].tolist()\n",
        "\n",
        "def clean_qp(qp_list):\n",
        "    return [s for s in qp_list if not re.match(r'^[A-Da-d][\\.:\\)]', s.strip()) and \"Option\" not in s and \"following\" not in s]\n",
        "\n",
        "# Optimized evidence enhancement function - focused on condition matching and CoT coherence\n",
        "def enhance_evidence(statement, evidence, cot, conditions):\n",
        "    # If evidence is already meaningful, preserve it\n",
        "    if evidence and len(evidence.strip()) >= 20:\n",
        "        return evidence\n",
        "\n",
        "    # Check for condition references\n",
        "    condition_match = re.search(r'(?:condition|constraint)\\s*\\(?(\\d+)\\)?', statement.lower())\n",
        "    if condition_match:\n",
        "        condition_num = int(condition_match.group(1))\n",
        "        if 1 <= condition_num <= len(conditions):\n",
        "            # Check for not applicable cases\n",
        "            if any(x in statement.lower() for x in [\"not applicable\", \"not triggered\", \"doesn't apply\", \"no information\"]):\n",
        "                return f\"{conditions[condition_num-1]} | This is not applicable because the condition's premise is not satisfied.\"\n",
        "            return conditions[condition_num-1]\n",
        "\n",
        "    # For reasoning steps, extract key entities and find relevant CoT sentences\n",
        "    statement_entities = set(re.findall(r'\\b[A-F]\\b', statement))\n",
        "    if statement_entities:\n",
        "        sentences = re.split(r'[.!?]', cot)\n",
        "        for sent in sentences:\n",
        "            sent = sent.strip()\n",
        "            if len(sent) < 10:\n",
        "                continue\n",
        "\n",
        "            sent_entities = set(re.findall(r'\\b[A-F]\\b', sent))\n",
        "            if sent_entities and len(statement_entities.intersection(sent_entities)) >= 1:\n",
        "                if \"must\" in statement.lower() and \"must\" in sent.lower():\n",
        "                    # High priority evidence for inference steps\n",
        "                    return sent + \".\"\n",
        "                if len(statement_entities.intersection(sent_entities)) >= 2:\n",
        "                    # Good evidence with multiple entity matches\n",
        "                    return sent + \".\"\n",
        "\n",
        "    # Extract key terms from statement for a fallback match\n",
        "    statement_kw = set(re.findall(r'\\b[a-zA-Z]{4,}\\b', statement.lower()))\n",
        "    sentences = re.split(r'[.!?]', cot)\n",
        "\n",
        "    best_score = 0\n",
        "    best_sent = \"\"\n",
        "    for sent in sentences:\n",
        "        sent = sent.strip()\n",
        "        if len(sent) < 10:\n",
        "            continue\n",
        "\n",
        "        sent_kw = set(re.findall(r'\\b[a-zA-Z]{4,}\\b', sent.lower()))\n",
        "        overlap = len(statement_kw.intersection(sent_kw))\n",
        "\n",
        "        if overlap > best_score:\n",
        "            best_score = overlap\n",
        "            best_sent = sent\n",
        "\n",
        "    if best_score >= 2:\n",
        "        return best_sent + \".\"\n",
        "\n",
        "    # Keep original evidence if nothing better found\n",
        "    return evidence or \"Based on logical analysis.\"\n",
        "\n",
        "# Improved verification function to ensure proper values without over-filtering\n",
        "def normalize_verification(statement, verification):\n",
        "    # If verification field is missing or invalid, infer from statement content\n",
        "    if verification not in [\"true\", \"false\"]:\n",
        "        # Detect patterns suggesting inapplicability\n",
        "        if any(x in statement.lower() for x in [\n",
        "                \"not applicable\", \"doesn't apply\", \"does not apply\",\n",
        "                \"not triggered\", \"no information\", \"doesn't provide\",\n",
        "                \"doesn't give\", \"cannot determine\"\n",
        "            ]):\n",
        "            return \"false\"\n",
        "        return \"true\"  # Default to true if no negative indicators\n",
        "\n",
        "    return verification\n",
        "\n",
        "# Enhanced function for building reasoning connections\n",
        "def build_reasoning_connections(steps):\n",
        "    # First pass - assign step numbers\n",
        "    for i, step in enumerate(steps):\n",
        "        step[\"reasoning_step\"] = i + 1\n",
        "\n",
        "    # Second pass - build connections based on entities and topics\n",
        "    for i, curr_step in enumerate(steps):\n",
        "        if i == 0:\n",
        "            continue  # Skip first step\n",
        "\n",
        "        # Extract entities from current step\n",
        "        curr_entities = set(re.findall(r'\\b[A-F]\\b', curr_step[\"statement\"]))\n",
        "\n",
        "        # Only add dependencies for inference steps (typically marked \"true\")\n",
        "        if curr_step.get(\"Verification\") == \"true\":\n",
        "            related_steps = []\n",
        "\n",
        "            # Find related previous steps\n",
        "            for j, prev_step in enumerate(steps[:i]):\n",
        "                # Extract entities from previous step\n",
        "                prev_entities = set(re.findall(r'\\b[A-F]\\b', prev_step[\"statement\"]))\n",
        "\n",
        "                # Check for entity overlap - strong indicator of dependency\n",
        "                if prev_entities and curr_entities and prev_entities.intersection(curr_entities):\n",
        "                    related_steps.append(j + 1)\n",
        "\n",
        "                # Check for condition reference in inference\n",
        "                if prev_step.get(\"Verification\") == \"false\" and \"condition\" in prev_step[\"statement\"].lower():\n",
        "                    condition_match = re.search(r'condition\\s*\\(?(\\d+)\\)?', prev_step[\"statement\"].lower())\n",
        "                    if condition_match:\n",
        "                        # Potential dependency on condition state\n",
        "                        related_steps.append(j + 1)\n",
        "\n",
        "            # Add unique dependencies\n",
        "            if related_steps:\n",
        "                curr_step[\"related_to\"] = sorted(set(related_steps))\n",
        "\n",
        "    return steps\n",
        "\n",
        "# Custom weighted selection that balances verifier with evidence and reasoning quality\n",
        "def compute_candidate_score(candidate, verifier_scores=None):\n",
        "    if not candidate:\n",
        "        return -float('inf')  # Heavily penalize empty candidates\n",
        "\n",
        "    # Calculate quality metrics\n",
        "    metrics = {\n",
        "        \"num_steps\": len(candidate),\n",
        "        \"true_steps\": sum(1 for s in candidate if s.get(\"Verification\") == \"true\"),\n",
        "        \"has_reasoning\": any(\"reasoning_step\" in s for s in candidate),\n",
        "        \"has_connections\": sum(1 for s in candidate if \"related_to\" in s),\n",
        "        \"avg_evidence_len\": np.mean([len(s.get(\"evidence\", \"\")) for s in candidate]),\n",
        "    }\n",
        "\n",
        "    # Combine metrics with carefully tuned weights\n",
        "    base_score = (\n",
        "        metrics[\"num_steps\"] * 0.2 +  # More steps is good\n",
        "        metrics[\"true_steps\"] * 0.3 +  # True verifications are important\n",
        "        (metrics[\"true_steps\"] > 0) * 1.0 +  # Must have at least one true step\n",
        "        (metrics[\"has_connections\"] / max(1, metrics[\"num_steps\"])) * 1.5 +  # More connections is good\n",
        "        min(1.0, metrics[\"avg_evidence_len\"] / 50.0) * 0.5  # Better evidence is good (up to a point)\n",
        "    )\n",
        "\n",
        "    # Incorporate verifier influence with reduced weight\n",
        "    if verifier_scores and len(verifier_scores) > 0:\n",
        "        # Use mean rather than sum to keep scale consistent regardless of candidate length\n",
        "        verifier_score = np.mean([score for score in verifier_scores])\n",
        "        combined_score = (0.65 * base_score) + (0.35 * verifier_score * 5.0)\n",
        "        return combined_score\n",
        "\n",
        "    return base_score"
      ],
      "metadata": {
        "id": "2cD5BNT8ZhbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Models and Verifiers"
      ],
      "metadata": {
        "id": "vPFC2N_cZngb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# QP LM - Using beam search\n",
        "qp_tok = AutoTokenizer.from_pretrained(QP_LM_PATH)\n",
        "qp_tok.model_max_length = 1024\n",
        "qp_mod = AutoModelForCausalLM.from_pretrained(QP_LM_PATH).to(device)\n",
        "qp_pipe = pipeline(\"text-generation\", model=qp_mod, tokenizer=qp_tok,\n",
        "                   return_full_text=False, do_sample=False,\n",
        "                   num_beams=5, early_stopping=True,       =\n",
        "                   max_new_tokens=512, batch_size=4)\n",
        "\n",
        "# CP LM - Using beam search\n",
        "cp_tok = AutoTokenizer.from_pretrained(CP_LM_PATH)\n",
        "cp_tok.model_max_length = 2048\n",
        "cp_mod = AutoModelForCausalLM.from_pretrained(CP_LM_PATH).to(device)\n",
        "\n",
        "# Generate 2 beam candidates + 3 sampled candidates = 5 total\n",
        "cp_pipe = pipeline(\n",
        "     \"text-generation\",\n",
        "     model=cp_mod, tokenizer=cp_tok,\n",
        "     return_full_text=False,\n",
        "     do_sample=False, num_beams=5, num_return_sequences=2,\n",
        "     max_new_tokens=1024,\n",
        "     batch_size=4\n",
        ")\n",
        "\n",
        "cp_sampler = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=cp_mod, tokenizer=cp_tok,\n",
        "    return_full_text=False,\n",
        "     do_sample=True, temperature=0.8, num_return_sequences=3,\n",
        "     max_new_tokens=1024,\n",
        "     batch_size=4\n",
        ")\n",
        "\n",
        "# Load Verifiers\n",
        "def load_verifiers():\n",
        "    global qv_tok, qv_mod, cv_tok, cv_mod\n",
        "    qv_tok = AutoTokenizer.from_pretrained(QP_VER_PATH)\n",
        "    qv_mod = AutoModelForSequenceClassification.from_pretrained(QP_VER_PATH).to(device)\n",
        "    cv_tok = AutoTokenizer.from_pretrained(CP_VER_PATH)\n",
        "    cv_mod = AutoModelForSequenceClassification.from_pretrained(CP_VER_PATH).to(device)\n",
        "    return qv_tok, qv_mod, cv_tok, cv_mod\n",
        "\n",
        "\n",
        "qv_tok, qv_mod, cv_tok, cv_mod = load_verifiers()"
      ],
      "metadata": {
        "id": "32SdiRofZnMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Inference Function"
      ],
      "metadata": {
        "id": "hHXC8JGxZyS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid inference approach v4\n",
        "def process_one(example):\n",
        "    q_raw, cot_raw = example[\"question\"], example[\"cot\"]\n",
        "    sel_idx, ans = example.get(\"sel_idx\"), example.get(\"answer\")\n",
        "    q, cot = normalize_text(q_raw), normalize_text(cot_raw)\n",
        "\n",
        "    # Generate QP output\n",
        "    prompt = QP_TEMPLATE.format(demon=QP_DEMON, question=q)\n",
        "    raw_qp = qp_pipe(prompt, max_new_tokens=512)\n",
        "    if not isinstance(raw_qp, list):\n",
        "        raw_qp = [raw_qp]\n",
        "    best_qp = clean_qp(extract_json(raw_qp[0][\"generated_text\"]))\n",
        "\n",
        "    # Generate CP parses with standard approach\n",
        "    conds_str = json.dumps(best_qp, ensure_ascii=False)\n",
        "    prompt_cp = CP_TEMPLATE.format(\n",
        "        demon      = CP_DEMON,\n",
        "        question   = q,\n",
        "        conditions = conds_str,\n",
        "        cot        = cot\n",
        "    )\n",
        "\n",
        "    # Get 5 candidates (2 beam + 3 sampled)\n",
        "    raw_beams = cp_pipe(prompt_cp, max_new_tokens=1024)\n",
        "    raw_samples = cp_sampler(prompt_cp, max_new_tokens=1024)\n",
        "    raw_cp_outs = (raw_beams if isinstance(raw_beams, list) else [raw_beams]) \\\n",
        "            + (raw_samples if isinstance(raw_samples, list) else [raw_samples])\n",
        "\n",
        "    # Flatten outputs\n",
        "    raw_cp_flat = [\n",
        "        item\n",
        "        for sub in raw_cp_outs\n",
        "        for item in (sub if isinstance(sub, list) else [sub])\n",
        "    ]\n",
        "\n",
        "    # Parse candidates with improved processing\n",
        "    cps_candidates = []\n",
        "    for out in raw_cp_flat:\n",
        "        parsed = extract_json(out[\"generated_text\"])\n",
        "        if not parsed:\n",
        "            continue\n",
        "\n",
        "        seen = set()\n",
        "        cleaned = []\n",
        "\n",
        "        for st in parsed:\n",
        "            # Handle unexpected objects\n",
        "            if not isinstance(st, dict):\n",
        "                continue\n",
        "\n",
        "            stmt = st.get(\"statement\",\"\").strip()\n",
        "            ev   = st.get(\"evidence\",\"\").strip() or \"logical deduction\"\n",
        "\n",
        "            # Skip very short or duplicate statements\n",
        "            if len(stmt) < 3 or (stmt, ev) in seen:\n",
        "                continue\n",
        "\n",
        "            # Enhance evidence quality using context information\n",
        "            ev = enhance_evidence(stmt, ev, cot, best_qp)\n",
        "\n",
        "            # Normalize verification value\n",
        "            ver = normalize_verification(stmt, st.get(\"Verification\",\"true\"))\n",
        "\n",
        "            # Store the step\n",
        "            seen.add((stmt, ev))\n",
        "            cleaned.append({\"statement\": stmt, \"evidence\": ev, \"Verification\": ver})\n",
        "\n",
        "            # Preserve any existing reasoning metadata\n",
        "            if \"reasoning_step\" in st:\n",
        "                cleaned[-1][\"reasoning_step\"] = st[\"reasoning_step\"]\n",
        "\n",
        "            if \"related_to\" in st:\n",
        "                cleaned[-1][\"related_to\"] = st[\"related_to\"]\n",
        "\n",
        "        if cleaned:\n",
        "            # Only apply if the candidate doesn't already have reasoning structure\n",
        "            if not any(\"related_to\" in step for step in cleaned):\n",
        "                cleaned = build_reasoning_connections(cleaned)\n",
        "\n",
        "            cps_candidates.append(cleaned)\n",
        "\n",
        "    # Fallback for empty results\n",
        "    if not cps_candidates:\n",
        "        cps_candidates = [[]]\n",
        "\n",
        "    # Score candidates with verifier + quality metrics\n",
        "    premise = f\"Question:\\n{q}\\n\\nConditions:\\n\" + \"\\n\".join(f\"- {s}\" for s in best_qp) + f\"\\n\\nCoT:\\n{cot}\"\n",
        "\n",
        "    # Get verifier scores for each candidate\n",
        "    verifier_scores_by_candidate = []\n",
        "    for cp_list in cps_candidates:\n",
        "        if not cp_list:\n",
        "            verifier_scores_by_candidate.append([])\n",
        "            continue\n",
        "\n",
        "        prems = [premise] * len(cp_list)\n",
        "        hyps = [f\"Statement: {st['statement']}\\nEvidence: {st['evidence']}\" for st in cp_list]\n",
        "        scores = score_verifier_batch(prems, hyps, cv_tok, cv_mod)\n",
        "        verifier_scores_by_candidate.append(scores)\n",
        "\n",
        "    # Compute weighted scores that balance verifier with quality metrics\n",
        "    combined_scores = []\n",
        "    for i, candidate in enumerate(cps_candidates):\n",
        "        verifier_scores = verifier_scores_by_candidate[i] if i < len(verifier_scores_by_candidate) else []\n",
        "        score = compute_candidate_score(candidate, verifier_scores)\n",
        "        combined_scores.append(score)\n",
        "\n",
        "    # Select best candidate\n",
        "    best_idx = int(np.argmax(combined_scores)) if combined_scores else 0\n",
        "    best_cp = cps_candidates[best_idx] if cps_candidates and best_idx < len(cps_candidates) else []\n",
        "\n",
        "\n",
        "    for i, step in enumerate(best_cp):\n",
        "        step[\"reasoning_step\"] = i + 1\n",
        "\n",
        "    return {\n",
        "        \"question\": q_raw,\n",
        "        \"question_parsing\": best_qp,\n",
        "        \"answer\": ans,\n",
        "        \"id\": example[\"id\"],\n",
        "        \"cot\": cot_raw,\n",
        "        \"cot_parsing\": best_cp,\n",
        "        \"sel_idx\": sel_idx\n",
        "    }"
      ],
      "metadata": {
        "id": "C6iidylfZ1mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch and Run"
      ],
      "metadata": {
        "id": "nMRav1yeZ5wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(batch):\n",
        "    outs = [process_one({\n",
        "        \"question\": batch[\"question\"][i],\n",
        "        \"cot\":      batch[\"cot\"][i],\n",
        "        \"id\":       batch[\"id\"][i],\n",
        "        \"sel_idx\":  batch.get(\"sel_idx\", [None]*len(batch[\"id\"]))[i],\n",
        "        \"answer\":   batch.get(\"answer\", [None]*len(batch[\"id\"]))[i],\n",
        "    }) for i in range(len(batch[\"question\"]))]\n",
        "\n",
        "    return {\n",
        "        \"question\":        [o[\"question\"]        for o in outs],\n",
        "        \"question_parsing\":[o[\"question_parsing\"]for o in outs],\n",
        "        \"answer\":          [o[\"answer\"]          for o in outs],\n",
        "        \"id\":              [o[\"id\"]              for o in outs],\n",
        "        \"cot\":             [o[\"cot\"]             for o in outs],\n",
        "        \"cot_parsing\":     [o[\"cot_parsing\"]     for o in outs],\n",
        "        \"sel_idx\":         [o[\"sel_idx\"]         for o in outs],\n",
        "    }\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    gc.collect()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        ds = load_dataset(\"json\", data_files={\"test\": INPUT})[\"test\"]\n",
        "        print(f\"Loaded dataset with {len(ds)} examples\")\n",
        "\n",
        "        # Add more descriptive logging\n",
        "        print(\"Starting batch processing with optimally balanced verifier...\")\n",
        "        print(f\"Target metrics to exceed: QP_F1=0.7526, ST_F1=0.4015, ST_EV_F1=0.1849, RSN_F1=0.1405\")\n",
        "\n",
        "        out_ds = ds.map(\n",
        "            process_batch,\n",
        "            batched=True,\n",
        "            batch_size=2,\n",
        "            remove_columns=ds.column_names\n",
        "        )\n",
        "\n",
        "        print(f\"Processing complete. Writing results to {OUTPUT}\")\n",
        "        out_ds.to_json(OUTPUT, orient=\"records\", lines=False)\n",
        "        print(\"✅ Done — saved to\", OUTPUT)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {type(e).__name__}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "f8fhehYuZ8_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform Predictions"
      ],
      "metadata": {
        "id": "GDOGhyQ5Z_3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "INPUT_PATH  = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach5.json\"\n",
        "OUTPUT_PATH =\"/content/drive/MyDrive/llm-sr-project/final_results_hybrid_approach5.json\"\n",
        "\n",
        "def transform_example(ex):\n",
        "    # reorder each cot_parsing entry: statement → evidence → Verification\n",
        "    reordered = []\n",
        "    for step in ex.get(\"cot_parsing\", []):\n",
        "        reordered.append({\n",
        "            \"statement\":    step.get(\"statement\"),\n",
        "            \"evidence\":     step.get(\"evidence\"),\n",
        "            \"Verification\": step.get(\"Verification\"),\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"question\":         ex.get(\"question\"),\n",
        "        \"question_parsing\": ex.get(\"question_parsing\"),\n",
        "        \"answer\":           ex.get(\"answer\"),\n",
        "        \"id\":               ex.get(\"id\"),\n",
        "        \"cot\":              ex.get(\"cot\"),\n",
        "        \"cot_parsing\":      reordered,\n",
        "        \"sel_idx\":          ex.get(\"sel_idx\"),\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        examples = json.load(f)\n",
        "\n",
        "    structured = [transform_example(ex) for ex in examples]\n",
        "\n",
        "    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(structured, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Wrote {len(structured)} examples to {OUTPUT_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "qoo6rneyaB5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "ovLY1yNPaD0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EVAL_SCRIPT = \"/content/drive/MyDrive/llm-sr-project/eval.py\"\n",
        "PREDICTION_PATH = \"/content/drive/MyDrive/llm-sr-project/results_hybrid_approach5.json\"\n",
        "REFERENCE_PATH = \"/content/drive/MyDrive/llm-sr-project/test-reference.json\"\n",
        "\n",
        "!python {EVAL_SCRIPT} \\\n",
        "  --prediction {PREDICTION_PATH} \\\n",
        "  --reference {REFERENCE_PATH} \\\n",
        "  --question_threshold 0.95 \\\n",
        "  --statement_threshold 0.9 \\\n",
        "  --relation_threshold 0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5vqPuAvaGhh",
        "outputId": "1ad705a0-0499-4b80-ff6f-8f154c2deaec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-17 18:02:50.039021: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-05-17 18:02:50.058879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747504970.080446    5563 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747504970.086994    5563 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-17 18:02:50.108198: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "config.json: 100% 1.05k/1.05k [00:00<00:00, 8.10MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 738M/738M [00:31<00:00, 23.5MB/s]\n",
            "tokenizer_config.json: 100% 1.28k/1.28k [00:00<00:00, 8.66MB/s]\n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 111MB/s]\n",
            "tokenizer.json: 100% 8.66M/8.66M [00:01<00:00, 6.54MB/s]\n",
            "added_tokens.json: 100% 23.0/23.0 [00:00<00:00, 170kB/s]\n",
            "special_tokens_map.json: 100% 286/286 [00:00<00:00, 2.62MB/s]\n",
            "\u001b[?25lTotal number of predictions: \u001b[1;36m24\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0mAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  4%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  8%\u001b[0m \u001b[36m0:01:18\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 12%\u001b[0m \u001b[36m0:01:18\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 17%\u001b[0m \u001b[36m0:01:11\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 21%\u001b[0m \u001b[36m0:01:06\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 25%\u001b[0m \u001b[36m0:00:59\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 29%\u001b[0m \u001b[36m0:00:50\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 33%\u001b[0m \u001b[36m0:00:48\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 38%\u001b[0m \u001b[36m0:00:47\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 42%\u001b[0m \u001b[36m0:00:44\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 46%\u001b[0m \u001b[36m0:00:39\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 50%\u001b[0m \u001b[36m0:00:35\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 54%\u001b[0m \u001b[36m0:00:31\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 58%\u001b[0m \u001b[36m0:00:28\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[35m 62%\u001b[0m \u001b[36m0:00:24\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[35m 67%\u001b[0m \u001b[36m0:00:21\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[35m 71%\u001b[0m \u001b[36m0:00:19\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[35m 75%\u001b[0m \u001b[36m0:00:15\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[35m 79%\u001b[0m \u001b[36m0:00:14\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[35m 83%\u001b[0m \u001b[36m0:00:10\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[35m 88%\u001b[0m \u001b[36m0:00:08\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m 92%\u001b[0m \u001b[36m0:00:05\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[35m 96%\u001b[0m \u001b[36m0:00:03\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[3m           Evaluation Results           \u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
            "┃\u001b[35m \u001b[0m\u001b[35mMetric                     \u001b[0m\u001b[35m \u001b[0m┃\u001b[35m \u001b[0m\u001b[35mValue \u001b[0m\u001b[35m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
            "│ Question_Macro_F1           │ 0.7658 │\n",
            "│ Statement_Macro_F1          │ 0.4185 │\n",
            "│ Statement_Evidence_Macro_F1 │ 0.1214 │\n",
            "│ Reasoning_F1                │ 0.0939 │\n",
            "└─────────────────────────────┴────────┘\n",
            "Question_Macro_F1: 0.7658\n",
            "Statement_Macro_F1: 0.4185\n",
            "Statement_Evidence_Macro_F1: 0.1214\n",
            "Reasoning_F1: 0.0939\n"
          ]
        }
      ]
    }
  ]
}