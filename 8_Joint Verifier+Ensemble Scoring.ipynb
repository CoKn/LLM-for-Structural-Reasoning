{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1RkBXnGYwnllvbPTgXOoGC_6D7x_zCnqa","authorship_tag":"ABX9TyO648wQ3nK+UvHM8hxuQxuH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 8. Joint Verifier + Ensemble Scoring\n","\n","This notebook implements a hybrid LLaMA-3 + DeBERTa-v3-base verifier pipeline for structured reasoning (LLM-SR):\n","\n","1. **Data Preparation**  \n","   - Corrupt valid question-parsing (QP) and CoT-parsing (CP) outputs to generate negative examples  \n","   - Build a balanced train/dev JSONL dataset for verifier fine-tuning  \n","\n","2. **Verifier Training**  \n","   - Fine-tune DeBERTa-v3-base as a binary classifier on (premise, hypothesis) pairs  \n","   - Use class-weighted cross-entropy loss to handle label imbalance  \n","   - Evaluate with accuracy, binary/macro F1, and class-wise precision/recall  \n","\n","3. **Inference Pipeline**  \n","   - **QP Stage:** sample 3 QP candidates (temperature sampling)  \n","   - **CP Stage:** generate 3 CP candidates per QP (beam search)  \n","   - **Verifier Scoring:** score each (QP, CP) pair using the fine-tuned DeBERTa verifier  \n","   - **LM Fallback:** compute LLaMA log-probs for QP and CP candidates  \n","   - **Ensemble Reranking:**  \n","     - Compute dynamic threshold = max(0.6, median(verifier_scores)+0.5·std)  \n","     - Score_i = 0.65·verifier + 0.20·norm(QP_logprob) + 0.15·norm(CP_logprob)  \n","     - If best Score ≥ threshold, select that candidate; otherwise fallback to highest log-prob  \n","\n","4. **Evaluation**  \n","   - Save structured outputs to JSON  \n","   - Run `eval.py` with official thresholds for question, statement, and relation F1  \n","\n","**Goals:**  \n","- Leverage a learned verifier to correct LLM parsing errors  \n","- Combine model confidence and likelihoods for robust candidate selection  \n","- Maintain interpretable, structured (statement, evidence, verification) outputs  \n","\n"],"metadata":{"id":"KLRP1o0H7ZOR"}},{"cell_type":"markdown","source":["## Configuration"],"metadata":{"id":"cBioYyEp9fQK"}},{"cell_type":"code","source":["# Install core evaluation utilities\n","!pip install -q evaluate\n","!pip install json5\n","\n","!pip uninstall -y nltk\n","!pip install -q --upgrade nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yj2aq8Wk9hb2","executionInfo":{"status":"ok","timestamp":1747497416674,"user_tz":-120,"elapsed":9534,"user":{"displayName":"Erlisa Lokaj","userId":"15715708909238941246"}},"outputId":"3d9acd1f-ec6f-4bcd-e694-ca89a5b1ed87"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting json5\n","  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n","Downloading json5-0.12.0-py3-none-any.whl (36 kB)\n","Installing collected packages: json5\n","Successfully installed json5-0.12.0\n","Found existing installation: nltk 3.9.1\n","Uninstalling nltk-3.9.1:\n","  Successfully uninstalled nltk-3.9.1\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download(\"punkt_tab\")\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y_0TnDWm9jYx","executionInfo":{"status":"ok","timestamp":1747497539003,"user_tz":-120,"elapsed":2138,"user":{"displayName":"Erlisa Lokaj","userId":"15715708909238941246"}},"outputId":"907db825-5a2a-4d3b-ceaf-32add8726982"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["## Prepare data for training"],"metadata":{"id":"90M_De4D8Cvd"}},{"cell_type":"code","source":["import json\n","import json5\n","import random\n","import copy\n","from sklearn.model_selection import train_test_split\n","\n","# CONFIGURATION\n","INPUT = \"/content/drive/MyDrive/llm-sr-project/700dataset.json\"\n","OUT_TRAIN = \"/content/drive/MyDrive/llm-sr-project/verifier_train.jsonl\"\n","OUT_DEV   = \"/content/drive/MyDrive/llm-sr-project/verifier_dev.jsonl\"\n","NEG_PER_POS = 3    # Number of negative (corrupted) examples to generate per positive one\n","DEV_SIZE    = 0.1  # Fraction of the dataset to use as the dev set\n","\n","\n","def corrupt_question_parsing(qp):\n","    \"\"\"\n","    Corrupt a valid question_parsing (QP) list by either:\n","    - Randomly dropping one sentence (if more than one), or\n","    - Shuffling the entire list to break order-dependence\n","    \"\"\"\n","    qp2 = qp.copy()\n","    if random.random() < 0.5 and len(qp2)>1:\n","        # drop one random\n","        qp2.pop(random.randrange(len(qp2)))\n","    else:\n","        random.shuffle(qp2)\n","    return qp2\n","\n","def corrupt_cot_parsing(cp):\n","    \"\"\"\n","    Corrupt a valid cot_parsing (CP) list of dicts by:\n","    - Flipping the verification value (true ↔ false)\n","    - Swapping evidence between two steps\n","    - Dropping the 'evidence' field from one step\n","    \"\"\"\n","    cp2 = copy.deepcopy(cp)\n","    if not cp2: return cp2\n","    choice = random.choice([\"flip\", \"swap\", \"drop_field\"])\n","    if choice == \"flip\":\n","        # flip a random statement's flag\n","        idx = random.randrange(len(cp2))\n","        cp2[idx][\"Verification\"] = \"true\" if cp2[idx][\"Verification\"]==\"false\" else \"false\"\n","    elif choice == \"swap\":\n","        # swap evidence between two\n","        if len(cp2)>=2:\n","            i,j = random.sample(range(len(cp2)), 2)\n","            cp2[i][\"evidence\"], cp2[j][\"evidence\"] = cp2[j][\"evidence\"], cp2[i][\"evidence\"]\n","    else:  # drop_field\n","        idx = random.randrange(len(cp2))\n","        cp2[idx].pop(\"evidence\", None)\n","    return cp2\n","\n","def make_record(question, cot, qp, cp, label):\n","    \"\"\"\n","    Format a training example with:\n","    - `premise`: question and CoT\n","    - `hypothesis`: the structured parses (QP + CP)\n","    - `label`: 1 (valid) or 0 (corrupted)\n","    \"\"\"\n","    premise = f\"{question}\\n\\nCoT:\\n{cot}\"\n","    hyp_qp = json.dumps(qp, ensure_ascii=False)\n","    hyp_cp = json.dumps(cp, ensure_ascii=False)\n","    hypothesis = f\"QuestionParsing: {hyp_qp}  CoTParsing: {hyp_cp}\"\n","    return {\"premise\": premise, \"hypothesis\": hypothesis, \"label\": label}\n","\n","# STEP 1: Load valid (positive) examples\n","with open(INPUT, \"r\", encoding=\"utf-8\") as f:\n","    positives = json5.loads(f.read())\n","\n","all_records = []\n","for ex in positives:\n","    q   = ex[\"question\"]\n","    cot = ex[\"cot\"]\n","    qp  = ex[\"question_parsing\"]\n","    cp  = ex[\"cot_parsing\"]\n","\n","    # Add the original, valid example (label = 1)\n","    all_records.append(make_record(q, cot, qp, cp, 1))\n","\n","    # Generate negative (corrupted) versions\n","    for _ in range(NEG_PER_POS):\n","        qp_bad = corrupt_question_parsing(qp)\n","        cp_bad = corrupt_cot_parsing(cp)\n","\n","        # Avoid adding duplicates accidentally\n","        if qp_bad==qp and cp_bad==cp:\n","            continue\n","        all_records.append(make_record(q, cot, qp_bad, cp_bad, 0))\n","\n","# STEP 2: Split into training and dev sets\n","train, dev = train_test_split(all_records, test_size=DEV_SIZE, random_state=42, stratify=[r[\"label\"] for r in all_records])\n","\n","# STEP 3: Write out the datasets in JSONL format\n","for path, split in [(OUT_TRAIN, train), (OUT_DEV, dev)]:\n","    with open(path, \"w\", encoding=\"utf-8\") as f:\n","        for rec in split:\n","            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n","\n","print(f\"✔︎ wrote {len(train)} train + {len(dev)} dev examples\")"],"metadata":{"id":"3k_uaV6T7Y1u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train the Joint Verifier"],"metadata":{"id":"aDvoMibX8IvE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0Ta0arK5jwa"},"outputs":[],"source":["import numpy as np\n","import random\n","from collections import Counter\n","import torch\n","import torch.nn as nn\n","\n","from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorWithPadding\n",")\n","from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n","\n","# 1) Load Dataset and Balance Training Set\n","data_files = {\n","    \"train\":      \"/content/drive/MyDrive/llm-sr-project/verifier_train.jsonl\",\n","    \"validation\": \"/content/drive/MyDrive/llm-sr-project/verifier_dev.jsonl\"\n","}\n","ds = load_dataset(\"json\", data_files=data_files)\n","\n","# Balance training data: downsample negatives to match positives\n","def balance(split):\n","    labels = split[\"label\"]\n","    neg = [i for i,l in enumerate(labels) if l==0]\n","    pos = [i for i,l in enumerate(labels) if l==1]\n","    random.seed(42)\n","    neg_down = random.sample(neg, len(pos))\n","    idxs = neg_down + pos\n","    random.shuffle(idxs)\n","    return split.select(idxs)\n","\n","ds[\"train\"] = balance(ds[\"train\"])\n","print(\"Train balance:\", Counter(ds[\"train\"][\"label\"]))\n","print(\"Dev   balance:\",   Counter(ds[\"validation\"][\"label\"]))  # leave as is\n","\n","# 2) Load Pretrained Model and Tokenizer\n","MODEL = \"microsoft/deberta-v3-base\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","model     = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n","device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# 3) Define Class-Weighted Loss\n","# Helps to balance contribution of majority/minority class\n","counts = Counter(ds[\"train\"][\"label\"])\n","w0 = counts[1] / (counts[0]+counts[1])  # weight for class 0 (neg)\n","w1 = counts[0] / (counts[0]+counts[1])  # weight for class 1 (pos)\n","weights = torch.tensor([w0, w1], device=device)\n","loss_fn  = nn.CrossEntropyLoss(weight=weights)\n","\n","# Custom Trainer to use class-weighted loss\n","class WeightedTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n","        labels = inputs.pop(\"labels\").to(device)\n","        outputs= model(**inputs)\n","        logits = outputs.logits\n","        loss   = loss_fn(logits, labels)\n","        return (loss, outputs) if return_outputs else loss\n","\n","# 4) Preprocessing Function\n","def preprocess(ex):\n","    enc = tokenizer(ex[\"premise\"], ex[\"hypothesis\"],\n","                    truncation=True, padding=False)\n","    enc[\"labels\"] = ex[\"label\"]\n","    return enc\n","\n","tokenized = ds.map(preprocess, batched=True, remove_columns=ds[\"train\"].column_names)\n","data_collator = DataCollatorWithPadding(tokenizer)\n","\n","# 5) Evaluation Metrics\n","def compute_metrics(p):\n","    logits = p.predictions\n","    labels = p.label_ids\n","    probs  = torch.softmax(torch.tensor(logits), dim=1).numpy()[:,1]\n","    preds  = (probs > 0.5).astype(int)\n","\n","    acc     = accuracy_score(labels, preds)\n","    f1_bin  = f1_score(labels, preds, average=\"binary\")\n","    f1_mac  = f1_score(labels, preds, average=\"macro\")\n","    p0, r0, f0, _ = precision_recall_fscore_support(labels, preds, labels=[0], average=\"binary\", zero_division=0)\n","    p1, r1, f1, _ = precision_recall_fscore_support(labels, preds, labels=[1], average=\"binary\", zero_division=0)\n","\n","    return {\n","        \"accuracy\":   acc,\n","        \"f1_binary\":  f1_bin,\n","        \"f1_macro\":   f1_mac,\n","        \"precision_0\": p0,\n","        \"recall_0\":    r0,\n","        \"f1_0\":        f0,\n","        \"precision_1\": p1,\n","        \"recall_1\":    r1,\n","        \"f1_1\":        f1,\n","    }\n","\n","# 6) TrainingArguments\n","training_args = TrainingArguments(\n","    output_dir=\"deberta3-verifier\",\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_steps=50,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=8,\n","    learning_rate=1e-5,\n","    num_train_epochs=10,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1_macro\",\n","    greater_is_better=True,\n","    fp16=True,\n","    report_to=\"none\",\n","    warmup_steps=100,\n","    lr_scheduler_type=\"linear\",\n",")\n","\n","# 7) Initialize Trainer and Train\n","trainer = WeightedTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized[\"train\"],\n","    eval_dataset=tokenized[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()\n","\n","# 8) Save Final Model\n","trainer.save_model(\"/content/drive/MyDrive/llm-sr-project/deberta3-verifier-final\")\n","tokenizer.save_pretrained(\"/content/drive/MyDrive/llm-sr-project/deberta3-verifier-final\")"]},{"cell_type":"markdown","source":["## Inference and Evaluate"],"metadata":{"id":"4qOFXBhv8Qot"}},{"cell_type":"markdown","source":["### Imports and Setup"],"metadata":{"id":"wzDHbQdG8Z-a"}},{"cell_type":"code","source":["import unsloth\n","import torch, gc, json, re, ast, html\n","from torch.nn.functional import log_softmax\n","from transformers import (\n","    AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSequenceClassification, pipeline as hf_pipeline\n",")\n","from datasets import load_dataset\n","\n","\n","# Optional json5 for fallback parsing\n","try:\n","    import json5\n","    USE_JSON5 = True\n","except ImportError:\n","    USE_JSON5 = False"],"metadata":{"id":"9nW0cQ6D8WDv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Prompts Template"],"metadata":{"id":"1VJtgCo-8kxT"}},{"cell_type":"code","source":["# In-Contect Learning (ICL)Prompting\n","QP_DEMON = '''The question is:\n","\n","There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project. This assignment must satisfy:\n","(1) If A works on Alpha, then B works on Beta.\n","(2) If C works on Alpha, then D and E work on Beta.\n","(3) F works on a different project than E.\n","(4) D must work on a different project than A.\n","(5) If F works on Alpha, then B works on Alpha.\n","\n","If A works on Beta, which of the following must be true?\n","A. B works on Alpha\n","B. C works on Beta\n","C. D works on Alpha\n","D. F works on Beta\n","\n","The parsing result is:\n","\n","[\n","  \"There are 6 volunteers: A, B, C, D, E and F. They will be assigned to either Project Alpha or Project Beta. Each person works on exactly one project.\",\n","  \"If A works on Alpha, then B works on Beta\",\n","  \"If C works on Alpha, then D and E work on Beta\",\n","  \"F works on a different project than E\",\n","  \"D must work on a different project than A\",\n","  \"If F works on Alpha, then B works on Alpha\",\n","  \"A works on Beta\"\n","]\n","'''\n","\n","QP_TEMPLATE = '''Given a question, extract all relevant information from the question that would help to solve it.\n","\n","Output only a JSON list and nothing else. Follow the format shown in the example.\n","\n","Example:\n","{demon}\n","\n","Now, the question is:\n","\n","{question}\n","\n","Your output:\n","'''\n","\n","CP_DEMON = '''The question is:\n","\n","There are 6 volunteers: A, B, C, D, E and F. Each person works on exactly one project.\n","\n","Conditions:\n","(1) If A works on Alpha, then B works on Beta.\n","(2) If C works on Alpha, then D and E work on Beta.\n","(3) F works on a different project than E.\n","(4) D must work on a different project than A.\n","(5) If F works on Alpha, then B works on Alpha.\n","\n","Question:\n","If A works on Beta, which of the following must be true?\n","\n","CoT:\n","Since A works on Beta, Condition (1) is not triggered. Condition (2) is not triggered since C's assignment is unknown. Condition (3) doesn't give anything because E's assignment is unspecified. Condition (4) says D must work on a different project than A, so D must work on Alpha. Condition (5) depends on F, which is unknown.\n","\n","Parsing result:\n","\n","[\n","  {\n","    \"statement\": \"Condition (1) is not applicable\",\n","    \"evidence\": \"Condition (1): If A works on Alpha, then B works on Beta. | A is working on Beta\",\n","    \"Verification\": \"false\"\n","  },\n","  {\n","    \"statement\": \"Condition (2) is not applicable\",\n","    \"evidence\": \"Condition (2): If C works on Alpha, then D and E work on Beta. | C's assignment is unknown\",\n","    \"Verification\": \"false\"\n","  },\n","  {\n","    \"statement\": \"Condition (3) does not provide any info\",\n","    \"evidence\": \"Condition (3): F works on a different project than E. | E's assignment is unknown\",\n","    \"Verification\": \"false\"\n","  },\n","  {\n","    \"statement\": \"D must work on Alpha\",\n","    \"evidence\": \"Condition (4): D must work on a different project than A, and A is working on Beta\",\n","    \"Verification\": \"true\"\n","  },\n","  {\n","    \"statement\": \"Condition (5) is not applicable\",\n","    \"evidence\": \"Condition (5): If F works on Alpha, then B works on Alpha. | F's assignment is unknown\",\n","    \"Verification\": \"false\"\n","  }\n","]\n","'''\n","\n","CP_TEMPLATE = '''You are a reasoning assistant. Based on the question, conditions, and chain-of-thought (CoT), extract every inference or non-inference step as a JSON object.\n","\n","Example:\n","{demon}\n","\n","Now, given:\n","\n","Question:\n","{question}\n","\n","Conditions:\n","{conditions}\n","\n","Chain-of-Thought:\n","{cot}\n","\n","Your output:\n","'''"],"metadata":{"id":"lNG6Taat8niw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Helper Functions"],"metadata":{"id":"U8RU99WU8sPf"}},{"cell_type":"code","source":["# Helper Functions\n","def clean_quotes(t):\n","    return (t.replace('“','\"').replace('”','\"')\n","             .replace(\"‘\",\"'\").replace(\"’\",\"'\"))\n","\n","def normalize_text(t):\n","    t = clean_quotes(t)\n","    t = re.sub(r'\\?\\s(?=[A-Z])', ', ', t)\n","    t = re.sub(r'(?<=[a-zA-Z])\\.(?=[A-Z])', '. ', t)\n","    t = re.sub(r'(?<![A-Da-d])\\\\n(?!\\s?[A-Da-d]\\\\.)', ' ', t)\n","    return html.unescape(t).strip()\n","\n","def extract_first_json_array(raw: str):\n","    raw = raw.strip()\n","    start = raw.find('[')\n","    if start < 0: return None\n","    depth = 0\n","    for i,ch in enumerate(raw[start:], start):\n","        if ch=='[': depth+=1\n","        elif ch==']': depth-=1\n","        if depth==0:\n","            block = raw[start:i+1]\n","            for parser in (json.loads, ast.literal_eval, json5.loads if USE_JSON5 else None):\n","                if not parser: continue\n","                try: return parser(block)\n","                except: pass\n","            return None\n","    return None"],"metadata":{"id":"rGuR0jep8t-A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Models and Verifier"],"metadata":{"id":"utqJv3IW8w0K"}},{"cell_type":"code","source":["# Load Models and Verifier\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","BATCH_SIZE = 2\n","conf_min = 0.6\n","\n","# QP generation (3 samples)\n","qp_model_path = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_question_parsing\"\n","qp_tok = AutoTokenizer.from_pretrained(qp_model_path)\n","qp_tok.model_max_length = 1024\n","qp_tok.padding_side = \"left\"\n","qp_mod = AutoModelForCausalLM.from_pretrained(qp_model_path,\n","    device_map=\"auto\", torch_dtype=torch.float16)\n","qp_pipe = hf_pipeline(\"text-generation\", model=qp_mod, tokenizer=qp_tok,\n","    return_full_text=False, do_sample=True, temperature=0.7,\n","    num_return_sequences=3, max_new_tokens=512, batch_size=BATCH_SIZE)\n","\n","# CP generation (deterministic beam‐search)\n","cp_model_path = \"/content/drive/MyDrive/llm-sr-project/finetuned_llama3_cot_parsing\"\n","cp_tok = AutoTokenizer.from_pretrained(cp_model_path)\n","cp_tok.model_max_length = 2048\n","cp_tok.padding_side = \"left\"\n","cp_mod = AutoModelForCausalLM.from_pretrained(cp_model_path,\n","    device_map=\"auto\", torch_dtype=torch.float16)\n","cp_pipe = hf_pipeline(\"text-generation\", model=cp_mod, tokenizer=cp_tok,\n","    return_full_text=False, num_beams=5, do_sample=False,\n","    max_new_tokens=1024, batch_size=BATCH_SIZE)\n","\n","# Verifier model (fine-tuned DeBERTa)\n","verifier_path = \"/content/drive/MyDrive/llm-sr-project/deberta3-verifier-final\"\n","rew_tok = AutoTokenizer.from_pretrained(verifier_path)\n","rew_mod = AutoModelForSequenceClassification.from_pretrained(\n","    verifier_path, device_map=\"auto\", torch_dtype=torch.float16)\n","rew_mod.to(device)\n","\n","print(\"✅ Models loaded.\")"],"metadata":{"id":"DTF5YXqC8zJZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Main Scoring Function"],"metadata":{"id":"PgTHSA0081nt"}},{"cell_type":"code","source":["# Scoring Helpers\n","def compute_logprobs(cands, tokenizer_, model_):\n","    \"\"\"Return nested list of log‐prob sums for each candidate list.\"\"\"\n","    out = []\n","    for lst in cands:\n","        scores = []\n","        for item in lst:\n","            if not item:\n","                scores.append(float('-inf'))\n","                continue\n","            s = json.dumps(item) if not isinstance(item, str) else item\n","            enc = tokenizer_(s, return_tensors=\"pt\",\n","                             truncation=True, padding=True,\n","                             max_length=tokenizer_.model_max_length).to(device)\n","            with torch.no_grad():\n","                logits = model_(**enc, labels=enc[\"input_ids\"]).logits\n","            # sum token log‐probs\n","            lps = log_softmax(logits[0, :-1], dim=-1)\n","            lbls = enc[\"input_ids\"][0,1:]\n","            score = lps[range(lbls.size(0)), lbls].sum().item()\n","            scores.append(score)\n","        out.append(scores)\n","    return out\n","\n","# Main structured function with ensemble + consistency\n","def make_structured(batch):\n","\n","    # 1) Normalize\n","    questions = [normalize_text(q) for q in batch[\"question\"]]\n","    cots      = [normalize_text(c) for c in batch[\"cot\"]]\n","    ids       = batch[\"id\"]\n","    answers   = batch.get(\"answer\", [None]*len(questions))\n","\n","    # 2) Generate QP candidates\n","    qp_prompts = [QP_TEMPLATE.format(demon=QP_DEMON, question=q)\n","                  for q in questions]\n","    raw_qp = qp_pipe(qp_prompts, batch_size=len(qp_prompts))\n","    flat_qp = sum((sub if isinstance(sub,list) else [sub]\n","                   for sub in raw_qp), [])\n","    qp_lists = [extract_first_json_array(x[\"generated_text\"]) or []\n","                for x in flat_qp]\n","    qp_cands = [qp_lists[i*3:(i+1)*3] for i in range(len(questions))]\n","\n","    # 3) Generate CP candidates with cross‐feedback\n","    cp_prompts, mapping = [], []\n","    for qi,(q,cot,qp_list) in enumerate(zip(questions,cots,qp_cands)):\n","        for pi,qp in enumerate(qp_list):\n","            prompt = CP_TEMPLATE.format(\n","                demon=CP_DEMON,\n","                question=q,\n","                conditions=json.dumps(qp,ensure_ascii=False),\n","                cot=cot\n","            )\n","            cp_prompts.append(prompt)\n","            mapping.append((qi,pi))\n","    raw_cp = cp_pipe(cp_prompts, batch_size=len(cp_prompts))\n","    flat_cp = sum((sub if isinstance(sub,list) else [sub]\n","                   for sub in raw_cp), [])\n","    cp_lists = [extract_first_json_array(x[\"generated_text\"]) or []\n","                for x in flat_cp]\n","    # regroup\n","    cp_cands = [[] for _ in questions]\n","    for (qi,pi), lst in zip(mapping, cp_lists):\n","        while len(cp_cands[qi])<=pi:\n","            cp_cands[qi].append([])\n","        cp_cands[qi][pi] = lst\n","\n","    # 4) Verifier scoring\n","    premises, hyps, pairs = [], [], []\n","    for qi,(q,cot,qp_list,cp_list) in enumerate(zip(questions,cots,qp_cands,cp_cands)):\n","        for pi,(qp,cp) in enumerate(zip(qp_list,cp_list)):\n","            if not qp or not cp: continue\n","            prem = f\"{q}\\n\\nCoT:\\n{cot}\"\n","            hyp = \"Question Parsing:\\n\" + \"\\n\".join(f\"- {s}\" for s in qp)\n","            hyp += \"\\n\\nCoT Parsing:\\n\" + \"\\n\".join(f\"- {st.get('statement','')}\" for st in cp)\n","            premises.append(prem); hyps.append(hyp); pairs.append((qi,pi))\n","    # batch & score\n","    ver_scores = []\n","    for i in range(0,len(premises),32):\n","        toks = rew_tok(premises[i:i+32], hyps[i:i+32],\n","                       return_tensors=\"pt\", padding=True,\n","                       truncation=True).to(device)\n","        with torch.no_grad():\n","            logits = rew_mod(**toks).logits\n","        probs = (torch.sigmoid(logits).squeeze(-1)\n","                 if logits.size(1)==1\n","                 else logits.softmax(1)[:,1])\n","        ver_scores.extend(probs.cpu().tolist())\n","\n","    # 5) LM fallback\n","    qp_lps = compute_logprobs(qp_cands, qp_tok, qp_mod)\n","    cp_lps = compute_logprobs(cp_cands, cp_tok, cp_mod)\n","\n","    # 6) Dynamic threshold\n","    if ver_scores:\n","        med, std = np.median(ver_scores), np.std(ver_scores)\n","        thr_dyn = max(conf_min, med + 0.5*std)\n","    else:\n","        thr_dyn = conf_min\n","\n","    # 7) Weighted ensemble reranking\n","    best_qp, best_cp = [], []\n","    # group ver_scores by question candidate index\n","    ver_dict = {}\n","    for (qi,pi),score in zip(pairs, ver_scores):\n","        ver_dict.setdefault(qi, {})[pi] = score\n","\n","    for qi in range(len(questions)):\n","        scores = []\n","        for pi in range(len(qp_cands[qi])):\n","            v = ver_dict.get(qi,{}).get(pi,0.0)\n","            lq = qp_lps[qi][pi] if pi<len(qp_lps[qi]) else -1e9\n","            lc = cp_lps[qi][pi] if pi<len(cp_lps[qi]) else -1e9\n","            # normalize lps\n","            nq = 1/(1+math.exp(-lq/100))\n","            nc = 1/(1+math.exp(-lc/100))\n","            # weights\n","            w = 0.65*v + 0.20*nq + 0.15*nc\n","            scores.append((pi,w))\n","        if not scores:\n","            best_qp.append([]); best_cp.append([])\n","            continue\n","        pi_best,score_best = max(scores, key=lambda x:x[1])\n","        if score_best>=thr_dyn:\n","            best_qp.append(qp_cands[qi][pi_best])\n","            best_cp.append(cp_cands[qi][pi_best])\n","        else:\n","            # fallback to LM-only\n","            pi_lm = max(range(len(qp_lps[qi])), key=lambda i: qp_lps[qi][i])\n","            best_qp.append(qp_cands[qi][pi_lm])\n","            best_cp.append(cp_cands[qi][pi_lm] if pi_lm<len(cp_cands[qi]) else [])\n","\n","    return {\n","        \"question\":         questions,\n","        \"question_parsing\": best_qp,\n","        \"cot\":              cots,\n","        \"cot_parsing\":      best_cp,\n","        \"id\":               ids,\n","        \"answer\":           answers,\n","    }"],"metadata":{"id":"_L3ilC1L89Ac"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run and Save"],"metadata":{"id":"TyGih5qX9ALs"}},{"cell_type":"code","source":["# Run and Save Predictions\n","if __name__==\"__main__\":\n","    torch.cuda.empty_cache(); gc.collect()\n","    ds = load_dataset(\"json\", data_files={\"test\":\"/content/drive/MyDrive/llm-sr-project/testingData-blank.json\"})[\"test\"]\n","    out = ds.map(make_structured, batched=True, batch_size=BATCH_SIZE,\n","                 remove_columns=ds.column_names)\n","    out.to_json(\"/content/drive/MyDrive/llm-sr-project/results_ensemble.json\",\n","                orient=\"records\", lines=False)\n","    print(\"✅ Done!\")"],"metadata":{"id":"kHEYXmAF8_vw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Transform Predictions"],"metadata":{"id":"-csgCmmy9Ld_"}},{"cell_type":"code","source":["import json\n","\n","INPUT_PATH  = \"/content/drive/MyDrive/llm-sr-project/results_ensemble.json\"\n","OUTPUT_PATH = \"/content/drive/MyDrive/llm-sr-project/final_results_ensemble.json\"\n","\n","def transform_example(ex):\n","    # reorder each cot_parsing entry: statement → evidence → Verification\n","    reordered = []\n","    for step in ex.get(\"cot_parsing\", []):\n","        reordered.append({\n","            \"statement\":    step.get(\"statement\"),\n","            \"evidence\":     step.get(\"evidence\"),\n","            \"Verification\": step.get(\"Verification\"),\n","        })\n","\n","    return {\n","        \"question\":         ex.get(\"question\"),\n","        \"question_parsing\": ex.get(\"question_parsing\"),\n","        \"answer\":           ex.get(\"answer\"),\n","        \"id\":               ex.get(\"id\"),\n","        \"cot\":              ex.get(\"cot\"),\n","        \"cot_parsing\":      reordered,\n","        \"sel_idx\":          ex.get(\"sel_idx\"),\n","    }\n","\n","def main():\n","    with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n","        examples = json.load(f)\n","\n","    structured = [transform_example(ex) for ex in examples]\n","\n","    with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(structured, f, ensure_ascii=False, indent=2)\n","\n","    print(f\"Wrote {len(structured)} examples to {OUTPUT_PATH}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"ljV1Gwhw9Nu8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluate"],"metadata":{"id":"F83CCweG9G43"}},{"cell_type":"code","source":["EVAL_SCRIPT = \"/content/drive/MyDrive/llm-sr-project/eval.py\"\n","PREDICTION_PATH = \"/content/drive/MyDrive/llm-sr-project/final_results_ensemble.json\"\n","REFERENCE_PATH = \"/content/drive/MyDrive/llm-sr-project/test-reference.json\"\n","\n","!python {EVAL_SCRIPT} \\\n","  --prediction {PREDICTION_PATH} \\\n","  --reference {REFERENCE_PATH} \\\n","  --question_threshold 0.95 \\\n","  --statement_threshold 0.9 \\\n","  --relation_threshold 0.9"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXYL8Qc29Jei","executionInfo":{"status":"ok","timestamp":1747497641297,"user_tz":-120,"elapsed":92612,"user":{"displayName":"Erlisa Lokaj","userId":"15715708909238941246"}},"outputId":"9c6643bb-0c2c-4a4a-c78b-b1a1bf74d2b2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-05-17 15:59:20.655049: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-05-17 15:59:20.673412: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1747497560.695235    1784 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1747497560.701763    1784 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-05-17 15:59:20.723004: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","config.json: 100% 1.05k/1.05k [00:00<00:00, 8.99MB/s]\n","Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","model.safetensors: 100% 738M/738M [00:04<00:00, 166MB/s]\n","tokenizer_config.json: 100% 1.28k/1.28k [00:00<00:00, 11.8MB/s]\n","spm.model: 100% 2.46M/2.46M [00:00<00:00, 9.59MB/s]\n","tokenizer.json: 100% 8.66M/8.66M [00:00<00:00, 21.7MB/s]\n","added_tokens.json: 100% 23.0/23.0 [00:00<00:00, 244kB/s]\n","special_tokens_map.json: 100% 286/286 [00:00<00:00, 2.67MB/s]\n","\u001b[?25lTotal number of predictions: \u001b[1;36m24\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0mAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  4%\u001b[0m \u001b[36m-:--:--\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  8%\u001b[0m \u001b[36m0:00:42\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 12%\u001b[0m \u001b[36m0:00:54\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 17%\u001b[0m \u001b[36m0:00:54\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 21%\u001b[0m \u001b[36m0:00:54\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 25%\u001b[0m \u001b[36m0:00:49\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 29%\u001b[0m \u001b[36m0:00:44\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 33%\u001b[0m \u001b[36m0:00:43\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 38%\u001b[0m \u001b[36m0:00:39\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 42%\u001b[0m \u001b[36m0:00:39\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 46%\u001b[0m \u001b[36m0:00:34\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 50%\u001b[0m \u001b[36m0:00:32\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 54%\u001b[0m \u001b[36m0:00:29\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 58%\u001b[0m \u001b[36m0:00:25\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[35m 62%\u001b[0m \u001b[36m0:00:23\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[35m 67%\u001b[0m \u001b[36m0:00:20\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[35m 71%\u001b[0m \u001b[36m0:00:17\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[35m 75%\u001b[0m \u001b[36m0:00:14\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[35m 79%\u001b[0m \u001b[36m0:00:12\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[35m 83%\u001b[0m \u001b[36m0:00:10\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[35m 88%\u001b[0m \u001b[36m0:00:07\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[35m 92%\u001b[0m \u001b[36m0:00:05\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[35m 96%\u001b[0m \u001b[36m0:00:03\u001b[0m\u001b[92m────────────────────────────────────────────────────────────────────────────────\u001b[0m\n","\u001b[2K\u001b[36m Evaluating ...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[3m           Evaluation Results           \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n","┃\u001b[35m \u001b[0m\u001b[35mMetric                     \u001b[0m\u001b[35m \u001b[0m┃\u001b[35m \u001b[0m\u001b[35mValue \u001b[0m\u001b[35m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n","│ Question_Macro_F1           │ 0.7253 │\n","│ Statement_Macro_F1          │ 0.2152 │\n","│ Statement_Evidence_Macro_F1 │ 0.0953 │\n","│ Reasoning_F1                │ 0.0681 │\n","└─────────────────────────────┴────────┘\n","Question_Macro_F1: 0.7253\n","Statement_Macro_F1: 0.2152\n","Statement_Evidence_Macro_F1: 0.0953\n","Reasoning_F1: 0.0681\n"]}]}]}