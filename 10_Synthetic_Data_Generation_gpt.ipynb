{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Synthetic Data Generation with Gpt",
   "id": "ebaeedf11805ed18"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-03T13:22:17.703611Z",
     "start_time": "2025-06-03T13:22:16.501167Z"
    }
   },
   "source": [
    "import os\n",
    "from utils.AzureAdapter import AzureAdapter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"AZURE_API_KEY\")\n",
    "api_endpoint = os.getenv(\"AZURE_API_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_API_VERSION\")\n",
    "deployment_name = \"gpt-4o\"\n",
    "\n",
    "llm = AzureAdapter(api_key=api_key, api_endpoint=api_endpoint, api_version=api_version)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T14:15:46.689897Z",
     "start_time": "2025-06-03T14:15:46.680534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from random import randint\n",
    "from utils.save_and_load_json import save_json\n",
    "\n",
    "def parse_and_save(llm_response, folder_path=\"\", save_data=True):\n",
    "    seed = int(time.time())\n",
    "\n",
    "    # Parse the response from the llm\n",
    "    data = json.loads(llm_response)\n",
    "\n",
    "    # Add sel_idx and id\n",
    "    if type(data) == dict and 'puzzles' in data.keys():\n",
    "        data = data['puzzles']\n",
    "\n",
    "    elif type(data) == dict and 'questions' in data.keys():\n",
    "        data = data['questions']\n",
    "\n",
    "    elif type(data) == dict:\n",
    "        data = [data]\n",
    "\n",
    "    # Save each puzzle as own json\n",
    "    for puzzle in data:\n",
    "        rand =  randint(0, 1000000)\n",
    "        puzzle['id'] = rand\n",
    "        puzzle['sel_idx'] = rand\n",
    "        if save_data:\n",
    "            save_json(data=puzzle, file_path=f'{folder_path}/{seed}_{rand}.json')\n",
    "\n",
    "    print((type(data), len(data)))\n",
    "    return data\n",
    "\n",
    "def has_enough_json_files(folder_path, target_count):\n",
    "    \"\"\"\n",
    "    Check if a folder contains at least the target number of JSON files.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise ValueError(f\"The folder path '{folder_path}' does not exist or is not a directory\")\n",
    "\n",
    "    json_count = sum(1 for file in os.listdir(folder_path) if file.lower().endswith('.json'))\n",
    "\n",
    "    return json_count >= target_count"
   ],
   "id": "85cc18c9c1c1f7c4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generating puzzles with one llm call",
   "id": "bb335258a96e709d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T13:31:56.927029Z",
     "start_time": "2025-06-03T13:25:33.552733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from prompts.prompts import entire_puzzle_generation_prompt\n",
    "\n",
    "folder_path = 'data/synthetic/raw/gpt_one_call'\n",
    "\n",
    "while not has_enough_json_files(folder_path, target_count=300):\n",
    "    prompt = 'Generate generate 10 new puzzles'\n",
    "    try:\n",
    "        res = llm.call_model(prompt=prompt, system_prompt=entire_puzzle_generation_prompt, deployment_name=deployment_name)\n",
    "        data = parse_and_save(folder_path=folder_path, llm_response=res)\n",
    "        print(f'{len(data)} successfully generated')\n",
    "    except Exception as ex:\n",
    "        print('Skipping generation!')\n",
    "        print(ex)"
   ],
   "id": "fb85557da2051766",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'list'>, 1)\n",
      "1 successfully generated\n",
      "(<class 'list'>, 2)\n",
      "2 successfully generated\n",
      "Skipping generation!\n",
      "Unterminated string starting at: line 240 column 19 (char 14164)\n",
      "(<class 'list'>, 9)\n",
      "9 successfully generated\n",
      "Skipping generation!\n",
      "Unterminated string starting at: line 228 column 33 (char 19599)\n",
      "(<class 'list'>, 2)\n",
      "2 successfully generated\n",
      "(<class 'list'>, 2)\n",
      "2 successfully generated\n",
      "(<class 'list'>, 2)\n",
      "2 successfully generated\n",
      "(<class 'list'>, 5)\n",
      "5 successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 successfully generated\n",
      "(<class 'list'>, 5)\n",
      "5 successfully generated\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Creating Puzzles in two llm calls",
   "id": "b1678ffc89fbddd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T13:47:33.354508Z",
     "start_time": "2025-06-03T13:33:09.787189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from prompts.prompts import question_generation_prompt, solution_generation_prompt\n",
    "import json\n",
    "\n",
    "n = 1\n",
    "folder_path = 'data/synthetic/raw/gpt_two_calls'\n",
    "questions = None\n",
    "retry = False\n",
    "retry_count = 0\n",
    "\n",
    "# Ensure folder exists\n",
    "# os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "while not has_enough_json_files(folder_path=folder_path, target_count=265):\n",
    "    if not retry:\n",
    "        try:\n",
    "            # Create puzzle questions\n",
    "            prompt_questions = \"Generate {} new puzzle questions\".format(n)\n",
    "            res_questions = llm.call_model(prompt=prompt_questions, system_prompt=question_generation_prompt, deployment_name=deployment_name)\n",
    "            questions = parse_and_save(llm_response=res_questions, save_data=False)\n",
    "            print(f'{len(questions)} questions successfully generated')\n",
    "            retry_count = 0\n",
    "        except Exception as ex:\n",
    "            print('Skipping question generation!')\n",
    "            print(ex)\n",
    "            questions = None\n",
    "            retry = False\n",
    "            retry_count = 0\n",
    "\n",
    "    if questions:\n",
    "        try:\n",
    "            formatted_questions = json.dumps(questions, indent=2)\n",
    "            prompt_solutions = f\"Solve all {n} logical puzzles:\\n{formatted_questions}\"\n",
    "\n",
    "            res_solutions = llm.call_model(prompt=prompt_solutions, system_prompt=solution_generation_prompt, deployment_name=deployment_name)\n",
    "            solutions = parse_and_save(folder_path=folder_path, llm_response=res_solutions, save_data=True)\n",
    "            print(f'{len(solutions)} solutions successfully generated')\n",
    "            retry = False\n",
    "            retry_count = 0\n",
    "        except Exception as ex:\n",
    "            print('Retrying solution generation!')\n",
    "            print(ex)\n",
    "            retry = True\n",
    "            retry_count += 1\n",
    "            if retry_count > 3:\n",
    "                print(\"Too many retries, generating new questions\")\n",
    "                retry = False\n",
    "                questions = None\n",
    "                retry_count = 0"
   ],
   "id": "38c726a826ebc414",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 questions successfully generated\n",
      "(<class 'list'>, 1)\n",
      "1 solutions successfully generated\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Puzzle verification\n",
    "\n",
    "##### Duplicates\n",
    "Sometimes LLMs generate redundant puzzle so we need to filter out redundant puzzles. Here we can use two different approaches.\n",
    "1. We can use a LLM as classifier to assess based on the semantics if there are duplicates\n",
    "2. We can use a syntactic assessment via a the levenstein distance"
   ],
   "id": "3ed222dcc6056fc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Levenstein Distance",
   "id": "ad2a2ca050ab63eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T19:39:11.118908Z",
     "start_time": "2025-06-02T19:39:11.103651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.save_and_load_json import load_all_json_files\n",
    "\n",
    "data = load_all_json_files(folder_path='data/synthetic/raw/gpt_two_calls')"
   ],
   "id": "e0f5f86d4af2e6a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 JSON files from ./data/synthetic/gpt_two_calls\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T19:36:33.188864Z",
     "start_time": "2025-06-02T19:33:24.742271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Levenshtein distance\n",
    "from utils.duplicate_management import find_similar_items\n",
    "\n",
    "keys = ['question', 'question_parsing', 'answer', 'cot', 'cot_parsing']\n",
    "dup = find_similar_items(data, keys_to_compare=keys, threshold=.5)\n",
    "\n",
    "print(dup)"
   ],
   "id": "2b468f878115489c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:10:22.035499Z",
     "start_time": "2025-06-02T15:10:22.030594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find a specific item by ID\n",
    "item_1 = next((item for item in data if item.get('id') == 669342), None)\n",
    "item_2 = next((item for item in data if item.get('id') == 129199), None)\n",
    "\n",
    "print(item_1)\n",
    "print(item_2)"
   ],
   "id": "5077f869b0ec15ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'There are five students: A, B, C, D, and E who must be assigned to one of two study groups, X or Y. The following conditions apply: 1) If A is in group X, then B must be in group Y. 2) C and D must be in different groups. 3) If E is in group Y, then C must also be in group Y. 4) A and D cannot be in the same group. Question: If C is in group X, which of the following must be true? a) A is in group X. b) B is in group Y. c) E is in group Y. d) D is in group Y.', 'question_parsing': ['Entities: Students A, B, C, D, E.', 'Initial setup: Assign each to group X or Y.', 'Rule 1: If A is in X, then B is in Y.', 'Rule 2: C and D must be in different groups.', 'Rule 3: If E is in Y, then C is in Y.', 'Rule 4: A and D cannot be in the same group.', 'Specific scenario: C is in group X.'], 'answer': 'd', 'cot': \"Given C is in group X, D must be in group Y (Rule 2). Since A and D cannot be in the same group (Rule 4), A must be in group X. Rule 1 does not force a contradiction here since it would need A to be in group X and B in group Y, which does not conflict with the given scenario. Rule 3 does not apply since E's position is not constrained directly. Therefore, D in group Y is what must be true.\", 'cot_parsing': [{'statement': 'C is in group X, so D must be in group Y.', 'evidence': 'Rule 2: C and D must be in different groups.', 'Verification': 'true'}, {'statement': 'Since D is in Y, A must be in X.', 'evidence': 'Rule 4: A and D cannot be in the same group.', 'Verification': 'true'}], 'id': 669342, 'sel_idx': 669342}\n",
      "{'question': 'Five objects: L, M, N, O, and P must be placed in rooms 1 and 2. The conditions are: 1) If L is in room 1, then M must be in room 2. 2) N and O must be in different rooms. 3) If P is in room 2, then N must also be in room 2. 4) O and P cannot be in room 1 together. Question: If N is in room 1, which of the following must be true? a) L is in room 1. b) M is in room 2. c) P is in room 1. d) O is in room 2.', 'question_parsing': ['Entities: Objects L, M, N, O, P.', 'Initial setup: Assign each to room 1 or room 2.', 'Rule 1: If L is in 1, then M is in 2.', 'Rule 2: N and O must be in different rooms.', 'Rule 3: If P is in 2, then N is in 2.', 'Rule 4: O and P cannot both be in room 1.', 'Specific scenario: N is in room 1.'], 'answer': 'd', 'cot': \"Given N is in room 1, O must be in room 2 (Rule 2). Rule 4 does not apply since it requires both O and P to be in the same room. If P were in room 2, Rule 3 would be violated as N is confirmed in room 1. So, P must be in room 1. None of the constraints force L or M's placement based on the given scenario. Thus, O being in room 2 is the valid deduction.\", 'cot_parsing': [{'statement': 'N in room 1 requires O to be in room 2.', 'evidence': 'Rule 2: N and O in different rooms.', 'Verification': 'true'}, {'statement': 'P cannot be in room 2.', 'evidence': 'To avoid violation of Rule 3 with N in room 1.', 'Verification': 'true'}], 'id': 129199, 'sel_idx': 129199}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Language Model Classifier\n",
    "\n",
    "Since the syntactic filter indicates that there are no redundancies we will not implement a semantic duplicate finder via a language model."
   ],
   "id": "d9e08b7b6b9e13cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Verify structure of generated json files",
   "id": "49544e65c0a5c108"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T14:15:50.756279Z",
     "start_time": "2025-06-03T14:15:50.687836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.save_and_load_json import load_all_json_files\n",
    "from utils.structure_verification import verify_all_gpt_items\n",
    "\n",
    "# one gpt call\n",
    "folder_path = './data/synthetic/raw/gpt_one_call'\n",
    "data_one_gpt_call = load_all_json_files(folder_path=folder_path)\n",
    "\n",
    "invalid_items, valid_items = verify_all_gpt_items(data_one_gpt_call)\n",
    "print(len(valid_items))\n",
    "if not invalid_items:\n",
    "    print(\"All items match the required structure.\")\n",
    "else:\n",
    "    for idx, errs in invalid_items:\n",
    "        print(f\"Item at index {idx} has errors:\")\n",
    "        for e in errs:\n",
    "            print(\"  -\", e)\n",
    "\n",
    "save_json(data=valid_items, file_path='./data/synthetic/processed/gpt_one_call/gpt_one_call_processed.json')"
   ],
   "id": "f4ec1f76a20b83a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 301 JSON files from ./data/synthetic/raw/gpt_one_call\n",
      "Found 43 wrongly formatted entries.\n",
      "258\n",
      "Item at index 7 has errors:\n",
      "  -   cot_parsing[0] missing keys: {'Verification'}\n",
      "  -   cot_parsing[0] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[1] missing keys: {'Verification'}\n",
      "  -   cot_parsing[1] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[2] missing keys: {'Verification'}\n",
      "  -   cot_parsing[2] has unexpected keys: {'verification'}\n",
      "Item at index 15 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'message'}\n",
      "Item at index 19 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 23 has errors:\n",
      "  - Missing keys at top level: {'cot', 'cot_parsing', 'answer'}\n",
      "Item at index 34 has errors:\n",
      "  -   cot_parsing[0] missing keys: {'Verification'}\n",
      "  -   cot_parsing[0] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[1] missing keys: {'Verification'}\n",
      "  -   cot_parsing[1] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[2] missing keys: {'Verification'}\n",
      "  -   cot_parsing[2] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[3] missing keys: {'Verification'}\n",
      "  -   cot_parsing[3] has unexpected keys: {'verification'}\n",
      "Item at index 41 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 43 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 50 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 54 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 58 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 63 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 70 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'message'}\n",
      "Item at index 71 has errors:\n",
      "  -   cot_parsing[1] missing keys: {'Verification', 'evidence'}\n",
      "Item at index 72 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'message'}\n",
      "Item at index 75 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 76 has errors:\n",
      "  - \"question_parsing\" should be a list, got 'dict'\n",
      "Item at index 81 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 90 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 96 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 98 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 101 has errors:\n",
      "  -   cot_parsing[0] missing keys: {'Verification'}\n",
      "  -   cot_parsing[0] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[1] missing keys: {'Verification'}\n",
      "  -   cot_parsing[1] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[2] missing keys: {'Verification'}\n",
      "  -   cot_parsing[2] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[3] missing keys: {'Verification'}\n",
      "  -   cot_parsing[3] has unexpected keys: {'verification'}\n",
      "Item at index 104 has errors:\n",
      "  -   cot_parsing[0] missing keys: {'Verification'}\n",
      "  -   cot_parsing[0] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[1] missing keys: {'Verification'}\n",
      "  -   cot_parsing[1] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[2] missing keys: {'Verification'}\n",
      "  -   cot_parsing[2] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[3] missing keys: {'Verification'}\n",
      "  -   cot_parsing[3] has unexpected keys: {'verification'}\n",
      "Item at index 108 has errors:\n",
      "  -   cot_parsing[0] missing keys: {'Verification'}\n",
      "  -   cot_parsing[0] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[1] missing keys: {'Verification'}\n",
      "  -   cot_parsing[1] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[2] missing keys: {'Verification'}\n",
      "  -   cot_parsing[2] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[3] missing keys: {'Verification'}\n",
      "  -   cot_parsing[3] has unexpected keys: {'verification'}\n",
      "Item at index 113 has errors:\n",
      "  - \"question_parsing\" should be a list, got 'dict'\n",
      "Item at index 121 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'message'}\n",
      "Item at index 137 has errors:\n",
      "  - \"question_parsing\" should be a list, got 'dict'\n",
      "Item at index 140 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 157 has errors:\n",
      "  - \"question_parsing\" should be a list, got 'dict'\n",
      "Item at index 172 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'errors'}\n",
      "Item at index 177 has errors:\n",
      "  -   cot_parsing[0] missing keys: {'Verification'}\n",
      "  -   cot_parsing[0] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[1] missing keys: {'Verification'}\n",
      "  -   cot_parsing[1] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[2] missing keys: {'Verification'}\n",
      "  -   cot_parsing[2] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[3] missing keys: {'Verification'}\n",
      "  -   cot_parsing[3] has unexpected keys: {'verification'}\n",
      "Item at index 190 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 200 has errors:\n",
      "  - \"question_parsing\" should be a list, got 'dict'\n",
      "Item at index 201 has errors:\n",
      "  - \"question_parsing\" should be a list, got 'dict'\n",
      "Item at index 208 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 210 has errors:\n",
      "  - \"question_parsing\" should be a list, got 'dict'\n",
      "Item at index 237 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 239 has errors:\n",
      "  - \"question_parsing\" should be a list, got 'dict'\n",
      "Item at index 247 has errors:\n",
      "  -   cot_parsing[0] missing keys: {'Verification'}\n",
      "  -   cot_parsing[0] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[1] missing keys: {'Verification'}\n",
      "  -   cot_parsing[1] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[2] missing keys: {'Verification'}\n",
      "  -   cot_parsing[2] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[3] missing keys: {'Verification'}\n",
      "  -   cot_parsing[3] has unexpected keys: {'verification'}\n",
      "Item at index 251 has errors:\n",
      "  -   cot_parsing[0] missing keys: {'Verification'}\n",
      "  -   cot_parsing[0] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[1] missing keys: {'Verification'}\n",
      "  -   cot_parsing[1] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[2] missing keys: {'Verification'}\n",
      "  -   cot_parsing[2] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[3] missing keys: {'Verification'}\n",
      "  -   cot_parsing[3] has unexpected keys: {'verification'}\n",
      "Item at index 259 has errors:\n",
      "  -   cot_parsing[0] missing keys: {'Verification'}\n",
      "  -   cot_parsing[0] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[1] missing keys: {'Verification'}\n",
      "  -   cot_parsing[1] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[2] missing keys: {'Verification'}\n",
      "  -   cot_parsing[2] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[3] missing keys: {'Verification'}\n",
      "  -   cot_parsing[3] has unexpected keys: {'verification'}\n",
      "Item at index 264 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'message'}\n",
      "Item at index 266 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'error'}\n",
      "Item at index 297 has errors:\n",
      "  - \"question_parsing\" should be a list, got 'dict'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T14:15:53.913215Z",
     "start_time": "2025-06-03T14:15:53.648562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# two gpt calls\n",
    "folder_path = './data/synthetic/raw/gpt_two_calls'\n",
    "data_one_gpt_call = load_all_json_files(folder_path=folder_path)\n",
    "print(type(data_one_gpt_call[0]))\n",
    "\n",
    "invalid_items, valid_items = verify_all_gpt_items(data_one_gpt_call)\n",
    "print(len(valid_items))\n",
    "if not invalid_items:\n",
    "    print(\"All items match the required structure.\")\n",
    "else:\n",
    "    for idx, errs in invalid_items:\n",
    "        print(f\"Item at index {idx} has errors:\")\n",
    "        for e in errs:\n",
    "            print(\"  -\", e)\n",
    "\n",
    "save_json(data=valid_items, file_path='./data/synthetic/processed/gpt_two_calls/gpt_two_calls_processed.json')"
   ],
   "id": "dc6a84b7a46d5589",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 265 JSON files from ./data/synthetic/raw/gpt_two_calls\n",
      "<class 'dict'>\n",
      "Found 11 wrongly formatted entries.\n",
      "254\n",
      "Item at index 12 has errors:\n",
      "  -   cot_parsing[0] missing keys: {'Verification'}\n",
      "  -   cot_parsing[0] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[1] missing keys: {'Verification'}\n",
      "  -   cot_parsing[1] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[2] missing keys: {'Verification'}\n",
      "  -   cot_parsing[2] has unexpected keys: {'verification'}\n",
      "Item at index 32 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'questions'}\n",
      "Item at index 35 has errors:\n",
      "  -   question_parsing[0] should be a str, got 'dict'\n",
      "  -   question_parsing[1] should be a str, got 'dict'\n",
      "  -   question_parsing[2] should be a str, got 'dict'\n",
      "  -   question_parsing[3] should be a str, got 'dict'\n",
      "  -   question_parsing[4] should be a str, got 'dict'\n",
      "  -   question_parsing[5] should be a str, got 'dict'\n",
      "  -   question_parsing[6] should be a str, got 'dict'\n",
      "  -   question_parsing[7] should be a str, got 'dict'\n",
      "Item at index 37 has errors:\n",
      "  -   question_parsing[7] should be a str, got 'list'\n",
      "  -   question_parsing[8] should be a str, got 'list'\n",
      "Item at index 55 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'questions'}\n",
      "Item at index 56 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'puzzle'}\n",
      "Item at index 98 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'questions'}\n",
      "Item at index 107 has errors:\n",
      "  -   cot_parsing[0] missing keys: {'Verification'}\n",
      "  -   cot_parsing[0] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[1] missing keys: {'Verification'}\n",
      "  -   cot_parsing[1] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[2] missing keys: {'Verification'}\n",
      "  -   cot_parsing[2] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[3] missing keys: {'Verification'}\n",
      "  -   cot_parsing[3] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[4] missing keys: {'Verification'}\n",
      "  -   cot_parsing[4] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[5] missing keys: {'Verification'}\n",
      "  -   cot_parsing[5] has unexpected keys: {'verification'}\n",
      "Item at index 157 has errors:\n",
      "  -   cot_parsing[0] missing keys: {'Verification'}\n",
      "  -   cot_parsing[0] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[1] missing keys: {'Verification'}\n",
      "  -   cot_parsing[1] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[2] missing keys: {'Verification'}\n",
      "  -   cot_parsing[2] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[3] missing keys: {'Verification'}\n",
      "  -   cot_parsing[3] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[4] missing keys: {'Verification'}\n",
      "  -   cot_parsing[4] has unexpected keys: {'verification'}\n",
      "Item at index 160 has errors:\n",
      "  - Missing keys at top level: {'cot', 'question_parsing', 'cot_parsing', 'answer', 'question'}\n",
      "  - Unexpected extra keys at top level: {'questions'}\n",
      "Item at index 219 has errors:\n",
      "  -   cot_parsing[0] missing keys: {'Verification'}\n",
      "  -   cot_parsing[0] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[1] missing keys: {'Verification'}\n",
      "  -   cot_parsing[1] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[2] missing keys: {'Verification'}\n",
      "  -   cot_parsing[2] has unexpected keys: {'verification'}\n",
      "  -   cot_parsing[3] missing keys: {'Verification'}\n",
      "  -   cot_parsing[3] has unexpected keys: {'verification'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2434084c95c847af"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-nlp-project",
   "language": "python",
   "name": ".venv-nlp-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
